[
  {
    "objectID": "r-basics.html",
    "href": "r-basics.html",
    "title": "3  R basics",
    "section": "",
    "text": "3.1 Math operations",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#math-operations",
    "href": "r-basics.html#math-operations",
    "title": "3  R basics",
    "section": "",
    "text": "Sum\n\n1+1\n\n[1] 2\n\n\n\n\nSubtraction\n\n5-2\n\n[1] 3\n\n\n\n\nMultiplication\n\n2*2\n\n[1] 4\n\n\n\n\nDivision\n\n8/2\n\n[1] 4\n\n\n\n\nRound the number\n\nround(3.14)\n\n[1] 3\n\nround(3.14, 1) # The \"1\" indicates to round it up to 1 decimal digit.\n\n[1] 3.1\n\n\nYou can use help ?round in the console to see the description of the function, and the default arguments.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#basic-shortpaths",
    "href": "r-basics.html#basic-shortpaths",
    "title": "3  R basics",
    "section": "3.2 Basic shortpaths",
    "text": "3.2 Basic shortpaths\n\nPerform Combinations\n\nc(1, 2, 3)\n\n[1] 1 2 3\n\nc(1:3) # The \":\" indicates a range between the first and second numbers. \n\n[1] 1 2 3\n\n\n\n\nCreate a comment with ctrl + shift + c\n\n# Comments help you organize your code. The software will not run the comment. \n\n\n\nCreate a table\nA simple table with the number of trips by car, PT, walking, and cycling in a hypothetical street segment at a certain period.\nDefine variables\n\nmodes &lt;- c(\"car\", \"PT\", \"walking\", \"cycling\") # you can use \"=\" or \"&lt;-\"\nTrips = c(200, 50, 300, 150) # key sensitive (uppercase and lowercase letters are different)\n\nJoin the variables to create a table\n\ntable_example = data.frame(modes, Trips)\n\nTake a look at the table\nVisualize the table by clicking on the “Data” in the “Environment” page or use :\n\nView(table_example)\n\nLook at the first row\n\ntable_example[1,] #rows and columns start from 1 in R, differently from Python which starts from 0.\n\n  modes Trips\n1   car   200\n\n\nLook at first row and column\n\ntable_example[1,1]\n\n[1] \"car\"",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#practical-exercise",
    "href": "r-basics.html#practical-exercise",
    "title": "3  R basics",
    "section": "3.3 Practical exercise",
    "text": "3.3 Practical exercise\nDataset: the number of trips between all municipalities in the Lisbon Metropolitan Area, Portugal (INE 2018).\n\nImport dataset\nYou can click directly in the file under the “Files” pan, or:\n\ndata = readRDS(\"data/TRIPSmode_mun.Rds\")\n\n\n\n\n\n\n\nAfter you type \" you can use tab to navigate between folders and files and enter to autocomplete.\n\n\n\n\n\nTake a first look at the data\nSummary statistics\n\nsummary(data)\n\n  Origin_mun        Destination_mun        Total             Walk       \n Length:315         Length:315         Min.   :     7   Min.   :     0  \n Class :character   Class :character   1st Qu.:   330   1st Qu.:     0  \n Mode  :character   Mode  :character   Median :  1090   Median :     0  \n                                       Mean   : 16825   Mean   :  4033  \n                                       3rd Qu.:  5374   3rd Qu.:     0  \n                                       Max.   :875144   Max.   :306289  \n      Bike              Car            PTransit            Other        \n Min.   :   0.00   Min.   :     0   Min.   :     0.0   Min.   :    0.0  \n 1st Qu.:   0.00   1st Qu.:   263   1st Qu.:     5.0   1st Qu.:    0.0  \n Median :   0.00   Median :   913   Median :   134.0   Median :    0.0  \n Mean   :  80.19   Mean   :  9956   Mean   :  2602.6   Mean   :  152.4  \n 3rd Qu.:   0.00   3rd Qu.:  4408   3rd Qu.:   975.5   3rd Qu.:   62.5  \n Max.   :5362.00   Max.   :349815   Max.   :202428.0   Max.   :11647.0  \n\n\nCheck the structure of the data\n\nstr(data)\n\nClasses 'tbl_df', 'tbl' and 'data.frame':   315 obs. of  8 variables:\n $ Origin_mun     : chr  \"Alcochete\" \"Alcochete\" \"Alcochete\" \"Alcochete\" ...\n $ Destination_mun: chr  \"Alcochete\" \"Almada\" \"Amadora\" \"Barreiro\" ...\n $ Total          : num  20478 567 188 867 114 ...\n $ Walk           : num  6833 0 0 0 0 ...\n $ Bike           : num  320 0 0 0 0 0 0 0 91 0 ...\n $ Car            : num  12484 353 107 861 114 ...\n $ PTransit       : num  833 0 81 5 0 ...\n $ Other          : num  7 214 0 0 0 0 0 0 0 0 ...\n\n\nCheck the number of rows (observations) and columns (variables)\n\nnrow(data)\n\n[1] 315\n\nncol(data)\n\n[1] 8\n\n\nOpen the dataset\n\nView(data)\n\n\n\n\nThe following three rules make a dataset tidy: variables are columns, observations are rows, and values are cells. [@wickham17r]\n\n\n\n\nExplore the data\nCheck the total number of trips\nUse $ to select a variable of the data\n\nsum(data$Total)\n\n[1] 5299853\n\n\nPercentage of car trips related to the total\n\nsum(data$Car)/sum(data$Total) * 100\n\n[1] 59.17638\n\n\nPercentage of active trips related to the total\n\n(sum(data$Walk) + sum(data$Bike)) / sum(data$Total) * 100\n\n[1] 24.44883\n\n\n\n\nModify original data\nCreate a column with the sum of the number of trips for active modes\n\ndata$Active = data$Walk + data$Bike\n\nFilter by condition (create new tables)\nFilter trips only with origin from Lisbon\n\ndata_Lisbon = data[data$Origin == \"Lisboa\",]\n\nFilter trips with origin different from Lisbon\n\ndata_out_Lisbon = data[data$Origin != \"Lisboa\",]\n\nFilter trips with origin and destination in Lisbon\n\ndata_in_Out_Lisbon = data[data$Origin == \"Lisboa\" & data$Destination == \"Lisboa\",]\n\nRemove the first column\n\ndata = data[ ,-1] #first column\n\nCreate a table only with origin, destination and walking trips\nThere are many ways to do the same operation.\n\nnames(data)\n\n[1] \"Destination_mun\" \"Total\"           \"Walk\"            \"Bike\"           \n[5] \"Car\"             \"PTransit\"        \"Other\"           \"Active\"         \n\n\n\ndata_walk2 = data[ ,c(1,2,4)]\n\n\ndata_walk3 = data[ ,-c(3,5:9)]\n\n\n\nExport data\nSave data in .csv and .Rds\n\nwrite.csv(data, 'data/dataset.csv', row.names = FALSE)\nsaveRDS(data, 'data/dataset.Rds') #Choose a different file. \n\n\n\nImport data\n\ncsv_file = read.csv(\"data/dataset.csv\")\nrds_file = readRDS(\"data/dataset.Rds\")\n\n\n\n\n\n\n\nINE. 2018. “Mobilidade e Funcionalidade Do Território Nas Áreas Metropolitanas do Porto e de Lisboa: 2017.” Lisboa: Instituto National de Estatística. https://www.ine.pt/xportal/xmain?xpid=INE&xpgid=ine_publicacoes&PUBLICACOESpub_boui=349495406&PUBLICACOESmodo=2&xlang=pt.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R basics</span>"
    ]
  },
  {
    "objectID": "spatial-data.html",
    "href": "spatial-data.html",
    "title": "9  Introduction to spatial data",
    "section": "",
    "text": "9.1 Import vector data\nDownload and open Municipalities_geo.gpkg under EITcourse/data repository.\nWithin the sf package, we use the st_read() to read spatial features.\nCode\nMunicipalities_geo = st_read(\"data/Municipalities_geo.gpkg\")",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to spatial data</span>"
    ]
  },
  {
    "objectID": "spatial-data.html#import-vector-data",
    "href": "spatial-data.html#import-vector-data",
    "title": "9  Introduction to spatial data",
    "section": "",
    "text": "You can also open directly from url from github. Example:\nurl = \"https://github.com/U-Shift/EITcourse/raw/main/data/Municipalities_geo.gpkg\" Municipalities_geo = st_read(url)\n\n\n\n\n9.1.1 Projected vs Geographic Coordinate Systems\nA projected coordinate system is a flat representation of the Earth’s surface. A geographic coordinate system is a spherical representation of the Earth’s surface.\n\n\n\nSource: ESRI\n\n\nThe st_crs() function can be used to check the coordinate reference system of a spatial object.\n\n\nCode\nst_crs(Municipalities_geo)\n\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        MEMBER[\"World Geodetic System 1984 (G2296)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\nWGS84 is the most common geographic coordinate system, used in GPS, and EPSG:4326 is code for it.\nIf we want to project the data to a projected coordinate system, to use metric units instead of degrees, we can use the st_transform() function.\nIn this case, the EPGS:3857 is the code for the Pseudo-Mercator coordinate system.\n\n\nCode\nMunicipalities_projected = st_transform(Municipalities_geo, crs = 3857)\n\n\nNow see the differences when calling Municipalities_geo and Municipalities_projected.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to spatial data</span>"
    ]
  },
  {
    "objectID": "spatial-data.html#join-geometries-to-data-frames",
    "href": "spatial-data.html#join-geometries-to-data-frames",
    "title": "9  Introduction to spatial data",
    "section": "9.2 Join geometries to data frames",
    "text": "9.2 Join geometries to data frames\nImport TRIPSmun.Rds file and check data class\n\n\nCode\nTRIPSmun = readRDS(\"../data/TRIPSmun.Rds\")\nclass(TRIPSmun)\n\n\n[1] \"data.frame\"\n\n\nCode\nclass(Municipalities_geo)\n\n\n[1] \"sf\"         \"data.frame\"\n\n\nTo join the geometries from the Municipalities_geo to the data frame, we can use the left_join() function from the dplyr package.\n\n\nCode\nTRIPSgeo =\n  TRIPSmun |&gt; \n  left_join(Municipalities_geo)\n\nclass(TRIPSgeo)\n\n\n[1] \"data.frame\"\n\n\nAs you can see, this does not make the object a spatial feature. To do this, we need to use the st_as_sf() function.\n\n\nCode\nTRIPSgeo = TRIPSgeo |&gt; st_as_sf()\nclass(TRIPSgeo)\n\n\n[1] \"sf\"         \"data.frame\"\n\n\nNow we have a spatial feature with the data frame.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to spatial data</span>"
    ]
  },
  {
    "objectID": "spatial-data.html#create-spatial-data-from-coordinates",
    "href": "spatial-data.html#create-spatial-data-from-coordinates",
    "title": "9  Introduction to spatial data",
    "section": "9.3 Create spatial data from coordinates",
    "text": "9.3 Create spatial data from coordinates\nThe st_as_sf() function can also be used to create a spatial feature from a data frame with coordinates. In that case, we need to specify the columns with the coordinates.\nWe will use survey data (in .txt) with the participants’ home latitude/longitude coordinates to create a spatial feature.\n\n\nCode\nSURVEY = read.csv(\"../data/SURVEY.txt\", sep = \"\\t\") # tab delimiter\nclass(SURVEY)\n\n\n[1] \"data.frame\"\n\n\nCode\nSURVEYgeo = st_as_sf(SURVEY, coords = c(\"lon\", \"lat\"), crs = 4326) # create spatial feature\nclass(SURVEYgeo)\n\n\n[1] \"sf\"         \"data.frame\"\n\n\nWe can also set the crs of the spatial feature on the fly.\nCheck the differences between both data variables.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to spatial data</span>"
    ]
  },
  {
    "objectID": "spatial-data.html#visuzlize-spatial-data",
    "href": "spatial-data.html#visuzlize-spatial-data",
    "title": "9  Introduction to spatial data",
    "section": "9.4 Visuzlize spatial data",
    "text": "9.4 Visuzlize spatial data\nRepresent Transport Zones with Total and Car, using plot().\n\n\nCode\nplot(TRIPSgeo) # all variables\n\n\n\n\n\n\n\n\n\nCode\nplot(TRIPSgeo[\"Municipality\"])\n\n\n\n\n\n\n\n\n\nCode\nplot(TRIPSgeo[\"Total\"])\n\n\n\n\n\n\n\n\n\nCode\nplot(TRIPSgeo[\"Car\"])\n\n\n\n\n\n\n\n\n\nCode\n# plot pointy data\nplot(SURVEYgeo)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the next chapter we will learn how to create interactive maps.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to spatial data</span>"
    ]
  },
  {
    "objectID": "spatial-data.html#export-spatial-data",
    "href": "spatial-data.html#export-spatial-data",
    "title": "9  Introduction to spatial data",
    "section": "9.5 Export spatial data",
    "text": "9.5 Export spatial data\nYou can save your spatial data in different formats using the function st_write(), such as shapefiles (ESRI), GeoJSON, and GeoPackage.\nThis is also useful to convert spatial data between formats.\n\n\nCode\nst_write(TRIPSgeo, \"data/TRIPSgeo.gpkg\") # as geopackage\nst_write(TRIPSgeo, \"data/TRIPSgeo.shp\") # as shapefile\nst_write(TRIPSgeo, \"data/TRIPSgeo.geojson\") # as geojson\nst_write(TRIPSgeo, \"data/TRIPSgeo.csv\", layer_options = \"GEOMETRY=AS_WKT\") # as csv, with WKT geometry\n\n\n\n\n\n\n\n\nIf you already have a file with the same name, you can use the delete_dns = TRUE argument to overwrite it.\n\n\n\n\n\n\n\n\n\nPebesma, Edzer, and Roger Bivand. 2023. Spatial Data Science: With Applications in R. Boca Raton: Chapman; Hall/CRC. https://doi.org/10.1201/9780429459016.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to spatial data</span>"
    ]
  },
  {
    "objectID": "interactive-maps.html",
    "href": "interactive-maps.html",
    "title": "15  Interactive maps",
    "section": "",
    "text": "15.1 Mapview\nMapview allows to create quick interactive maps, only by declaring the function mapview().\nCode\nlibrary(mapview)\nmapview(TRIPSgeo)\nTo color the points by a variable, you can use the zcol argument.\nCode\nmapview(TRIPSgeo, zcol = \"Total\")\nAs you can see, a color palette is automatically assigned to the continuous variable.\nTry to use a categorical variable.\nCode\nmapview(TRIPSgeo,\n        zcol = \"Municipality\", # depending on the variable\n        alpha.regions = 0.4, # also add transparency\n        color = \"white\" # border color\n        )\nYou can go crazy with all the options that mapview offers. Please refer to the documentation to see all the options.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Interactive maps</span>"
    ]
  },
  {
    "objectID": "interactive-maps.html#mapview",
    "href": "interactive-maps.html#mapview",
    "title": "15  Interactive maps",
    "section": "",
    "text": "Note that you can change the basemap, and click on the geometries to see the data associated with them.\n\n\n\n\n\n\n15.1.1 Export\nYou can directly export the map as an html file or image, using the Viewer panel.\n\n\n\n\n\n\n\nThis is the most straightforward solution.\n\n\n\nYou can also export a map as an html file or image using code.\n\n\nCode\n# install.packages(\"webshot2\") # you will need this\n\nmap = mapview(TRIPSgeo, zcol = \"Total\") # fisrt create a objet with the desired map\n\nmapshot2(map, \"data/map.html\") # as webpage\nmapshot2(map, file = \"data/map.png\") # as image",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Interactive maps</span>"
    ]
  },
  {
    "objectID": "interactive-maps.html#rmarkdown",
    "href": "interactive-maps.html#rmarkdown",
    "title": "15  Interactive maps",
    "section": "15.2 Rmarkdown",
    "text": "15.2 Rmarkdown\nTo include a map on a report, website, paper (any type), you can create an Rmarkdown file.\n\nAnd include a R code chunk (ctrl + alt + i) with a map. If the output is html, you will get an interactive map on your document!",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Interactive maps</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Engel, Claudia A. 2023. Introduction to r. cengel.github.io/R-intro/.\n\n\nINE. 2018. “Mobilidade e Funcionalidade Do Território Nas Áreas Metropolitanas do Porto e de Lisboa:\n2017.” Lisboa: Instituto National de Estatística. https://www.ine.pt/xportal/xmain?xpid=INE&xpgid=ine_publicacoes&PUBLICACOESpub_boui=349495406&PUBLICACOESmodo=2&xlang=pt.\n\n\n———. 2022. “Censos 2021- XVI Recenseamento\nGeral da População. VI Recenseamento Geral da Habitação.”\nLisboa: Instituto National de Estatística. https://censos.ine.pt/xurl/pub/65586079.\n\n\nLovelace, Robin, and Richard Ellison. 2018. “Stplanr: A Package\nfor Transport Planning.” The R Journal 10\n(2): 10. https://doi.org/10.32614/RJ-2018-053.\n\n\nLovelace, Robin, Rosa Félix, and Dustin Carlino. 2022. “Jittering:\nA Computationally Efficient Method for Generating Realistic Route\nNetworks from Origin-Destination Data.” Findings, April.\nhttps://doi.org/10.32866/001c.33873.\n\n\nLovelace, Robin, and Malcolm Morgan. 2024. Od: Manipulate and Map\nOrigin-Destination Data. https://github.com/itsleeds/od.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2024.\nGeocomputation with r. Second. Chapman; Hall/CRC. https://r.geocompx.org/.\n\n\nPebesma, Edzer, and Roger Bivand. 2023. Spatial Data Science: With\nApplications in R. Boca Raton: Chapman; Hall/CRC. https://doi.org/10.1201/9780429459016.\n\n\nPereira, Rafael H. M., Marcus Saraiva, Daniel Herszenhut, Carlos Kaue\nVieira Braga, and Matthew Wigginton Conway. 2021. “R5r: Rapid\nRealistic Routing on Multimodal Transport Networks with r5 in\nr.” Findings, March. https://doi.org/10.32866/001c.21262.\n\n\nPereira, Rafael HM, and Daniel Herszenhut. 2023. Introduction to\nUrban Accessibility: A Practical Guide with r. Instituto de\nPesquisa Econômica Aplicada (Ipea). https://ipeagit.github.io/intro_access_book/.\n\n\nZomorrodi, Ryan. 2024. Centr: Weighted and Unweighted Spatial\nCenters. https://ryanzomorrodi.github.io/centr/.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "data-manipulation.html",
    "href": "data-manipulation.html",
    "title": "4  Data manipulation",
    "section": "",
    "text": "4.1 Select variables\nHave a look at your dataset. You can open using View(), look at the information at the “Environment” panel, or even print the same information using glimpse()\nglimpse(TRIPS)\nWe will create a new dataset with Origin, Walk, Bike and Total. This time we will use the select() function.\nTRIPS_new = select(TRIPS, Origin, Walk, Bike, Total) # the first argument is the dataset\nThe first argument, as usually in R, is the dataset, and the remaining ones are the columns to select.\nWith most of the dplyr functions you don’t need to refer to data$... you can simply type the variable names (and even without the \"...\"!). This makes coding in R simpler :)\nYou can also remove columns that you don’t need.\nTRIPS_new = select(TRIPS_new, -Total) # dropping the Total column",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html#select-variables",
    "href": "data-manipulation.html#select-variables",
    "title": "4  Data manipulation",
    "section": "",
    "text": "4.1.1 Using pipes!\nNow, let’s introduce pipes. Pipes are a rule as: “With this, do this.”\nThis is useful to skip the first argument of the functions (usually the dataset to apply the function).\nApplying a pipe to the select() function, we can write as:\n\nTRIPS_new = TRIPS |&gt; select(Origin, Walk, Bike, Total)\n\nTwo things to note:\n\nThe pipe symbol can be written as |&gt; or %&gt;%. 2 To write it you may also use the ctrl+shift+m shortcut.\nAfter typing select( you can press tab and the list of available variables of that dataset will show up! Enter to select. With this you prevent typo errors.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html#filter-observations",
    "href": "data-manipulation.html#filter-observations",
    "title": "4  Data manipulation",
    "section": "4.2 Filter observations",
    "text": "4.2 Filter observations\nYou can filter observations based on a condition using the filter() function.\n\nTRIPS2 = TRIPS[TRIPS$Total &gt; 25000,] # using r-base, you cant forget the comma\nTRIPS2 = TRIPS2 |&gt; filter(Total &gt; 25000) # using dplyr, it's easier\n\nYou can have other conditions inside the condition.\n\nsummary(TRIPS$Total)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    361    5918   17474   22457   33378  112186 \n\nTRIPS3 = TRIPS |&gt; filter(Total &gt; median(Total)) \n\nOther filter conditions:\n\n== equal, != different\n&lt; smaller, &gt; greater, &lt;= smaller or equal, &gt;= greater or equal\n& and, | or\nis.na, !is.na is not NA\n%in%, !%in% not in",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html#create-new-variables",
    "href": "data-manipulation.html#create-new-variables",
    "title": "4  Data manipulation",
    "section": "4.3 Create new variables",
    "text": "4.3 Create new variables\nYou can also try again to create a variable of Car percentage using pipes! To create a new variable or change an existing one (overwriting), you can use the mutate() function.\n\nTRIPS$Car_perc = TRIPS$Car/TRIPS$Total * 100 # using r-base\n\nTRIPS = TRIPS |&gt; mutate(Car_perc = Car/Total * 100) # using dplyr",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html#change-data-type",
    "href": "data-manipulation.html#change-data-type",
    "title": "4  Data manipulation",
    "section": "4.4 Change data type",
    "text": "4.4 Change data type\nData can be in different formats. For example, the variable Origin is a character, but we can convert it to a numeric variable.\n\nclass(TRIPS$Origin)\n\n[1] \"character\"\n\nTRIPS = TRIPS |&gt; \n  mutate(Origin_num = as.integer(Origin)) # you can use as.numeric() as well\nclass(TRIPS$Origin_num)\n\n[1] \"integer\"\n\n\nMost used data types are:\n\ninteger (int)\nnumeric (num)\ncharacter (chr)\nlogical (logical)\ndate (Date)\nfactor (factor)\n\n\n4.4.1 Factors\nFactors are useful to deal with categorical data. You can convert a character to a factor using as.factor(), and also use labels and levels for categorical ordinal data.\nWe can change the Lisbon variable to a factor, and Internal too.\n\nTRIPS = TRIPS |&gt; \n  mutate(Lisbon_factor = factor(Lisbon, labels = c(\"No\", \"Yes\")),\n         Internal_factor = factor(Internal, labels = c(\"Inter\", \"Intra\")))\n\nBut how do we know which levels come first? A simple way is to use table() or unique() functions.\n\nunique(TRIPS$Lisbon) # this will show all the different values\n\n[1] 0 1\n\ntable(TRIPS$Lisbon) # this will show the frequency of each value\n\n\n  0   1 \n188  48 \n\ntable(TRIPS$Lisbon_factor)\n\n\n No Yes \n188  48 \n\n\nThe first number to appear is the first level, and so on.\nYou can see the difference between using a continuous variable (in this case Lisbon` has 0 and 1) and a categorical variable (Lisbon_factor).\n\nplot(TRIPS$Lisbon) # the values range between 0 and 1\n\n\n\n\n\n\n\nplot(TRIPS$Lisbon_factor) # the values are categorical and labeled with Yes/No",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html#join-data-tables",
    "href": "data-manipulation.html#join-data-tables",
    "title": "4  Data manipulation",
    "section": "4.5 Join data tables",
    "text": "4.5 Join data tables\nWhen having relational tables - i.e. with a common identifier - it is useful to be able to join them in a very efficient way.\nleft_join is a function that joins two tables by a common column. The first table is the one that will be kept, and the second one will be joined to it. How left_join works:\n\n\n\n\n\n\n\nA visual representation of the left join where every row in x appears in the output.Source: R for Data Science.\n\n\nLet’s join the municipalities to this table with a supporting table that includes all the relation between neighbourhoods and municipalities, and the respective names and codes.\n\nMunicipalities = readRDS(\"data/Municipalities_names.Rds\")\n\n\nhead(TRIPS)\n\n# A tibble: 6 × 13\n  Origin Total  Walk  Bike   Car PTransit Other Internal Lisbon Car_perc\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n1 110501 35539 11325  1309 21446     1460     0        0      0     60.3\n2 110501 47602  3502   416 37727     5519   437        1      0     79.3\n3 110506 37183 12645    40 22379     2057    63        0      0     60.2\n4 110506 42313  1418   163 37337     3285   106        1      0     88.2\n5 110507 30725  9389  1481 19654      201     0        0      0     64.0\n6 110507 54586  2630   168 44611     6963   215        1      0     81.7\n# ℹ 3 more variables: Origin_num &lt;int&gt;, Lisbon_factor &lt;fct&gt;,\n#   Internal_factor &lt;fct&gt;\n\ntail(Municipalities)\n\n    Mun_code Neighborhood_code        Municipality\n113     1109            110913               Mafra\n114     1114            111409 Vila Franca de Xira\n115     1109            110918               Mafra\n116     1109            110904               Mafra\n117     1502            150202           Alcochete\n118     1109            110911               Mafra\n                                             Neighborhood\n113                                         Santo Isidoro\n114                                   Vila Franca de Xira\n115 União das freguesias de Azueira e Sobral da Abelheira\n116                                            Encarnação\n117                                               Samouco\n118                                             Milharado\n\n\nWe can see that we have a common variable: Origin in TRIPS and Neighborhood_code in Municipalities.\nTo join these two tables we need to specify the common variable in each table, using the by argument.\n\nTRIPSjoin = TRIPS |&gt; left_join(Municipalities, by = c(\"Origin\" = \"Neighborhood_code\"))\n\nIf you prefer, you can mutate or rename a variable so both tables have the same name. When both tables have the same name, you don’t need to specify the by argument.\n\nMunicipalities = Municipalities |&gt; rename(Origin = \"Neighborhood_code\") # change name\nTRIPSjoin = TRIPS |&gt; left_join(Municipalities) # automatic detects common variable\n\nAs you can see, both tables don’t need to be the same length. The left_join function will keep all the observations from the first table, and join the second table to it. If there is no match, the variables from the second table will be filled with NA.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html#group_by-and-summarize",
    "href": "data-manipulation.html#group_by-and-summarize",
    "title": "4  Data manipulation",
    "section": "4.6 group_by and summarize",
    "text": "4.6 group_by and summarize\nWe have a very large table with all the neighbourhoods and their respective municipalities. We want to know the total number of trips with origin in each municipality.\nTo make it easier to understand, let’s keep only the variables we need.\n\nTRIPSredux = TRIPSjoin |&gt; select(Origin, Municipality, Internal, Car, Total)\nhead(TRIPSredux)\n\n# A tibble: 6 × 5\n  Origin Municipality Internal   Car Total\n  &lt;chr&gt;  &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 110501 Cascais             0 21446 35539\n2 110501 Cascais             1 37727 47602\n3 110506 Cascais             0 22379 37183\n4 110506 Cascais             1 37337 42313\n5 110507 Cascais             0 19654 30725\n6 110507 Cascais             1 44611 54586\n\n\nWe can group this table by the Municipality variable and summarize the number of trips with origin in each municipality.\n\nTRIPSsum = TRIPSredux |&gt; \n  group_by(Municipality) |&gt; # you won't notice any chagne with only this\n  summarize(Total = sum(Total))\nhead(TRIPSsum)\n\n# A tibble: 6 × 2\n  Municipality   Total\n  &lt;chr&gt;          &lt;dbl&gt;\n1 Alcochete      36789\n2 Almada        289834\n3 Amadora       344552\n4 Barreiro      133658\n5 Cascais       373579\n6 Lisboa       1365111\n\n\nWe summed the total number of trips in each municipality.\nIf we want to group by more than one variable, we can add more group_by() functions.\n\nTRIPSsum2 = TRIPSredux |&gt; \n  group_by(Municipality, Internal) |&gt; \n  summarize(Total = sum(Total),\n            Car = sum(Car))\nhead(TRIPSsum2)\n\n# A tibble: 6 × 4\n# Groups:   Municipality [3]\n  Municipality Internal  Total    Car\n  &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Alcochete           0  16954   9839\n2 Alcochete           1  19835  15632\n3 Almada              0 105841  49012\n4 Almada              1 183993 125091\n5 Amadora             0 117727  33818\n6 Amadora             1 226825 142386\n\n\nWe summed the total number of trips and car trips in each municipality, separated by inter and intra municipal trips.\n\n\n\n\n\n\nIt is a good practice to use the ungroup() function after the group_by() function. This will remove the grouping. If you don’t do this, the grouping will be kept and you may have unexpected results in the next time you use that dataset.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html#arrange-data",
    "href": "data-manipulation.html#arrange-data",
    "title": "4  Data manipulation",
    "section": "4.7 Arrange data",
    "text": "4.7 Arrange data\nYou can sort a dataset by one or more variables.\nFor instance, arrange() by Total trips, ascending or descending order.\n\nTRIPS2 = TRIPSsum2 |&gt; arrange(Total)\nTRIPS2 = TRIPSsum2 |&gt; arrange(-Total) # descending\n\nTRIPS2 = TRIPSsum2 |&gt; arrange(Municipality) # alphabetic\n\nTRIPS4 = TRIPS |&gt; arrange(Lisbon_factor, Total) # more than one variable\n\nThis is not the same as opening the view table and click on the arrows. When you do that, the order is not saved in the dataset. If you want to save the order, you need to use the arrange() function.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html#all-together-now",
    "href": "data-manipulation.html#all-together-now",
    "title": "4  Data manipulation",
    "section": "4.8 All together now!",
    "text": "4.8 All together now!\nThis is the pipes magic. It takes the last result and applies the next function to it. “With this, do this.”. You can chain as many functions as you want.\n\nTRIPS_pipes = TRIPS |&gt; \n  select(Origin, Internal, Car, Total) |&gt; \n  \n  mutate(Origin_num = as.integer(Origin)) |&gt; \n  mutate(Internal_factor = factor(Internal, labels = c(\"Inter\", \"Intra\"))) |&gt; \n  \n  filter(Internal_factor == \"Inter\")|&gt;\n  \n  left_join(Municipalities) |&gt;\n  \n  group_by(Municipality) |&gt;\n  summarize(Total = sum(Total),\n            Car = sum(Car),\n            Car_perc = Car/Total * 100) |&gt; \n  ungroup() |&gt; \n  \n  arrange(desc(Car_perc))\n\nWith this code we will have a table with the total number of intercity trips, by municipality, with their names instead of codes, arranged by the percentage of car trips.\n\nTRIPS_pipes\n\n# A tibble: 18 × 4\n   Municipality         Total    Car Car_perc\n   &lt;chr&gt;                &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 Mafra                65811  46329     70.4\n 2 Sesimbra             49370  31975     64.8\n 3 Cascais             161194  96523     59.9\n 4 Palmela              66428  39688     59.7\n 5 Alcochete            16954   9839     58.0\n 6 Setúbal             129059  70318     54.5\n 7 Montijo              57164  30900     54.1\n 8 Seixal              120747  63070     52.2\n 9 Sintra              237445 123408     52.0\n10 Oeiras              134862  66972     49.7\n11 Almada              105841  49012     46.3\n12 Loures              132310  60478     45.7\n13 Barreiro             52962  24160     45.6\n14 Odivelas             93709  39151     41.8\n15 Vila Franca de Xira 115152  47201     41.0\n16 Moita                51040  17394     34.1\n17 Amadora             117727  33818     28.7\n18 Lisboa              280079  69038     24.6",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html#other-dplyr-functions",
    "href": "data-manipulation.html#other-dplyr-functions",
    "title": "4  Data manipulation",
    "section": "4.9 Other dplyr functions",
    "text": "4.9 Other dplyr functions\nYou can explore other dplyr functions and variations to manipulate data in the dplyr cheat sheet:\n\nTake a particular attention to pivot_wider and pivot_longer (tidyr package) to transform OD matrices in wide and long formats.\n\n\n\nOD matrix in long format\n\n\nOrigins\nDestinations\nTrips\n\n\n\n\nA\nB\n20\n\n\nA\nC\n45\n\n\nB\nA\n10\n\n\nC\nC\n5\n\n\nC\nA\n30\n\n\n\n\n\n\n\n\nOD matrix in wide format\n\n\nTrips\nA\nB\nC\n\n\n\n\nA\nNA\n20\n45\n\n\nB\n10\nNA\nNA\n\n\nC\n30\nNA\n5",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html#footnotes",
    "href": "data-manipulation.html#footnotes",
    "title": "4  Data manipulation",
    "section": "",
    "text": "This dataset includes the number of trips with origin in each neighborhood, divided by mode of transport, and inter or intra municipal trips.↩︎\nYou can change this in RStudio &gt; Tools &gt; Global Options &gt; Code.↩︎",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "5  Exploratory Data Analysis",
    "section": "",
    "text": "5.1 Intro\nThis chapter will show you how to use visualization and transformation to explore your data in a systematic way, a task that statisticians call exploratory data analysis, or EDA for short. EDA is an iterative cycle, where you should feel free to “fall in love” with your data.\nYou:\nSee Wickham, Çetinaka-Rundel, and Grolemund (2017) for more, in particular the EDA chapter.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#do-it-yourself-with-r",
    "href": "eda.html#do-it-yourself-with-r",
    "title": "5  Exploratory Data Analysis",
    "section": "",
    "text": "# EXPLORATORY DATA ANALYSIS\n\n# The database used in this example is a treated database from the Mobility Survey.   \n# executed for the metropolitan areas of Lisbon and Porto in 2018 (IMOB 2018).\n# We will only focus on trips within the metropolitan area of Lisbon. \n\n# Variables\n\n  #'*Origin_dicofre16* - Code of Freguesias as set by INE after 2016 (Distrito + Concelho + Freguesia)\n  #'*Total* - number of trips with origin in Origin_dicofre16\n  #'*Walk* - number of walking trips with origin in Origin_dicofre16\n  #'*Bike* - number of bike trips with origin in Origin_dicofre16\n  #'*Car* - number of car trips with origin in Origin_dicofre16. Includes taxi and motorcycle.\n  #'*PTransit* - number of Public Transit trips with origin in Origin_dicofre16\n  #'*Other* - number of other trips (truck, van, tractor, aviation) with origin in Origin_dicofre16\n  #'*Distance* - average trip distance (km) with origin in Origin_dicofre16\n  #'*Duration* - average trip duration (minutes) with origin in Origin_dicofre16\n  #'*Car_perc* - percentage of car trips with origin in Origin_dicofre16\n  #'*N_INDIVIDUOS* - number of residents in Origin_dicofre16 (Censos 2021)\n  #'*Male_perc* - percentage of male residents in Origin_dicofre16 (Censos 2021)\n  #'*IncomeHH* - average household income in Origin_dicofre16\n  #'*Nvehicles* - average number of car/motorcycle vehicles in the household in Origin_dicofre16\n  #'*DrivingLic* - percentage of car driving licence holders in Origin_dicofre16\n  #'*CarParkFree_Work* - percentage of respondents with free car parking at the work location, in Origin_dicofre16\n  #'*PTpass* - percentage of public transit monthly pass holders in Origin_dicofre16\n  #'*internal* - binary variable (factor). \"Yes\": internal trips in that freguesia (Origin_dicofre16), \"No\": external trips from that freguesia\n  #'*Lisboa* - binary variable (factor). \"Yes\": the freguesia is part of Lisbon municipality, \"No\": otherwise\n  #'*Area_km2* - area of in Origin_dicofre16, in km2\n\n# Initial Steps\n\n    # Install Libraries\n    #   For the first time, you will need to install some of the packages. \n    #     Step by step: \n \n    #       1. Go to Packages on the lower right display window and click install\n    #       2. Write the library you want to install and click \"install\"\n   \n    #           Or... `install.packages(\"readxl\",\"tidyverse\")` etc...\n \n    #       Depending on the version of your R, `DataExplorer` may need to be installed from source, such as\n\n    #   if (!require(devtools)) install.packages(\"devtools\")\n    #   devtools::install_github(\"boxuancui/DataExplorer\")\n\n\n    # Import Libraries\n\n      library(tidyverse) # Pack of most used libraries for data science\n      library(skimr) # Library used for providing a summary of the data\n      library(DataExplorer) # Library used in data science to perform exploratory data analysis\n      library(corrplot) # Library used for correlation plots\n\n\n# Import dataset\n \n      dataset &lt;- readRDS(\"data/IMOBmodel.Rds\")\n\n# Transform the dataset into a dataframe\n      df &lt;- data.frame(dataset)\n      \n# Take a look at the dataset\n      View(df)\n \n    # Check the summary statistics\n      summary(df)\n      \n    # Check the structure of the dataset \n      str(df)\n\n    # Take a first look at the dataset\n      head(df, 10)\n\n    # Check the type and class of the dataset\n      typeof(df)\n      class(df)\n\n\n#' *Note:* Most libraries work with dataframes. It is good practice to always transform the dataset to dataframe format.       \n\n# Compare the structure of the `dataset` with `df`\n        \n# Show summary statistics of the dataframe\n \n      skim(df)\n    \n# Identify missing data\n \n    # Is there missing data? How many?\n      table(is.na(df))\n\n    # Plot the percentage of missing data\n      plot_missing(df)\n      \n# Detect outliers  \n \n    # In order to detect outliers and do correlations (further in the exercise),it is necessary to have only continuous variables.\n    \n    # a) Create a new database only with continuous variables. \n\n      str(df)\n      names(df)\n      \n      df_continuous = df[,-c(1,18,19)]\n   \n      boxplot(df_continuous, las = 2)\n      \n      boxplot(df_continuous$Total, las = 1) #positively skewed\n\n    # b) Take out the outliers from the variable Total\n\n    # Create function \"outlier\"\n      outlier &lt;- function(x){\n        quant &lt;- quantile(x, probs=c(0.25, 0.75))  # calculates the Q1 and Q3\n        caps &lt;- quantile(x, probs=c(0.05, 0.95)) # calculates the 5th and 95th percentile\n        H &lt;- 1.5* IQR(x, na.rm = TRUE) # calculates interquartile range\n        x[x &lt; (quant[1] - H)] &lt;- caps[1]  # replace any values in x that are LESS than Q1 - 1.5 * IQR with the 5th percentile value\n        x[x &gt; (quant[2] + H)] &lt;- caps[2]  # # replace any values in x that are MORE than Q1 + 1.5 * IQR with the 95th percentile value\n        return(x)\n        }\n\n    # Assign the same database for df_outliers\n  \n      df_outliers = df_continuous\n  \n    # Replace the outliers in the variable Total\n  \n      df_outliers$Total = outlier(df_continuous$Total)\n\n    # c) Take a look again at the boxplots\n \n      boxplot(df_outliers, las = 2)\n\n        # Compare results of the dataset with and without the outliers  \n  \n          # Calculate the mean\n  \n            mean(df$Total)\n            mean(df_outliers$Total)\n\n          # Calculate the median\n            median(df$Total)\n            median(df_outliers$Total)\n\n          # standard deviation\n            var(df$Total)\n            var(df_outliers$Total)\n\n#' *Note:* There are many methods to treat outliers. This is just one of them.\n  # In the next lecture we will demonstrate other methods of detecting outliers such as the Cook distance and QQ plot.    \n\n# Histograms\n    \n      # a) Plot histograms of all the continuous variables\n      plot_histogram(df, ncol = 3) #with 3 columns\n      \n    # b) Check how other variables \n    #' How do the other variables behave regarding *Car_perc*\n    #' Plot boxplots of each independent variable with *Car_perc*\n\n      plot_boxplot(df, by = \"Car_perc\", ncol = 3)\n\n#' *Note*: If you increase the \"Car_perc\", it will decrease PTpass. Take a look at the relation with the other variables. \n#' \n\n# Correlations\n    \n    # Plot correlation heatmaps\n  \n#' *Note:* Correlations are only between continuous variables. \n  \n      res &lt;- cor.mtest(df_continuous, conf.level = .95) #store the results so you can call the p-value at the corrplot\n\n      corrplot(cor(df_continuous), p.mat = res$p, method = \"circle\", type = \"upper\", order=\"hclust\", sig.level = 0.05)\n\n#' *Note:* The pairwise correlations that are crossed are statistically insignificant. \n#  The null hypothesis is that correlation is zero. \n#  This means that the correlations are only significant when you reject the null hypothesis (pvalue &lt; 0.05).\n   \n#' See `?corrplot` for more options.  \n#  Try putting into method \"color\" or \"circle\", and see the difference.  \n\n    # Check the _pvalue_ of a crossed pair correlation: \n \n      cor.test(df_continuous$IncomeHH, df_continuous$Other)\n\n      cor.test(df_continuous$IncomeHH, df_continuous$Duration)\n\n# The default for `cor.test` is Pearson, two-sided, with a 95% confident level. Check `?cor.test` for more options.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "linear-regression.html",
    "href": "linear-regression.html",
    "title": "6  Multiple Linear Regression",
    "section": "",
    "text": "6.1 Load packages\nlibrary(tidyverse) # Pack of most used libraries for data science\nlibrary(skimr) # summary of the data\nlibrary(DataExplorer) # exploratory data analysis\nlibrary(corrplot) # correlation plots\n\nlibrary(car) # Testing autocorrelation (Durbin Watson)\nlibrary(olsrr) # Testing multicollinearity (VIF, TOL, etc.)",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#do-it-yourself-with-r",
    "href": "linear-regression.html#do-it-yourself-with-r",
    "title": "6  Multiple Linear Regression",
    "section": "",
    "text": "# Example exercise: Trip production of the Metropolitan Area of Lisbon.\n\n#' **Your task**: Estimate a linear regression model that predicts the car percentage per county.  \n \n# Variables:\n#' \n#'*Origin_dicofre16* - Code of Freguesias as set by INE after 2016 (Distrito + Concelho + Freguesia)\n#'*Total* - number of trips with origin in Origin_dicofre16\n#'*Walk* - number of walking trips with origin in Origin_dicofre16\n#'*Bike* - number of bike trips with origin in Origin_dicofre16\n#'*Car* - number of car trips with origin in Origin_dicofre16. Includes taxi and motorcycle.\n#'*PTransit* - number of Public Transit trips with origin in Origin_dicofre16\n#'*Other* - number of other trips (truck, van, tractor, aviation) with origin in Origin_dicofre16\n#'*Distance* - average trip distance (km) with origin in Origin_dicofre16\n#'*Duration* - average trip duration (minutes) with origin in Origin_dicofre16\n#'*Car_perc* - percentage of car trips with origin in Origin_dicofre16\n#'*N_INDIVIDUOS* - number of residents in Origin_dicofre16 (Censos 2021)\n#'*Male_perc* - percentage of male residents in Origin_dicofre16 (Censos 2021)\n#'*IncomeHH* - average household income in Origin_dicofre16\n#'*Nvehicles* - average number of car/motorcycle vehicles in the household in Origin_dicofre16\n#'*DrivingLic* - percentage of car driving licence holders in Origin_dicofre16\n#'*CarParkFree_Work* - percentage of respondents with free car parking at the work location, in Origin_dicofre16\n#'*PTpass* - percentage of public transit monthly pass holders in Origin_dicofre16\n#'*internal* - binary variable (factor). \"Yes\": internal trips in that freguesia (Origin_dicofre16), \"No\": external trips from that freguesia\n#'*Lisboa* - binary variable (factor). \"Yes\": the freguesia is part of Lisbon municipality, \"No\": otherwise\n#'*Area_km2* - area of in Origin_dicofre16, in km2\n \n  # Let's begin!\n \n    # Import Libraries\n      library(tidyverse) # Pack of most used libraries\n      library(skimr) # Library used for providing a summary of the data\n      library(DataExplorer) # Library used in data science to perform exploratory data analysis\n      library(corrplot) # Library used for correlation plots\n      library(car) # Library used for testing autocorrelation (Durbin Watson)\n      library(olsrr) # Library used for testing multicollinearity (VIF, TOL, etc.)\n      library(corrplot) # For correlation plots\n\n    # Import dataset and transform into dataframe\n        dataset = readRDS(\"data/IMOBmodel.Rds\")\n        \n        # Check the class\n          class(dataset)\n\n \n        # Transform the dataset into a dataframe\n          df = data.frame(dataset)\n\n#'*Assumption 1:* Dependent variable is continuous. \n    \n    # Show summary statistics\n      skim(df)\n      summary(df)\n\n    # Show boxplot\n      boxplot(df$Distance)\n      \n      summary(df$Distance)\n \n# Multiple Linear Regression\n#' Equation with `Car_perc` as the dependent variable:  \n\n# Checking assumptions\n  # Before running the model, you need to check if the assumptions are met.\n \n    # Linear relation\n      \n#'*Assumption 2:* There is a linear relationship between dependent variable (DV) and independent variables (IV)\n \n    par(mfrow=c(2,3)) #set plot area as 2 rows and 3 columns\n    \n    plot(x = df$Car_perc, y = df$Total, xlab = \"Car_perc (%)\", ylab = \"Total (number of trips)\")  \n    plot(x = df$Car_perc, y = df$Walk, xlab = \"Car_perc\", ylab = \"Walk\")  \n    plot(x = df$Car_perc, y = df$Bike, xlab = \"Car_perc\", ylab = \"Bike\")  \n    plot(x = df$Car_perc, y = df$Car, xlab = \"Car_perc\", ylab = \"Car\")  \n    plot(x = df$Car_perc, y = df$PTransit, xlab = \"Car_perc\", ylab = \"PTransit\")\n    plot(x = df$Car_perc, y = df$Other, xlab = \"Car_perc\", ylab = \"Other\")\n    plot(x = df$Car_perc, y = df$Distance, xlab = \"Car_perc\", ylab = \"Distance\")\n    plot(x = df$Car_perc, y = df$Duration, xlab = \"Car_perc\", ylab = \"Duration\")\n    plot(x = df$Car_perc, y = df$N_INDIVIDUOS, xlab = \"Car_perc\", ylab = \"N_INDIVIDUOS\")\n    plot(x = df$Car_perc, y = df$Male_perc, xlab = \"Car_perc\", ylab = \"Male_perc\")\n    plot(x = df$Car_perc, y = df$IncomeHH, xlab = \"Car_perc\", ylab = \"IncomeHH\")\n    plot(x = df$Car_perc, y = df$Nvehicles, xlab = \"Car_perc\", ylab = \"Nvehicles\")\n    plot(x = df$Car_perc, y = df$DrivingLic, xlab = \"Car_perc\", ylab = \"Driving License\")\n    plot(x = df$Car_perc, y = df$CarParkFree_Work, xlab = \"Car_perc\", ylab = \"Free car parking at work\")\n    plot(x = df$Car_perc, y = df$PTpass, xlab = \"Car_perc\", ylab = \"PTpass\")\n    plot(x = df$Car_perc, y = df$internal, xlab = \"Car_perc\", ylab = \"internal trips\")\n    plot(x = df$Car_perc, y = df$Lisboa, xlab = \"Car_perc\", ylab = \"Lisboa\")\n    plot(x = df$Car_perc, y = df$Area_km2, xlab = \"Car_perc\", ylab = \"Area_km2\")\n\n\n    #' Or you could execute a pairwise scatterplot matrix, that compares every variable with each other: \n   \n    pairs(df[,c(2:17,20)], pch = 19, lower.panel = NULL) #cannot put categorical and character variables in this function\n\n    #This function is not visible with many variables. \n    #Try reducing the size. \n\n    pairs(df[,c(2:10)], pch = 19, lower.panel = NULL)\n\n#'*Assumption 3:* The Dependent Variable should be normally distributed.  \n\n  #' Check the histogram of `Car_perc`\n\n    par(mfrow=c(1,1))\n    hist(df$Duration)\n  \n  # If the sample is smaller than 50 observations, use Shapiro-Wilk test: \n\n    shapiro.test(df$Car_perc)\n\n  # If not, use the Kolmogorov-Smirnov test\n \n    ks.test(df$Car_perc, \"pnorm\", mean=mean(df$Car_perc), sd = sd(df$Car_perc))\n\n#' The null hypothesis of both tests is that the distribution is normal. \n#' Therefore, for the distribution to be normal, the pvalue &gt; 0.05 and you should not reject the null hypothesis.\n\n#'* Multiple linear regression model*\n    \n#' Check the correlation plot before choosing the variables. \n\n\n  model &lt;- lm(Car_perc ~ Total +\n               Walk +\n               Bike +\n               Car +\n               PTransit +\n               Other +\n               Distance +\n               Duration +\n               N_INDIVIDUOS +\n               Male_perc +\n               IncomeHH +\n               Nvehicles +\n               DrivingLic +\n               CarParkFree_Work +\n               PTpass +\n               internal +\n               Lisboa +\n               Area_km2,\n              data = df)\nsummary(model)\n\n#'*Tip:* Use the function `names(df)` in the console to obtain the names of the variables.  \n#'*Tip:* Use ctrl+shift+c to comment a variable\n \n#' **Assessing the model**:\n \n#' 1. First check the **pvalue** and the **F statistics** of the model to see if there is any statistical relation \n#' between the dependent variable and the independent variables. \n#' If pvalue &lt; 0.05 and the F statistics &gt; Fcritical = 2,39, then the model is statistically acceptable.  \n\n#' 2. The **R-square** and **Adjusted R-square** evaluate the amount of variance that is explained by the model. \n#' The difference between one and another is that the R-square does not consider the number of variables.\n#' If you increase the number of variables in the model, the R-square will tend to increase which can lead to overfitting. \n#' On the other hand, the Adjusted R-square adjust to the number of independent variables. \n \n#' 3. Take a look at the **t-value** and the Pr(&gt;|t|). \n#' If the t-value &gt; 1,96 or Pr(&gt;|t|) &lt; 0,05, then the IV is statistically significant to the model.\n   \n#' 4. To analyze the **estimates** of the variables, you should first check the **signal** \n#' and evaluate if the independent variable has a direct or inverse relationship with the dependent variable. \n#' It is only possible to evaluate the **magnitude** of the estimate if all variables are continuous and standardized \n#' or by calculating the elasticities. The elasticities are explained and demonstrated in chapter 4. \n\n#' Residuals\n#' Check the following assumptions\n\n#' *Assumption 4:* The error (E) is independent across observations and the error variance is constant across IV – Homoscedasticity\n#' *Assumption 5:* Disturbances are approximately normally distributed\n\n#' * **Residuals vs Fitted:** This plot is used to detect non-linearity, heteroscedasticity, and outliers. \n\n#' **Normal Q-Q:** The quantile-quantile (Q-Q) plot is used to check if the disturbances follow a normal distribution.\n\n#' * **Scale-Location:** This plot is used to verify if the residuals are spread equally (homoscedasticity) or not \n#' (heteroscedasticity) through the sample. \n#' * **Residuals vs Leverage:** This plot is used to detect the impact of the outliers in the model. \n#' If the outliers are outside the Cook-distance, this may lead to serious problems in the model. \n \n#' Try analyzing the plots and check if the model meets the assumptions. \n    par(mfrow=c(2,2))\n    plot(model)\n\n \n#' *Assumption 6:* Non-autocorrelation of disturbances\n#' Execute the Durbin-Watson test to evaluate autocorrelation of the residuals\n    durbinWatsonTest(model)\n\n#' &gt; **Note:** In the Durbin-Watson test, values of the D-W Statistic vary from 0 to 4. \n#' If the values are from 1.8 to 2.2 this means that there is no autocorrelation in the model.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "factor-analysis.html",
    "href": "factor-analysis.html",
    "title": "7  Factor Analysis",
    "section": "",
    "text": "7.1 Do it yourself with R\nCopy the R code above in your session.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Factor Analysis</span>"
    ]
  },
  {
    "objectID": "factor-analysis.html#do-it-yourself-with-r",
    "href": "factor-analysis.html#do-it-yourself-with-r",
    "title": "7  Factor Analysis",
    "section": "",
    "text": "#' #### Example exercise: \"Residential location satisfaction in the Lisbon metropolitan area\"\n#' \n#' The aim of this study was to examine the perception of households towards their residential location considering several land use and accessibility factors as well as household socioeconomic and attitudinal characteristics.\n#' \n#' _Reference:_ Martinez, L. G., de Abreu e Silva, J., & Viegas, J. M. (2010). Assessment of residential location satisfaction in the Lisbon metropolitan area, TRB (No. 10-1161).\n#' \n#' **Your task:** Analyse the data and create meaningful latent factors.  \n#' \n#' ## Data\n#' #### Variables: \n#' \n#' * `DWELCLAS`: Classification of the dwelling;\n#' * `INCOME`: Income of the household;\n#' * `CHILD13`: Number of children under 13 years old;\n#' * `H18`: Number of household members above 18 years old;\n#' * `HEMPLOY`: Number of household members employed;\n#' * `HSIZE`: Household size;\n#' * `IAGE`: Age of the respondent;\n#' * `ISEX`: Sex of the respondent;\n#' * `NCARS`: Number of cars in the household;\n#' * `AREA`: Area of the dwelling;\n#' * `BEDROOM`: Number of bedrooms in the dwelling;\n#' * `PARK`: Number of parking spaces in the dwelling; \n#' * `BEDSIZE`: BEDROOM/HSIZE;\n#' * `PARKSIZE`: PARK/NCARS;\n#' * `RAGE10`: 1 if Dwelling age &lt;= 10;\n#' * `TCBD`: Private car distance in time to CBD;\n#' * `DISTTC`: Euclidean distance to heavy public transport system stops;\n#' * `TWCBD`: Private car distance in time of workplace to CBD;\n#' * `TDWWK`: Private car distance in time of dwelling to work place;\n#' * `HEADH`: 1 if Head of the Household;\n#' * `POPDENS`: Population density per hectare;\n#' * `EQUINDEX`: Number of undergraduate students/Population over 20 years old (500m)\n#' \n#' \n#' #### Rules of thumb: \n#' * At least 10 variables\n#' * n &lt; 50 (Unacceptable); n &gt; 200 (recommended)\n#' * It is recommended to use continuous variables. If your data contains categorical variables, you should transform them to dummy variables. \n#' \n#' #### Assumptions: \n#' * Normality; \n#' * linearity; \n#' * Homogeneity;\n#' * Homoscedasticity (some multicollinearity is desirable);\n#' * Correlations between variables &lt; 0.3 (not appropriate to use Factor Analysis)\n#' \n#' ## Let's start!\n#' #### Import Libraries\nlibrary(foreign) # Library used to read SPSS files\nlibrary(nFactors) # Library used for factor analysis\nlibrary(tidyverse) # Library used in data science to perform exploratory data analysis\nlibrary(summarytools) # Library used for checking the summary of the dataset\nlibrary(psych) # Library used for factor analysis\nlibrary(GPArotation) # Library used for factor analysis\n\n#' \n#' ### Get to know your dataset\n#' ##### Import dataset\n#' \ndf &lt;- read.spss(\"Data/example_fact.sav\", to.data.frame = T) #transforms a list into a data.frame directly\n\n#' \n#' ##### Take a look at the main characteristics of the dataset\n#' \nclass(df) #type of data\nstr(df)\n\n#' \n#' ##### Check summary statistics of variables\ndescriptive_stats &lt;- dfSummary(df)\nview(descriptive_stats)\n\n#' &gt; **Note:** I used a different library of the MLR chapter for perfoming the summary statistics. \"R\" allows you to do the same or similar tasks with different packages. \n#' \n#' ##### Take a look at the first values of the dataset\n#' \nhead(df,5)\n\n#' \n#' ##### Make ID as row names or case number\n#' \ndf&lt;-data.frame(df, row.names = 1)\n\n#' \n#' ### Evaluating the assumptions for factoral analysis\n#' \n#' Let's run a random regression model in order to evaluate some assumptions\n#' \nrandom = rchisq(nrow(df), 32)\nfake = lm(random ~ ., data = df)\nstandardized = rstudent(fake)\nfitted = scale(fake$fitted.values)\n\n#' \n#' * **Normality**\n#' \nhist(standardized)\n\n#' \n#' * **Linearity**\n#' \nqqnorm(standardized)\nabline(0,1)\n\n#' \n#' * **Homogeneity** \n#' \nplot(fitted, standardized)\nabline(h=0,v=0)\n\n#' \n#' * **Correlations between variables** \n#' Correlation matrix\ncorr_matrix &lt;- cor(df, method = \"pearson\")\n\n#' \n#' The **Bartlett** test examines if there is equal variance (homogeneity) between variables. Thus, it evaluates if there is any pattern between variables. Check for correlation adequacy - Bartlett's Test.\ncortest.bartlett(corr_matrix, n = nrow(df))\n\n#' &gt; **Note:** The null hypothesis is that there is no correlation between variables. Therefore, in factor analysis you want to reject the null hypothesis.   \n#' \n#' ##### Check for sampling adequacy - KMO test\n#' \nKMO(corr_matrix)\n\n#' \n#' &gt; **Note:** We want at least 0.7 of the overall Mean Sample Adequacy (MSA). If, 0.6 &lt; MSA &lt; 0.7, it is not a good value, but acceptable in some cases.\n#' \n#' ##### Determine the number of factors to extract\n#' \n#' **1. Parallel Analysis**\n#' \nnum_factors = fa.parallel(df, fm = \"ml\", fa = \"fa\")\n\n#' \n#' &gt; **Note:** `fm` = factor math; `ml` = maximum likelihood; `fa` = factor analysis\n#' \n#' The selection of the number of factors in the Parallel analysis can be threefold:  \n#' \n#' * Detect where there is an \"elbow\" in the graph;\n#' * Detect the intersection between the \"FA Actual Data\" and the \"FA Simulated Data\";\n#' * Consider the number of factors with eigenvalue &gt; 1.\n#' \n#' \n#' **2. Kaiser Criterion**\n#' \nsum(num_factors$fa.values &gt; 1) #Determines the number of factors with eigenvalue &gt; 1\n\n#' \n#' You can also consider factors with eigenvalue &gt; 0.7, since some of the literature indicate that this value does not overestimate the number of factors as much as considering an eigenvalue = 1. \n#' \n#' **3. Principal Component Analysis (PCA)**\n#' \n#' * Print variance that explains the components\ndf_pca &lt;- princomp(df, cor=TRUE) #cor = TRUE, standardizes your dataset before running a PCA\nsummary(df_pca)  \n\n#' \n#' * Scree Plot\nplot(df_pca,type=\"lines\", npcs = 31) \n\n#' \n#' &gt; **Note:** Check the cummulative variance of the first components and the scree plot, and see if the PCA is a good approach to detect the number of factors in this case. \n#' \n#' **PCA is not the same thing as Factor Analysis!** PCA only considers the common information (variance) of the variables, while factor analysis takes into account also the unique variance of the variable. Both approaches are often mixed up. In this example we use PCA as only a first criteria for choosing the number of factors. PCA is very used in image recognition and data reduction of big data. \n#' \n#' ## Exploratory Factor Analysis\n#' \n#' * **Model 1**: No rotation\n#' * **Model 2**: Rotation Varimax\n#' * **Model 3**: Rotation Oblimin\n\n# No rotation\ndf_factor &lt;- factanal(df, factors = 4, rotation = \"none\", scores=c(\"regression\"), fm = \"ml\")\n# Rotation Varimax\ndf_factor_var &lt;- factanal(df, factors = 4, rotation = \"varimax\", scores=c(\"regression\"), fm = \"ml\")\n# Rotiation Oblimin\ndf_factor_obl &lt;- factanal(df, factors = 4, rotation = \"oblimin\", scores=c(\"regression\"), fm = \"ml\")\n\n#' \n#' Let's print out the results of `df_factor_obl`, and have a look. \nprint(df_factor, digits=2, cutoff=0.3, sort=TRUE) #cutoff of 0.3 due to the sample size is higher than 350 observations.\n\n#' \n#' &gt; **Note:** \n#' The variability contained in the factors = Communality + Uniqueness.  \n#' Varimax assigns orthogonal rotation, and oblimin assigns oblique rotation.\n#' \n#' \n#' Plot factor 1 against factor 2, and compare the results of different rotations\n#' \n#' * **No Rotation**\nplot(\n  df_factor$loadings[, 1],\n  df_factor$loadings[, 2],\n  xlab = \"Factor 1\",\n  ylab = \"Factor 2\",\n  ylim = c(-1, 1),\n  xlim = c(-1, 1),\n  main = \"No rotation\"\n)\nabline(h = 0, v = 0)\nload &lt;- df_factor$loadings[, 1:2]\ntext(\n  load,\n  names(df),\n  cex = .7,\n  col = \"blue\"\n)\n\n#' \n#' * **Varimax rotation**\nplot(\n  df_factor_var$loadings[, 1],\n  df_factor_var$loadings[, 2],\n  xlab = \"Factor 1\",\n  ylab = \"Factor 2\",\n  ylim = c(-1, 1),\n  xlim = c(-1, 1),\n  main = \"Varimax rotation\"\n)\nabline(h = 0, v = 0)\nload &lt;- df_factor_var$loadings[, 1:2]\ntext(\n  load,\n  labels = names(df),\n  cex = .7,\n  col = \"red\"\n)\n\n#' \n#' \n#' * **Oblimin Rotation**\nplot(\n  df_factor_obl$loadings[, 1],\n  df_factor_obl$loadings[, 2],\n  xlab = \"Factor 1\",\n  ylab = \"Factor 2\",\n  ylim = c(-1, 1),\n  xlim = c(-1, 1),\n  main = \"Oblimin rotation\"\n)\nabline(h = 0, v = 0)\nload &lt;- df_factor_obl$loadings[, 1:2]\ntext(\n  load,\n  labels = names(df),\n  cex = .7,\n  col = \"darkgreen\"\n)\n\n#' \n#' When you have more than two factors it is difficult to analyse the factors by the plots. Variables that have low explaining variance in the two factors analyzed, could be highly explained by the other factors not present in the graph. However, try comparing the plots with the factor loadings and plot the other graphs to get more familiar with exploratory factor analysis.  \n#'",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Factor Analysis</span>"
    ]
  },
  {
    "objectID": "clusters.html",
    "href": "clusters.html",
    "title": "8  Cluster Analysis",
    "section": "",
    "text": "8.1 Do it yourself with R\nCopy the R code above in your session.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "clusters.html#do-it-yourself-with-r",
    "href": "clusters.html#do-it-yourself-with-r",
    "title": "8  Cluster Analysis",
    "section": "",
    "text": "#' \n#' #### Example exercise: Airports\n#' \n#' **Your task**: Create and evaluate the many types of clustering methods. \n#' \n#' #### Variables: \n#' \n#' * `Code`: Code of the airport;  \n#' * `Airport`: Name of the airport;  \n#' * `Ordem`: ID of the observations;  \n#' * `Passengers`: Number of passengers;  \n#' * `Movements`: Number of flights;  \n#' * `Numberofairlines`: Number of airlines in each airport;  \n#' * `Mainairlineflightspercentage`: Percentage of flights of the main airline of each airport;  \n#' * `Maximumpercentageoftrafficpercountry`: Maximum percentage of flights per country;  \n#' * `NumberofLCCflightsweekly`: Number of weekly low cost flights`;   \n#' * `NumberofLowCostAirlines`: Number of low cost airlines of each airport;  \n#' * `LowCostAirlinespercentage`: Percentage of the number of low cost airlines in each airport;  \n#' * `Destinations`: Number of flights arriving at each airport;  \n#' * `Average_route_Distance`: Average route distance in km;  \n#' * `DistancetoclosestAirport`: Distance to closest airport in km\n#' * `DistancetoclosestSimilarAirport`: Distance to closest similar airport in km;  \n#' * `AirportRegionalRelevance`: Relevance of the airport in a regional scale (0 - 1);  \n#' * `Distancetocitykm`: Distance between the airport and the city in km;  \n#' * `Inhabitantscorrected`: Population of the city;  \n#' * `numberofvisitorscorrected`: Number of vistors that arrived in the airport;  \n#' * `GDP corrected`: Corrected value of the Gross Domestic Product;  \n#' * `Cargoton`: Cargo ton. The total number of cargo transported in a certain period multiplied by the number o flights.\n#' \n#' ## Startup\n#' ##### Import Libraries\nlibrary(readxl) # Reading excel files\nlibrary(skimr) # Summary statistics\nlibrary(tidyverse) # Pack of useful tools\nlibrary(mclust) # Model based clustering\nlibrary(cluster) # Cluster analysis\nlibrary(factoextra) # Visualizing distances\n\n#' \n#' ##### Import dataset as a dataframe\ndataset &lt;- read_excel(\"data/Data_Aeroports_Clustersv1.xlsX\")\ndf &lt;- data.frame(dataset)\n\n#' \n#' ## Get to know your data\n#' ##### Summary statistics\nskim(df)\n\n#' ##### Sneak the plot\n#' Now let us plot an example and take a look\nplot(Numberofairlines ~ Destinations, df) #plot\ntext(Numberofairlines ~ Destinations, df, label = Airport, pos = 4, cex = 0.6) #labels over the previous plot\n\n#' \n#' By looking at the plot, you may already have a clue on the number of clusters with this two variabls. However, this is not clear and it does not consider the other variables in the analysis. \n#' \n#' ### Prepare data before performing a cluster analysis\n#' ##### Deal with missing data\ntable(is.na(df))\n\n#' In this example we do not have missing values. In case you do have in the future, you can take out the missing values with _listwise deletion_ (`df &lt;- na.omit(df)`) or use other ways of treating missing values.\n#' \n#' ##### Continuous variables\n#' Leave only continuous variables, and make `Ordem` as the row ID variable\ndf_reduced = df[,!(names(df) %in% c(\"Code\",\"Airport\"))]\ndf_reduced = data.frame(df_reduced, row.names = 1) #Ordem is the 1st variable in the df\n\n#' \n#' Take a look at the scale of the variables. See how they are different!\nhead(df_reduced)\n\n#' \n#' ##### Standardize variables\n#' Z-score standardization:  $(x_{i} - x_{\\text{mean}}) / {\\sigma}$\nmean &lt;- apply(df_reduced, 2, mean) # The \"2\" in the function is used to select the columns. MARGIN: c(1,2)\nsd &lt;- apply(df_reduced, 2, sd)\ndf_scaled &lt;- scale(df_reduced, mean, sd)\n\n#'   \n#' ## Hierarchical Clustering\n#' \n#' ##### Measuring Similarity through Euclidean distances\n#' \ndistance &lt;- dist(df_scaled, method = \"euclidean\")\n\n#' \n#' &gt; **Note:** There are other forms of distance measures that can be used such as:  \n#'   i) Minkowski distance; \n#'   ii) Manhattan distance; \n#'   iii) Mahanalobis distance.\n#'   \n#' ##### Visualize distances in heatmap\n#'\nfviz_dist(distance, gradient = list(low = \"#00AFBB\", mid = \"white\", high = \"#FC4E07\"), order = FALSE)\n\n#' \n#' #### Types of hierarchical clustering  \n#' There are many types of hierarchical clustering. Let's explore some.\n#' \n#' **1. Single linkage (nearest neighbor) clustering algorithm**\n#' \n#' Based on a bottom-up approach, by linking two clusters that have the closest distance between each other. \nmodels &lt;- hclust(distance, \"single\")\nplot(models, labels = df$Airport, xlab = \"Distance - Single linkage\", cex=0.6, hang = -1)\n\nrect.hclust(models, 4, border = \"purple\") # Visualize the cut on the dendogram, with 4 clusters\n\n#' \n#' \n#' **2. Complete linkage (Farthest neighbor) clustering algorithm**\n#' \n#' Complete linkage is based on the maximum distance between observations in each cluster.\nmodelc &lt;- hclust(distance, \"complete\")\nplot(modelc, labels = df$Airport, xlab = \"Distance - Complete linkage\", cex=0.6, hang = -1)\nrect.hclust(modelc, 4, border = \"blue\") \n\n#' \n#' **3. Average linkage between groups** \n#' \n#' The average linkage considers the distance between clusters to be the average of the distances between observations in one cluster to all the members in the other cluster. \nmodela &lt;- hclust(distance, \"average\")\nplot(modela, labels = df$Airport, xlab = \"Distance - Average linkage\", cex=0.6, hang = -1)\nrect.hclust(modelc, 4, border = \"red\")\n\n#' \n#' **4. Ward`s method**\n#'   \n#' The Ward`s method considers the measures of similarity as the sum of squares within the cluster summed over all variables. \nmodelw &lt;- hclust(distance, \"ward.D2\")\nplot(modelw, labels = df$Airport, xlab = \"Distance - Ward method\", cex=0.6, hang = -1)\nrect.hclust(modelw, 4, border = \"orange\")\n\n#' \n#' **5. Centroid method**\n#' \n#' The centroid method considers the similarity between two clusters as the distance between its centroids.\nmodelcen &lt;- hclust(distance, \"centroid\")\nplot(modelcen, labels = df$Airport, xlab = \"Distance - Centroid method\", cex=0.6, hang = -1)\nrect.hclust(modelcen, 4, border = \"darkgreen\")\n\n#' \n#' ##### Comparing results from different hierarchical methods\n#' Now lets evaluate the **membership** of each observation with the `cutree` function for each method.\n#' \nmember_single &lt;- cutree(models, 4)\nmember_com &lt;- cutree(modelc, 4)\nmember_av &lt;- cutree(modela, 4)\nmember_ward &lt;- cutree(modelw, 4)\nmember_cen &lt;- cutree(modelcen, 4)\n\n#'   \n#' \n#' Compare how common each method is to each other.\ntable(member_com, member_av) # compare the complete linkage with the average linkage\n\n#' \n#' &gt; **Note:** Try comparing other methods, and evaluate how common they are.   \n#'   \n#' ##### Silhouette Plots: evaluate which method is more appropriate \n#' The silhouette plot evaluates how similar an observation is to its own cluster compared to other clusters. The clustering configuration is appropriate when most objects have high values. Low or negative values indicate that the clustering method is not appropriate or the number of clusters is not ideal. \n#' \nplot(silhouette(member_single, distance))\nplot(silhouette(member_com, distance))\nplot(silhouette(member_av, distance))\nplot(silhouette(member_ward, distance))\nplot(silhouette(member_cen, distance))\n\n#' \n#' \n#' ## Non-Hirarchical Clustering \n#' \n#' ##### K-means clustering\n#' * k-means with n=3 clusters\nkm_clust &lt;- kmeans(df_scaled, 3)\nkm_clust #print the results\n\n#' \n#' * Other ways of setting the number of clusters  \n#' \n#' This algorithm will detect how many clusters from 1 to 10 explains more variance\n#' \n  k &lt;- list()\n  for(i in 1:10){\n    k[[i]] &lt;- kmeans(df_scaled, i)\n  }\n\n#'   \n#' &gt; **Note**: Try printing the k value and take a look at the ratio `between_SS` / `total_SS`. Evaluate how it varies when you add clusters. \n#' \n#' Now, lets  plot `between_SS` / `total_SS` into a scree plot\n#' \nbetSS_totSS &lt;- list()\nfor(i in 1:10){\nbetSS_totSS[[i]] &lt;- k[[i]]$betweenss/k[[i]]$totss\n}\nplot(1:10, betSS_totSS, type = \"b\", ylab = \"Between SS / Total SS\", xlab = \"Number of clusters\")\n\n#' \n#' \n#' Let take out the outliers and see the difference in the k-means clustering:\n#'  \n#' * **Examine the boxplots**\n#'   \npar(cex.axis=0.6, mar=c(11,2,1,1))# Make labels fit in the boxplot\nboxplot(df_scaled, las = 2) #labels rotated to vertical\n\n#' \n#' * **Detect the outliers** \n#' \noutliers &lt;- boxplot.stats(df_scaled)$out\noutliers\n\n#' \n#' * **Remove rows with outliers** \n#' \nnrow(df_scaled) #32\nout_ind &lt;- which(df_scaled %in% c(outliers)) #the row.names that contain outliers\ndf_no_outliers = df_scaled[-c(out_ind),] #remove those rows from the df_scaled\nnrow(df_no_outliers) #31\n\n#' \n#' &gt; **Note:** There are many methods to treat outliers. This is just one of them. Note that it is not very appropriate, since it removes many observations that are relevant for the analysis. Try using other methods and evaluate the difference.  \n#' \n#' \n#' Execute a k-means clustering with the dataset without the outliers and see the difference. \n#' \nkm_no_outliers &lt;- kmeans(df_no_outliers, 3)\nkm_no_outliers\n\n#' \n#' #### Ploting the clusters\n#' Finally, plotting the clusters results to check if they make sense.  \n#' Let us go back to first example and take a look. \n#' \n#' * **K-means with outliers**\n#' \nplot(Numberofairlines ~ Destinations, df, col = km_clust$cluster)\nwith(df, text(Numberofairlines ~ Destinations, label = Airport, pos = 1, cex = 0.6))\n\n#' \n#' * **K-means without outliers**\n#'\nplot(Numberofairlines ~ Destinations, df, col = km_no_outliers$cluster)\nwith(df, text(Numberofairlines ~ Destinations, label = Airport, pos = 1, cex = 0.6))",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "centroids.html",
    "href": "centroids.html",
    "title": "10  Centroids of transport zones",
    "section": "",
    "text": "10.1 Geometric centroids\nTaking the Municipalities_geo data from the previous section, we will calculate the geometric centroids, using the st_centroid() function.\nCode\nlibrary(dplyr)\nlibrary(sf)\nlibrary(mapview)\n\nMunicipalities_geo = st_read(\"../data/Municipalities_geo.gpkg\", quiet = TRUE)\n\nCentroid_geo = st_centroid(Municipalities_geo)\nThis creates points at the geometric center of each polygon.\nCode\nmapview(Centroid_geo)\n\n\n\n\n\n\nCode\nmapview(Centroid_geo) + mapview(Municipalities_geo, alpha.regions = 0) # both maps, with full transparency in polygons\nBut… is this the best way to represent the center of a transport zone?\nThese results may be biased by the shape of the polygons, and not represent where activities are. Example: lakes, forests, etc.\nTo overcome this, we can use weighted centroids.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Centroids of transport zones</span>"
    ]
  },
  {
    "objectID": "centroids.html#weighted-centroids",
    "href": "centroids.html#weighted-centroids",
    "title": "10  Centroids of transport zones",
    "section": "10.2 Weighted centroids",
    "text": "10.2 Weighted centroids\nWe will weight centroids of transport zones by population and by number of buildings.\nFor this, we will need the census data (INE 2022).\n\n\nCode\nCensus = st_read(\"../data/census.gpkg\", quiet = TRUE)\n\nmapview(Census |&gt; filter(Municipality == \"Lisboa\"), zcol = \"Population\")\n\n\n\n\n\n\nIt was not that easy to estimate weighted centroids with R, as it is with GIS software. But there is this new package centr that can help us (Zomorrodi 2024).\nWe need to specify the group we want to calculate the mean centroids, and the weight variable we want to use.\n\n\nCode\n# test\nlibrary(centr)\nCentroid_pop = Census |&gt; \n  mean_center(group = \"Municipality\", weight = \"Population\")\n\n\nWe can do the same for buildings.\n\n\nCode\nCentroid_build = Census |&gt; \n  mean_center(group = \"Municipality\", weight = \"Buildings\")",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Centroids of transport zones</span>"
    ]
  },
  {
    "objectID": "centroids.html#compare-centroids-in-a-map",
    "href": "centroids.html#compare-centroids-in-a-map",
    "title": "10  Centroids of transport zones",
    "section": "10.3 Compare centroids in a map",
    "text": "10.3 Compare centroids in a map\n\n10.3.1 Interactive map\n\n\nCode\nmapview(Centroid_geo, col.region = \"blue\") +\n  mapview(Centroid_pop, col.region = \"red\") +\n  mapview(Centroid_build, col.region = \"black\") +\n  mapview(Municipalities_geo, alpha.regions = 0) # polygon limits\n\n\n\n\n\n\nSee how the building, population and geometric centroids of Sintra are apart, from closer to Tagus, to the rural area.\n\n\n10.3.2 Static map\nTo produce the same map, using only plot() and st_geometry(), we need to make sure that the geometries have the same crs.\n\n\nCode\nst_crs(Centroid_geo) # 4326 WGS84\nst_crs(Centroid_pop) # 3763 Portugal TM06\n\n\nSo, we need to transform the geometries to the same crs.\n\n\nCode\nCentroid_pop = st_transform(Centroid_pop, crs = 4326)\nCentroid_build = st_transform(Centroid_build, crs = 4326)\n\n\nNow, to use plot() we incrementally add layers to the plot.\n\n\nCode\n# Plot the Municipalities_geo polygons first (with no fill)\nplot(st_geometry(Municipalities_geo), col = NA, border = \"black\")\n\n# Add the Centroids_geo points in blue\nplot(st_geometry(Centroid_geo), col = \"blue\", pch = 16, add = TRUE) # add!\n\n# Add the Centroid_pop points in red\nplot(st_geometry(Centroid_pop), col = \"red\", pch = 16, add = TRUE)\n\n# Add the Centroid_build points in black\nplot(st_geometry(Centroid_build), col = \"black\", pch = 16, add = TRUE)\n\n\n\n\n\nStatic map of different centroids of Municipalities\n\n\n\n\nIn the next section we will use these centroids to calculate the desire lines between them.\n\n\n\n\n\n\nINE. 2022. “Censos 2021- XVI Recenseamento Geral da População. VI Recenseamento Geral da Habitação.” Lisboa: Instituto National de Estatística. https://censos.ine.pt/xurl/pub/65586079.\n\n\nZomorrodi, Ryan. 2024. Centr: Weighted and Unweighted Spatial Centers. https://ryanzomorrodi.github.io/centr/.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Centroids of transport zones</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MQAT",
    "section": "",
    "text": "1 Introduction\nThis website aims to provide tools to deal with exploring and treating transportation datasets using R programming, an open-source and widely used tool for data analytics in urban mobility.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#mobility-data",
    "href": "index.html#mobility-data",
    "title": "MQAT",
    "section": "1.1 Mobility data",
    "text": "1.1 Mobility data\nThere is an emerging increase in mobility data, through new forms of technology, which result in very large and diverse datasets.\n\n\n\nE-Scooter trip data in Lisbon. How to deal with it?\n\n\nKnowing how to get, treat and analyze complex datasets with the up-to-date technologies is extremely relevant for academia, policy makers and start-ups, since it allows them to:\n\nacquire critical view on urban mobility based on data;\nspatially identify locations in the city that require policy priorities;\nand improve the efficiency of data analysis processes.\n\n\nWhy R and GIS\nMost academic programs focus on teaching modelling and deep analysis of data. However, there is a need to learn how to explore and prepare a dataset for modelling. The use of programming and GIS techniques have enormous advantages, including their flexibility; reproducibility; and transparency and understanding the step-by-step process.\nThe use of GIS techniques in transportation is, traditionally, not considered in transportation learning programs, despite being of enormous relevance when doing accessibility analysis or reeling with georreferenced transportation data, such as bike sharing route trips’ datasets, origin-destination flows datasets, home/work locations, GTFS public transit data, and so on. There is a need to learn how to locate these open datasets, how to explore them and how to integrate them into transportation and urban analysis. Additionally, the use of open source software and datasets allows researchers to perform methods that are reproducible and transparent.\n\nTLDR\n\nOpen-source tools widely used in data analytics and spatial analysis\nFlexibility and reproducibility in data manipulation and visualization\nCritical for urban mobility and transportation research, with spatial relevance\nLarge transportation datasets are becoming increasingly common",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#course-objectives",
    "href": "index.html#course-objectives",
    "title": "MQAT",
    "section": "1.2 Course objectives",
    "text": "1.2 Course objectives\n\nIntroduce R Programming Basics\n\nEquip participants with foundational skills in R programming\nEmphasize reproducible research practices to ensure transparency and replicability in analyses\n\n\n\nTeach Data Manipulation Techniques\n\nUse key R packages for data cleaning, manipulation, and summarization of datasets\nEnable participants to efficiently handle large and complex transportation datasets\n\n\n\nSpatial Data Visualization\n\nIntroduce methods for quick and effective spatial data visualization using R and GIS tools\nProvide hands-on experience with creating interactive maps and visualizations\n\n\n\nPerform Basic Spatial Analysis\n\nTeach participants how to perform spatial analysis of transportation datasets using GIS techniques with R\nCover practical applications such as georeferencing data, accessibility analysis, and routing ODs\nUtilize real-world transportation data for practical, hands-on learning",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#target-audience",
    "href": "index.html#target-audience",
    "title": "MQAT",
    "section": "1.3 Target audience",
    "text": "1.3 Target audience\n\nPh.D. candidates from DTN and other researchers\nPolicy makers and practitioners in urban mobility\nBeginners to intermediate R users, no prior experience needed",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#recommended-readings",
    "href": "index.html#recommended-readings",
    "title": "MQAT",
    "section": "Recommended readings",
    "text": "Recommended readings\n\nEngel (2023) Introduction to R.\nWickham, Çetinaka-Rundel, and Grolemund (2017) R for Data Science.\nLovelace, Nowosad, and Muenchow (2024) Geocomputation with R.\n\n\n\n\n\n\n\nEngel, Claudia A. 2023. Introduction to r. cengel.github.io/R-intro/.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2024. Geocomputation with r. 2nd ed. Chapman; Hall/CRC. https://r.geocompx.org/.\n\n\nWickham, Hadley, Mine Çetinaka-Rundel, and Garrett Grolemund. 2017. R for Data Science. 2nd ed. O’Reilly Sebastopol. https://r4ds.hadley.nz/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "2  Software",
    "section": "",
    "text": "2.1 R\nYou will need R installed on your computer. R stats (how it is also known) is a programming language and free software environment for statistical computing and graphics supported by the R Foundation for Statistical Computing.\nThe download links live at The Comprehensive R Archive Network (aka CRAN). he most recent version is 4.5.1, but you can use &gt;= 4.1.x if you already have it installed.\nAfter this installation, you don’t need to open R base. Please proceed to install RStudio.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Software</span>"
    ]
  },
  {
    "objectID": "software.html#r",
    "href": "software.html#r",
    "title": "2  Software",
    "section": "",
    "text": "WindowsMacUbuntu\n\n\nDownload R-4.5.1 for Windows and run the executable file.\n\n\n\n\n\n\nImportant\n\n\n\nYou will also need to install Rtools, which is a collection of tools necessary to build R packages in Windows.\n\n\n\n\nDownload R-4.5.1 for MacOX. You will have to choose between the arm64 or the x86-64 version.\nDownload the .pkg file and install it as usual.\n\n\n\nThese are instructions for Ubuntu. If you use other linux distribution, please follow the instructions on The Comprehensive R Archive Network - CRAN.\n\nYou can look for R in the Ubuntu Software Center or install it via the terminal:\n\n# sudo apt update && sudo apt upgrade -y\nsudo apt install r-base\n\nOr, if you prefer, you can install the latest version of R from CRAN:\n\n# update indices\nsudo apt update -qq\n# install two helper packages we need\nsudo apt install --no-install-recommends software-properties-common dirmngr\n# add the signing key (by Michael Rutter) for these repos\nwget -qO- https://cloud.r-project.org/bin/linux/ubuntu/marutter_pubkey.asc | sudo tee -a /etc/apt/trusted.gpg.d/cran_ubuntu_key.asc\n# add the R 4.0 repo from CRAN -- adjust 'focal' to 'groovy' or 'bionic' as needed\nsudo add-apt-repository \"deb https://cloud.r-project.org/bin/linux/ubuntu $(lsb_release -cs)-cran40/\"\n\nThen run:\n\nsudo apt install r-base r-base-core r-recommended r-base-dev\n\n[Optional] To keep up-to-date r version and packages, you can follow the instructions at r2u",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Software</span>"
    ]
  },
  {
    "objectID": "software.html#rstudio",
    "href": "software.html#rstudio",
    "title": "2  Software",
    "section": "2.2 RStudio",
    "text": "2.2 RStudio\nRStudio Desktop is an integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management.\nRStudio is available for free download from Posit RStudio.\n\nWindows 10/11MacOSUbuntu\n\n\nDownload RStudio 2025.05 and run the executable file.\n\n\nDownload RStudio 2025.05 and install it as usual.\n\n\n\nThese are instructions for Ubuntu 24. If you use other linux distribution, please follow the instructions on Posit RStudio.\n\nInstall it via the terminal:\n\nsudo apt install libssl-dev libclang-dev\nwget https://download1.rstudio.org/electron/jammy/amd64/rstudio-2025.05.1-513-amd64.deb\nsudo dpkg -i rstudio*\nrm -v rstudio*",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Software</span>"
    ]
  },
  {
    "objectID": "software.html#r-packages",
    "href": "software.html#r-packages",
    "title": "2  Software",
    "section": "2.3 R packages",
    "text": "2.3 R packages\nYou will need to install some packages to work with the data and scripts in this course.\nYou can install them in RStudio by searching for them in the Packages tab:\n\nor by running the following code in the console:\n\ninstall.packages(\"tidyverse\")\ninstall.packages(\"readxl\")\n\ninstall.packages(\"sf\")\ninstall.packages(\"mapview\")\ninstall.packages(\"rmarkdown\")\ninstall.packages(\"centr\")\ninstall.packages(\"od\")\ninstall.packages(\"openrouteservice\")\n\ninstall.packages(c(\"remotes\", \"devtools\", \"usethis\")) # optional\ninstall.packages(\"osmextract\") # optional\ninstall.packages(\"stplanr\") # optional",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Software</span>"
    ]
  },
  {
    "objectID": "software.html#r5r",
    "href": "software.html#r5r",
    "title": "2  Software",
    "section": "2.4 r5r",
    "text": "2.4 r5r\nThe workshop “A crash course on urban accessibility with R” uses a few R packages that need to be installed on your machine. The simplest way to do this is running the code below. This might take a few minutes if this is the first time you install these packages.\n\npkgs = c(\"r5r\", \"accessibility\", \"rJavaEnv\", \"h3jsr\", \"quantreg\")\n\ninstall.packages(pkgs)\n\n\n2.4.1 Java Development Kit\nTo use the {r5r} package (version v2.0 or higher), you will need to have Java Development Kit (JDK) 21 installed on your computer. There are numerous open-source JDK implementations. The easiest way to install JDK is using the new {rJavaEnv} package in R.\n\n# check version of Java currently installed (if any) \nrJavaEnv::java_check_version_rjava()\n\n## if this is the first time you use {rJavaEnv}, you might need to run this code\n## below to consent the installation of Java.\n# rJavaEnv::rje_consent(provided = TRUE)\n\n# install Java 21\nrJavaEnv::java_quick_install(\n  version = 21,\n  distribution = 'Corretto')\n\n# check if Java was successfully installed\nrJavaEnv::java_check_version_rjava()\n\nAlternatively, you can manually download and install JDK 21.\n\nWindows and MacOSUbuntu\n\n\nGo to Java Development Kit 21, download the latest 21 build corresponding to your operating system and run the executable file.\n\n\nInstall it via the terminal:\n\nsudo apt install -y openjdk-21-jdk openjdk-21-jre\njava -version",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Software</span>"
    ]
  },
  {
    "objectID": "software.html#tinytex",
    "href": "software.html#tinytex",
    "title": "2  Software",
    "section": "2.5 tinytex",
    "text": "2.5 tinytex\nTo be able to export your Rmakdown document to a pdf file, you will need a LaTeX processor. tinytex is a low weight R package with the most used features of LaTeX that you can use with R.\n\n# you need to run both lines\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()\n\nTo use some LaTeX packages, you also need to install tlmgr.\n\ntinytex::tlmgr_install()\ntinytex::tlmgr_update() # updates all latex pkgs",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Software</span>"
    ]
  },
  {
    "objectID": "software.html#footnotes",
    "href": "software.html#footnotes",
    "title": "2  Software",
    "section": "",
    "text": "We will use RStudio, although if you already use other studio such as VScode, that’s also fine.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Software</span>"
    ]
  },
  {
    "objectID": "desire-lines.html",
    "href": "desire-lines.html",
    "title": "11  OD pairs and desire lines",
    "section": "",
    "text": "11.1 Desire lines with od_to_sf\nTo create desire lines, we need a dataset with OD pairs and other dataset with the corresponding transport zones (spatial data).\nThe TRIPSmode.Rds dataset includes origins, destinations and number of trips between municipalities.\nCode\nTRIPSmode = readRDS(\"../data/TRIPSmode.Rds\")\nThe Municipalities_geo.gpkg dataset includes the geometry of the transport zones.\nCode\nlibrary(sf)\nMunicipalities_geo = st_read(\"../data/Municipalities_geo.gpkg\", quiet = TRUE) # supress mesage\nThen, we need to load the od package. We will use the od_to_sf() function to create desire lines from OD pairs.\nCode\n# install.packages(\"od\")\nlibrary(od)\n\nTRIPSdlines = od_to_sf(TRIPSmode, z = Municipalities_geo) # z for zones\nFor this magic to work smoothly, the first two columns of the TRIPSmode dataset must be the origin and destination zones, and these zones need to correspond to the first column of the Municipalities_geo dataset (with an associated geometry).\nNow we can visualize the desire lines using the mapview function.\nCode\nlibrary(mapview)\nmapview(TRIPSdlines, zcol = \"Total\")\nAs you can see, this is too much information to be able to understand the flows.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>OD pairs and desire lines</span>"
    ]
  },
  {
    "objectID": "desire-lines.html#desire-lines-with-od_to_sf",
    "href": "desire-lines.html#desire-lines-with-od_to_sf",
    "title": "11  OD pairs and desire lines",
    "section": "",
    "text": "See more options with the ?stplanr::od2line function.\n\n\n\n\n\n\n\n11.1.1 Filtering desire lines\nFilter intrazonal trips.\n\n\nCode\nlibrary(dplyr)\n\nTRIPSdlines_inter = TRIPSdlines |&gt; \n  filter(Origin != Destination) |&gt; # remove intrazonal trips\n  filter(Total &gt; 5000) # remove noise\n\nmapview(TRIPSdlines_inter, zcol = \"Total\", lwd = 5)\n\n\n\n\n\n\nFilter trips with origin or destination not in Lisbon.\n\n\nCode\nTRIPSdl_noLX = TRIPSdlines_inter |&gt; \n  filter(Origin != \"Lisboa\", Destination != \"Lisboa\")\n\nmapview(TRIPSdl_noLX, zcol = \"Total\", lwd = 8) # larger line width\n\n\n\n\n\n\nTry to replace the Total with other variables, such as Car, PTransit, and see the differences.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>OD pairs and desire lines</span>"
    ]
  },
  {
    "objectID": "desire-lines.html#oneway-desire-lines",
    "href": "desire-lines.html#oneway-desire-lines",
    "title": "11  OD pairs and desire lines",
    "section": "11.2 Oneway desire lines",
    "text": "11.2 Oneway desire lines\nNote that the od_to_sf() function creates bidirectional desire lines. This can be not the ideal for visualization / representation purposes, as you will have 2 lines overlaping.\nThe function od_oneway() aggregates oneway lines to produce bidirectional flows.\nBy default, it returns the sum of each numeric column for each bidirectional origin-destination pair.\n\n\nCode\nnrow(TRIPSdlines)\n\n\n[1] 315\n\n\nCode\nTRIPSdlines_oneway = od_oneway(TRIPSdlines) |&gt; \n  filter(o != d) # remove empty geometries\nnrow(TRIPSdlines_oneway)\n\n\n[1] 150\n\n\nNote that for the last municipalities you have less combinations now. Nevertheless, all the possible combinations are represented.\n\n\nCode\nhead(TRIPSdlines_oneway[,c(1,2)]) # just the first 2 columns\n\n\nSimple feature collection with 6 features and 2 fields\nAttribute-geometry relationships: identity (2)\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -9.229502 ymin: 38.62842 xmax: -8.915878 ymax: 38.75981\nGeodetic CRS:  WGS 84\n          o        d                       geometry\n1 Alcochete   Almada LINESTRING (-8.915878 38.73...\n2 Alcochete  Amadora LINESTRING (-8.915878 38.73...\n3    Almada  Amadora LINESTRING (-9.193061 38.63...\n4 Alcochete Barreiro LINESTRING (-8.915878 38.73...\n5    Almada Barreiro LINESTRING (-9.193061 38.63...\n6   Amadora Barreiro LINESTRING (-9.229502 38.75...\n\n\nCode\ntail(TRIPSdlines_oneway[,c(1,2)])\n\n\nSimple feature collection with 6 features and 2 fields\nAttribute-geometry relationships: identity (2)\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -9.357651 ymin: 38.49491 xmax: -8.80664 ymax: 38.92208\nGeodetic CRS:  WGS 84\n           o                   d                       geometry\n145   Oeiras Vila Franca de Xira LINESTRING (-9.276317 38.71...\n146  Palmela Vila Franca de Xira LINESTRING (-8.80664 38.617...\n147   Seixal Vila Franca de Xira LINESTRING (-9.108785 38.60...\n148 Sesimbra Vila Franca de Xira LINESTRING (-9.120129 38.49...\n149  Setúbal Vila Franca de Xira LINESTRING (-8.887489 38.51...\n150   Sintra Vila Franca de Xira LINESTRING (-9.357651 38.82...\n\n\nExample of visualization with Public Transit trips in both ways.\n\n\nCode\nTRIPSdlines_oneway_noLX = TRIPSdlines_oneway |&gt; \n  filter(PTransit &gt; 5000) # reduce noise\n\nmapview(TRIPSdlines_oneway_noLX, zcol = \"PTransit\", lwd = 8)",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>OD pairs and desire lines</span>"
    ]
  },
  {
    "objectID": "desire-lines.html#using-population-centroids",
    "href": "desire-lines.html#using-population-centroids",
    "title": "11  OD pairs and desire lines",
    "section": "11.3 Using population centroids",
    "text": "11.3 Using population centroids\nThe od_to_sf() function uses the geometric center of the zones to create the desire lines. But if we replace those zones by the weighted centroids, we can have a more realistic representation of the flows.\n\n\nCode\n# Centroid_pop = st_read(\"data/Centroid_pop.gpkg\")\n\nTRIPSdlines_pop = od_to_sf(TRIPSmode, z = Centroid_pop) |&gt;  # works the same way\n  od_oneway() |&gt; # oneway\n  filter(o != d) # remove empty geometries\n\n\nCheck differences of lines with trips from/to Lisbon:\n\n\nCode\nTRIPSdlines_geo_LX = TRIPSdlines_oneway |&gt; \n  filter(o == \"Lisboa\" | d == \"Lisboa\") # or condition\nTRIPSdlines_pop_LX = TRIPSdlines_pop |&gt; \n  filter(o == \"Lisboa\" | d == \"Lisboa\")\n\nmapview(TRIPSdlines_geo_LX, color = \"blue\") + mapview(TRIPSdlines_pop_LX, color = \"red\")\n\n\n\n\n\n\nThe next step will be estimating the euclidean distances between these centroids, and compare them with the routing distances.\n\n\n\n\n\n\nLovelace, Robin, and Richard Ellison. 2018. “Stplanr: A Package for Transport Planning.” The R Journal 10 (2): 10. https://doi.org/10.32614/RJ-2018-053.\n\n\nLovelace, Robin, Rosa Félix, and Dustin Carlino. 2022. “Jittering: A Computationally Efficient Method for Generating Realistic Route Networks from Origin-Destination Data.” Findings, April. https://doi.org/10.32866/001c.33873.\n\n\nLovelace, Robin, and Malcolm Morgan. 2024. Od: Manipulate and Map Origin-Destination Data. https://github.com/itsleeds/od.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>OD pairs and desire lines</span>"
    ]
  },
  {
    "objectID": "desire-lines.html#footnotes",
    "href": "desire-lines.html#footnotes",
    "title": "11  OD pairs and desire lines",
    "section": "",
    "text": "See (Lovelace, Félix, and Carlino 2022).↩︎",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>OD pairs and desire lines</span>"
    ]
  },
  {
    "objectID": "distances.html",
    "href": "distances.html",
    "title": "12  Euclidean and routing distances",
    "section": "",
    "text": "12.1 Euclidean distances\nTaking the survey respondents’ location, we will estimate the distance to the university (IST) using the sf package.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Euclidean and routing distances</span>"
    ]
  },
  {
    "objectID": "distances.html#euclidean-distances",
    "href": "distances.html#euclidean-distances",
    "title": "12  Euclidean and routing distances",
    "section": "",
    "text": "Import survey data frame convert to sf\nWe will use a survey dataset with 200 observations, with the following variables: ID, Affiliation, Age, Sex, Transport Mode to IST, and latitude and longitude coordinates.\n\n\nCode\nlibrary(dplyr)\n\nSURVEY = read.csv(\"../data/SURVEY.txt\", sep = \"\\t\") # tab delimiter\nnames(SURVEY)\n\n\n[1] \"ID\"   \"AFF\"  \"AGE\"  \"SEX\"  \"MODE\" \"lat\"  \"lon\" \n\n\nAs we have the coordinates, we can convert this data frame to a spatial feature, as explained in the Introduction to spatial data section.\n\n\nCode\nlibrary(sf)\n\nSURVEYgeo = st_as_sf(SURVEY, coords = c(\"lon\", \"lat\"), crs = 4326) # convert to as sf data\n\n\n\n\nCreate new point at the university\nUsing coordinates from Instituto Superior Técnico, we can directly create a simple feature and assign its crs.\n\n\nCode\nUNIVERSITY = data.frame(place = \"IST\",\n                        lon = -9.1397404,\n                        lat = 38.7370168) |&gt;  # first a dataframe\n  st_as_sf(coords = c(\"lon\", \"lat\"), # then a spacial feature\n           crs = 4326)\n\n\nVisualize in a map:\n\n\nCode\nlibrary(mapview)\nmapview(SURVEYgeo, zcol = \"MODE\") + mapview(UNIVERSITY, col.region = \"red\", cex = 12)\n\n\n\n\n\n\n\n\nStraight lines\nFirst we will create lines connecting the survey locations to the university, using the st_nearest_points() function.\nThis function finds returns the nearest points between two geometries, and creates a line between them. This can be useful to find the nearest train station to each point, for instance.\nAs we only have 1 point at UNIVERSITY layer, we will have the same number of lines as number of surveys = 200.\n\n\nCode\nSURVEYeuclidean = st_nearest_points(SURVEYgeo, UNIVERSITY, pairwise = TRUE) |&gt;\n  st_as_sf() # this creates lines\n\nmapview(SURVEYeuclidean)\n\n\nWarning in cbind(`Feature ID` = fid, mat): number of rows of result is not a\nmultiple of vector length (arg 1)\n\n\n\n\n\n\nNote that if we have more than one point in the second layer, the pairwise = TRUE will create a line for each combination of points. Set to FALSE if, for instance, you have the same number of points in both layers and want to create a line between the corresponding points.\n\n\nDistance\nNow we can estimate the distance using the st_length() function.\n\n\nCode\n# compute the line length and add directly in the first survey layer\nSURVEYgeo = SURVEYgeo |&gt; \n  mutate(distance = st_length(SURVEYeuclidean))\n\n# remove the units - can be useful\nSURVEYgeo$distance = units::drop_units(SURVEYgeo$distance) \n\n\nWe could also estimate the distance using the st_distance() function directly, although we would not get and sf with lines.\n\n\nCode\nSURVEYgeo = SURVEYgeo |&gt; \n  mutate(distance = st_distance(SURVEYgeo, UNIVERSITY)[,1] |&gt;  # in meters\n           units::drop_units()) |&gt;  # remove units\n  mutate(distance = round(distance)) # round to integer\n\nsummary(SURVEYgeo$distance)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    298    1106    2186    2658    3683    8600 \n\n\nSURVEYgeo is still a points’ sf.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Euclidean and routing distances</span>"
    ]
  },
  {
    "objectID": "distances.html#routing-engines",
    "href": "distances.html#routing-engines",
    "title": "12  Euclidean and routing distances",
    "section": "12.2 Routing Engines",
    "text": "12.2 Routing Engines\nThere are different types of routing engines, regarding the type of network they use, the type of transportation they consider, and the type of data they need. We can have:\n\nUni-modal vs. Multi-modal\n\nOne mode per trip vs. One trip with multiple legs that can be made with different modes\nMulti-modal routing may require GTFS data (realistic Public Transit)\n\nOutput level of the results\n\nRoutes (1 journey = 1 route)\nLegs\nSegments\n\nRouting profiles\n\nType of user\nfastest / shortest path\navoid barriers / tolls, etc\n\n\n\n\n\nRouting options in OpenRouteService\n\n\n\nLocal vs. Remote (service request - usually web API)\n\nSpeed vs. Quota limits / price\nHard vs. Easy set up\nHardware limitations in local routing\nGlobal coverage in remote routing, with frequent updates\n\n\nExamples: OSRM, Dodgr, r5r, Googleway, CycleStreets, HERE.\n\nRouting distances with r5r\nWe use the r5r package to estimate the distance using a road network (Pereira et al. 2021).\n\n\n\n\n\n\nTo properly the setup r5r model for the area you are working on, you need to download the road network data from OpenStreetMap and, if needed, add a GTFS and DEM file, as it will be explained in the next section.\n\n\n\nWe will use only respondents with a distance to the university less than 2 km.\n\n\nCode\nSURVEYsample = SURVEYgeo |&gt; filter(distance &lt;= 2000)\nnrow(SURVEYsample)\n\n\n[1] 95\n\n\nWe need an id (unique identifier) for each survey location, to be used in the routing functions of r5r.\n\n\nCode\n# create id columns for both datasets\nSURVEYsample = SURVEYsample |&gt; \n  mutate(id = c(1:nrow(SURVEYsample))) # from 1 to the number of rows\n\nUNIVERSITY = UNIVERSITY |&gt; \n  mutate(id = 1) # only one row\n\n\n\nDistances by car\nEstimate the routes with time and distance by car, from survey locations to University.\n\n\nCode\nSURVEYcar = detailed_itineraries(\n  r5r_core = r5r_network,\n  origins = SURVEYsample,\n  destinations = UNIVERSITY,\n  mode = \"CAR\",\n  all_to_all = TRUE # if false, only 1-1 would be calculated\n)\n\nnames(SURVEYcar)\n\n\n [1] \"from_id\"          \"from_lat\"         \"from_lon\"         \"to_id\"           \n [5] \"to_lat\"           \"to_lon\"           \"option\"           \"departure_time\"  \n [9] \"total_duration\"   \"total_distance\"   \"segment\"          \"mode\"            \n[13] \"segment_duration\" \"wait\"             \"distance\"         \"route\"           \n[17] \"geometry\"        \n\n\nThe detailed_itineraries() function is super detailed!\n\n\n\n\n\n\nIf we want to know only time and distance, and not the route itself, we can use the travel_time_matrix().\n\n\n\n\n\nDistances by foot\nRepeat the same for WALK1.\n\n\nCode\nSURVEYwalk = detailed_itineraries(\n  r5r_core = r5r_network,\n  origins = SURVEYsample,\n  destinations = UNIVERSITY,\n  mode = \"WALK\",\n  all_to_all = TRUE # if false, only 1-1 would be calculated\n)\n\n\n\n\nDistances by PT\nFor Public Transit (TRANSIT) you may specify the egress mode, the departure time, and the maximum number of transfers.\n\n\nCode\nSURVEYtransit = detailed_itineraries(\n  r5r_core = r5r_network,\n  origins = SURVEYsample,\n  destinations = UNIVERSITY,\n  mode = \"TRANSIT\",\n  mode_egress = \"WALK\",\n  max_rides = 1, # The maximum PT rides allowed in the same trip\n  departure_datetime =  as.POSIXct(\"20-09-2023 08:00:00\",\n                                 format = \"%d-%m-%Y %H:%M:%S\"),\n  all_to_all = TRUE # if false, only 1-1 would be calculated\n)",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Euclidean and routing distances</span>"
    ]
  },
  {
    "objectID": "distances.html#compare-distances",
    "href": "distances.html#compare-distances",
    "title": "12  Euclidean and routing distances",
    "section": "12.3 Compare distances",
    "text": "12.3 Compare distances\nWe can now compare the euclidean and routing distances that we estimated for the survey locations under 2 km.\n\n\nCode\nsummary(SURVEYsample$distance) # Euclidean\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    298     790    1046    1112    1470    1963 \n\n\nCode\nsummary(SURVEYwalk$distance) # Walk\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    569    1090    1465    1505    1925    2710 \n\n\nCode\nsummary(SURVEYcar$distance) # Car\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    228    1401    1823    1893    2431    3177 \n\n\n\nWhat can you understand from this results?\n\n\n\n\n\n\n\n\n\n\n\nCircuity\nCompare 1 single route.\n\n\nCode\nmapview(SURVEYeuclidean[165,], color = \"black\") + # 1556 meters\n  mapview(SURVEYwalk[78,], color = \"red\") + # 1989 meters\n  mapview(SURVEYcar[78,], color = \"blue\") # 2565 meters\n\n\n\n\n\n\nWith this we can see the circuity of the routes, a measure of route / transportation efficiency, which is the ratio between the routing distance and the euclidean distance.\nThe cicuity for car (1.65) is usually higher than for walking (1.28) or biking, for shorter distances.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Euclidean and routing distances</span>"
    ]
  },
  {
    "objectID": "distances.html#visualize-routes",
    "href": "distances.html#visualize-routes",
    "title": "12  Euclidean and routing distances",
    "section": "12.4 Visualize routes",
    "text": "12.4 Visualize routes\nVisualize with transparency of 30%, to get a clue when they overlay.\n\n\nCode\nmapview(SURVEYwalk, alpha = 0.3)\n\n\n\n\n\n\nCode\nmapview(SURVEYcar, alpha = 0.3, color = \"red\")\n\n\n\n\n\n\nWe can also use the overline() function from stplanr package to break up the routes when they overline, and add them up.\n\n\nCode\n# we create a value that we can later sum\n# it can be the number of trips represented by this route\nSURVEYwalk$trips = 1 # in this case is only one respondent per route\n\nSURVEYwalk_overline = stplanr::overline(\n  SURVEYwalk,\n  attrib = \"trips\",\n  fun = sum\n)\n\nmapview(SURVEYwalk_overline, zcol = \"trips\", lwd = 3)\n\n\n\n\n\n\nWith this we can visually inform on how many people travel along a route, from the survey dataset2.\n\n\n\n\n\n\nPereira, Rafael H. M., Marcus Saraiva, Daniel Herszenhut, Carlos Kaue Vieira Braga, and Matthew Wigginton Conway. 2021. “R5r: Rapid Realistic Routing on Multimodal Transport Networks with r5 in r.” Findings, March. https://doi.org/10.32866/001c.21262.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Euclidean and routing distances</span>"
    ]
  },
  {
    "objectID": "distances.html#footnotes",
    "href": "distances.html#footnotes",
    "title": "12  Euclidean and routing distances",
    "section": "",
    "text": "For bike you would use BICYCLE.↩︎\nAssuming all travel by the shortest path.↩︎",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Euclidean and routing distances</span>"
    ]
  },
  {
    "objectID": "open-data.html",
    "href": "open-data.html",
    "title": "13  Open transportation data",
    "section": "",
    "text": "13.1 Road Networks",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Open transportation data</span>"
    ]
  },
  {
    "objectID": "open-data.html#road-networks",
    "href": "open-data.html#road-networks",
    "title": "13  Open transportation data",
    "section": "",
    "text": "13.1.1 OpenStreetMap\nThe OpenStreetMap is a collaborative online mapping project that creates a free editable map of the world.\nThis is the most used source of road network data for transportation analysis in academia, since it is available almost everywhere in the world, is open and free to use.\n\n\n\n\n\n\nAlthough it can be not 100% accurate, OSM is a good source of data for most of the cases.\n\n\n\nYou can access it’s visualization tool at www.openstreetmap.org. To edit the map, you can use the Editor, once you register.\nIf you want to download the data, you can use the following tools.\n\nOverpass API\nGeofabrik\n\nThese websites include all the OSM data, with much more information than you need.\n\n\n13.1.2 HOT Export Tool\nThis interactive tool helps you to select the region you want to extract, the type of information to include, and the output data format.\nAccess via export.hotosm.org1.\n\nAfter the export, you can read in R using the sf package:\n\n\nCode\nGent = sf::st_read(\"../data/Gent_center.gpkg\", quiet = TRUE)\n\nmapview::mapview(Gent, zcol = \"highway\")\n\n\n\n\n\n\n\n\n13.1.3 OSM in R\nThere are also some R packages that can help you to download and work with OpenStreetMap data, such as:\n\nosmdata\nosmextract\n\nThis is an example of how to download OpenStreetMap road network data using the osmextract package:\n\n\nCode\nlibrary(osmextract)\nOSM_Malta = oe_get_network(place = \"Malta\") # it will geocode the place\n\nMalta_main_roads = OSM_Malta |&gt; \n  filter(highway %in% c(\"primary\", \"secondary\", \"tertiary\", \"trunk\"))\n\nplot(Malta_main_roads[\"highway\"])",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Open transportation data</span>"
    ]
  },
  {
    "objectID": "open-data.html#transportation-services-data",
    "href": "open-data.html#transportation-services-data",
    "title": "13  Open transportation data",
    "section": "13.2 Transportation Services’ Data",
    "text": "13.2 Transportation Services’ Data\n\n13.2.1 GTFS\nGeneral Transit Feed Specification (GTFS) is standard format for documenting public transportation information, including: routes, schedules, stop locations, calendar patterns, trips, and possible transfers. Transit agencies are responsible for maintaining the data up-to-date.\nThis information is used in several applications, such as Google Maps, to provide public transportation directions. It can be offered for a city, a region, or even a whole country, depending on the PT agency.\nThe recent version 2 of the GTFS standard includes more information, such as real-time data.\nThe data is usually in a .zip file that includes several .txt files (one for each type of information) with tabular relations.\n\n\n\nSource: trilliumtransit.com\n\n\n\nOnline sources\nYou can find most GTFS data in the following websites:\n\nTransitLand\nTransitFeeds\n\nSome PT agencies also provide their open-data in their websites.\n\n\nR packages\nThere are some nice R packages to read and manipulate GTFS data, such as:\n\ntidytransit\ngtfstools\n\n\n\n\n\n\n\nBe aware that they may share the same function names, so it is important to use of of them at the time.\n\n\n\n\n\n\n13.2.2 National Access Points\nThe European Union has a directive that requires the member states to provide access to transportation data. Data includes not only Public Transportation data, but also road networks, car parking, and other transportation-related information.\nList of the European Union members states with National Access Points for Transportation data\nExample of Bus services data in Belgium:\n\n\n\nSource: Transport Data Belgium",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Open transportation data</span>"
    ]
  },
  {
    "objectID": "open-data.html#footnotes",
    "href": "open-data.html#footnotes",
    "title": "13  Open transportation data",
    "section": "",
    "text": "You need an OSM account to use it.↩︎",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Open transportation data</span>"
    ]
  },
  {
    "objectID": "software.html#open-route-service",
    "href": "software.html#open-route-service",
    "title": "2  Software",
    "section": "2.4 Open Route Service",
    "text": "2.4 Open Route Service\nSign up for an account and create a token.\nCopy your API.\nIn RStudio console, run:\n\n# install.packages(\"openrouteservice\")\n\nopenrouteservice::ors_api_key(\"YOUR-API-KEY\")\n\nThis will store your key on your .Renviron file, meaning that every time you open RStudio, you won’t need to run this command again.\nThis is useful also to write your openrouteservice scripts without sharing your key with others.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Software</span>"
    ]
  },
  {
    "objectID": "r-basics.html#basics",
    "href": "r-basics.html#basics",
    "title": "3  R basics",
    "section": "3.2 Basics",
    "text": "3.2 Basics\n\nCombinations\n\nc(1, 2, 3)\n\n[1] 1 2 3\n\nc(1:3) # The \":\" indicates a range between the first and second numbers. \n\n[1] 1 2 3\n\n\n\n\n\n\n\n\nTry it yourself\n\n\n\nTry to write a combination with the numbers 10, 11, 56, 57, 58.\n\n\n\n\nCreate a comment with ctrl + shift + c\n\n# Comments help you organize your code. \n# A line with a comment will not be executed. \n\n\n\nCreate a table\nA simple table with the number of trips by car, PT, walking, and cycling in a hypothetical street segment at a certain period.\nDefine variables\n\nmodes &lt;- c(\"car\", \"PT\", \"walking\", \"cycling\") # you can use \"=\" or \"&lt;-\"\ntrips = c(200, 50, 300, 150) # key sensitive (uppercase and lowercase letters are different)\n\nJoin the variables to create a table\n\ntable_example = data.frame(modes, trips)\n\nTake a look at the table\nVisualize the table by clicking on the “Data” in the “Environment” page or use :\n\nView(table_example)\n\nLook at the first row\n\ntable_example[1,] #rows and columns start from 1 in R, differently from Python which starts from 0.\n\n  modes trips\n1   car   200\n\n\nLook at first row and column\n\ntable_example[1,1]\n\n[1] \"car\"\n\ntable_example[1,4]\n\nNULL",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R basics</span>"
    ]
  },
  {
    "objectID": "eda.html#intro",
    "href": "eda.html#intro",
    "title": "5  Exploratory Data Analysis",
    "section": "",
    "text": "Generate questions about your data.\nSearch for answers by visualizing, transforming, and modelling your data.\nUse what you learn to refine your questions and/or generate new questions.\n\n\nNote: EDA is not a formal process with a strict set of rules!\n\n\n\n\n\n\n\n\nDo it yourself with R\n\n\n\nCopy the script ExploratoryDataAnalysis.R and paste it in your session.\nRun each line using CTRL + ENTER",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#install-and-load-packages",
    "href": "eda.html#install-and-load-packages",
    "title": "5  Exploratory Data Analysis",
    "section": "5.2 Install and load packages",
    "text": "5.2 Install and load packages\n\nlibrary(tidyverse) # Pack of most used libraries for data science\nlibrary(skimr) # summary of the data\nlibrary(DataExplorer) # exploratory data analysis\nlibrary(corrplot) # correlation plots",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#dataset",
    "href": "eda.html#dataset",
    "title": "5  Exploratory Data Analysis",
    "section": "5.3 Dataset",
    "text": "5.3 Dataset\nThe database used in this example is a treated database from the Mobility Survey for the metropolitan areas of Lisbon and Porto in 2018 (INE 2018). We will only focus on trips within the metropolitan area of Lisbon.\nIncluded variables:\n\nOrigin_dicofre16 - Code of Freguesia (district) as set by INE after 2016 (Distrito + Concelho + Freguesia), for trip origin\nTotal - number of trips with origin at each district\nWalk - number of walking trips\nBike - number of bike trips\nCar - number of car trips. Includes taxi and motorcycle.\nPTransit - number of Public Transit trips\nOther - number of other trips (truck, van, tractor, aviation)\nDistance - average trip distance (km)\nDuration - average trip duration (minutes)\nCar_perc - percentage of car trips\nN_INDIVIDUOS - number of residents (INE 2022)\nMale_perc - percentage of male residents (INE 2022)\nIncomeHH - average household income\nNvehicles - average number of car/motorcycle vehicles in the household\nDrivingLic - percentage of car driving licence holders\nCarParkFree_Work - percentage of respondents with free car parking at the work location\nPTpass - percentage of public transit monthly pass holders\ninternal - binary variable (factor). Yes: trip with same TAZ origin and destination, No: trips with different destination\nLisboa - binary variable (factor). Yes: the district is part of Lisbon municipality, No: otherwise\nArea_km2 - area of in Origin_dicofre16, in km2\n\n\n5.3.1 Import dataset\n\ndata = readRDS(\"../data/IMOBmodel.Rds\")\n\nTake a look at the dataset\n\nView(data) # open in table\nglimpse(data) # glimpse of the dataset\n\nRows: 236\nColumns: 20\n$ Origin_dicofre16 &lt;chr&gt; \"110501\", \"110501\", \"110506\", \"110506\", \"110507\", \"11…\n$ Total            &lt;dbl&gt; 35539, 47602, 37183, 42313, 30725, 54586, 57747, 6788…\n$ Walk             &lt;dbl&gt; 11325, 3502, 12645, 1418, 9389, 2630, 20423, 3573, 62…\n$ Bike             &lt;dbl&gt; 1309, 416, 40, 163, 1481, 168, 1406, 809, 13, 37, 3, …\n$ Car              &lt;dbl&gt; 21446, 37727, 22379, 37337, 19654, 44611, 33044, 5225…\n$ PTransit         &lt;dbl&gt; 1460, 5519, 2057, 3285, 201, 6963, 2477, 10534, 110, …\n$ Other            &lt;dbl&gt; 0, 437, 63, 106, 0, 215, 396, 717, 2, 169, 3, 824, 0,…\n$ Distance         &lt;dbl&gt; 11.779, 11.779, 9.868, 9.868, 9.600, 9.600, 12.875, 1…\n$ Duration         &lt;dbl&gt; 23.96, 23.96, 22.78, 22.78, 23.39, 23.39, 26.89, 26.8…\n$ Car_perc         &lt;dbl&gt; 60.34497, 79.25507, 60.18611, 88.24002, 63.96745, 81.…\n$ N_INDIVIDUOS     &lt;dbl&gt; 44165, 44165, 59238, 59238, 46529, 46529, 64192, 6419…\n$ Male_perc        &lt;dbl&gt; 47.66, 47.66, 47.12, 47.12, 45.54, 45.54, 46.14, 46.1…\n$ IncomeHH         &lt;dbl&gt; 2204.32, 2204.32, 1896.05, 1896.05, 2430.43, 2430.43,…\n$ Nvehicles        &lt;dbl&gt; 1.86, 1.86, 1.73, 1.73, 1.60, 1.60, 1.64, 1.64, 1.20,…\n$ DrivingLic       &lt;dbl&gt; 63.03, 63.03, 65.42, 65.42, 67.05, 67.05, 72.75, 72.7…\n$ CarParkFree_Work &lt;dbl&gt; 49.97, 49.97, 57.09, 57.09, 47.43, 47.43, 56.47, 56.4…\n$ PTpass           &lt;dbl&gt; 11.14, 11.14, 17.12, 17.12, 18.25, 18.25, 19.73, 19.7…\n$ internal         &lt;fct&gt; Yes, No, Yes, No, Yes, No, Yes, No, Yes, No, Yes, No,…\n$ Lisboa           &lt;fct&gt; No, No, No, No, No, No, No, No, Yes, Yes, Yes, Yes, Y…\n$ Area_km2         &lt;dbl&gt; 39.767414, 39.767414, 20.364310, 20.364310, 8.109861,…\n\nstr(data) # Structure of the dataset \n\n'data.frame':   236 obs. of  20 variables:\n $ Origin_dicofre16: chr  \"110501\" \"110501\" \"110506\" \"110506\" ...\n $ Total           : num  35539 47602 37183 42313 30725 ...\n $ Walk            : num  11325 3502 12645 1418 9389 ...\n $ Bike            : num  1309 416 40 163 1481 ...\n $ Car             : num  21446 37727 22379 37337 19654 ...\n $ PTransit        : num  1460 5519 2057 3285 201 ...\n $ Other           : num  0 437 63 106 0 215 396 717 2 169 ...\n $ Distance        : num  11.78 11.78 9.87 9.87 9.6 ...\n $ Duration        : num  24 24 22.8 22.8 23.4 ...\n $ Car_perc        : num  60.3 79.3 60.2 88.2 64 ...\n $ N_INDIVIDUOS    : num  44165 44165 59238 59238 46529 ...\n $ Male_perc       : num  47.7 47.7 47.1 47.1 45.5 ...\n $ IncomeHH        : num  2204 2204 1896 1896 2430 ...\n $ Nvehicles       : num  1.86 1.86 1.73 1.73 1.6 1.6 1.64 1.64 1.2 1.2 ...\n $ DrivingLic      : num  63 63 65.4 65.4 67 ...\n $ CarParkFree_Work: num  50 50 57.1 57.1 47.4 ...\n $ PTpass          : num  11.1 11.1 17.1 17.1 18.2 ...\n $ internal        : Factor w/ 2 levels \"Yes\",\"No\": 1 2 1 2 1 2 1 2 1 2 ...\n $ Lisboa          : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 2 2 ...\n $ Area_km2        : num  39.77 39.77 20.36 20.36 8.11 ...",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#summary-statistics",
    "href": "eda.html#summary-statistics",
    "title": "5  Exploratory Data Analysis",
    "section": "5.4 Summary statistics",
    "text": "5.4 Summary statistics\nHave an overview of the variables and their statistics’ summary.\n\nsummary(data) # Check the summary statistics\n\n Origin_dicofre16       Total             Walk              Bike        \n Length:236         Min.   :   361   Min.   :    0.0   Min.   :   0.00  \n Class :character   1st Qu.:  5918   1st Qu.:  763.2   1st Qu.:   0.00  \n Mode  :character   Median : 17474   Median : 3125.0   Median :  13.50  \n                    Mean   : 22457   Mean   : 5383.4   Mean   : 107.03  \n                    3rd Qu.: 33378   3rd Qu.: 8298.5   3rd Qu.:  98.75  \n                    Max.   :112186   Max.   :32646.0   Max.   :2040.00  \n      Car           PTransit         Other           Distance     \n Min.   :    0   Min.   :    0   Min.   :   0.0   Min.   : 6.835  \n 1st Qu.: 3243   1st Qu.:  249   1st Qu.:   2.0   1st Qu.: 9.539  \n Median : 9008   Median : 1057   Median :  44.0   Median :10.323  \n Mean   :13289   Mean   : 3474   Mean   : 203.4   Mean   :11.139  \n 3rd Qu.:21249   3rd Qu.: 4853   3rd Qu.: 281.5   3rd Qu.:12.097  \n Max.   :52631   Max.   :41672   Max.   :2391.0   Max.   :22.660  \n    Duration        Car_perc      N_INDIVIDUOS     Male_perc    \n Min.   :16.30   Min.   : 0.00   Min.   : 1566   Min.   :44.61  \n 1st Qu.:23.00   1st Qu.:45.40   1st Qu.:11060   1st Qu.:46.58  \n Median :24.70   Median :62.62   Median :20855   Median :47.51  \n Mean   :25.42   Mean   :59.00   Mean   :24324   Mean   :47.54  \n 3rd Qu.:27.73   3rd Qu.:75.72   3rd Qu.:36079   3rd Qu.:48.29  \n Max.   :37.42   Max.   :99.27   Max.   :68649   Max.   :55.94  \n    IncomeHH        Nvehicles       DrivingLic    CarParkFree_Work\n Min.   : 884.5   Min.   :1.020   Min.   :37.04   Min.   : 5.47   \n 1st Qu.:1417.8   1st Qu.:1.350   1st Qu.:57.67   1st Qu.:40.39   \n Median :1594.7   Median :1.545   Median :63.00   Median :50.51   \n Mean   :1732.6   Mean   :1.530   Mean   :62.50   Mean   :49.30   \n 3rd Qu.:1953.5   3rd Qu.:1.670   3rd Qu.:68.84   3rd Qu.:57.92   \n Max.   :3462.3   Max.   :2.420   Max.   :80.79   Max.   :87.60   \n     PTpass      internal  Lisboa       Area_km2      \n Min.   : 0.00   Yes:118   No :188   Min.   :  1.494  \n 1st Qu.:13.41   No :118   Yes: 48   1st Qu.:  5.044  \n Median :22.71                       Median : 11.598  \n Mean   :23.82                       Mean   : 25.553  \n 3rd Qu.:32.94                       3rd Qu.: 28.511  \n Max.   :60.45                       Max.   :282.125  \n\nskim(data) # In a more organized way\n\n\nData summary\n\n\nName\ndata\n\n\nNumber of rows\n236\n\n\nNumber of columns\n20\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n2\n\n\nnumeric\n17\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nOrigin_dicofre16\n0\n1\n6\n6\n0\n118\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ninternal\n0\n1\nFALSE\n2\nYes: 118, No: 118\n\n\nLisboa\n0\n1\nFALSE\n2\nNo: 188, Yes: 48\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nTotal\n0\n1\n22457.00\n19084.45\n361.00\n5917.75\n17474.00\n33377.50\n112186.00\n▇▃▂▁▁\n\n\nWalk\n0\n1\n5383.44\n6224.84\n0.00\n763.25\n3125.00\n8298.50\n32646.00\n▇▂▁▁▁\n\n\nBike\n0\n1\n107.03\n248.65\n0.00\n0.00\n13.50\n98.75\n2040.00\n▇▁▁▁▁\n\n\nCar\n0\n1\n13289.24\n12351.61\n0.00\n3243.00\n9008.00\n21248.75\n52631.00\n▇▃▂▁▁\n\n\nPTransit\n0\n1\n3473.79\n5467.82\n0.00\n249.00\n1057.00\n4853.00\n41672.00\n▇▁▁▁▁\n\n\nOther\n0\n1\n203.45\n336.04\n0.00\n2.00\n44.00\n281.50\n2391.00\n▇▁▁▁▁\n\n\nDistance\n0\n1\n11.14\n2.66\n6.84\n9.54\n10.32\n12.10\n22.66\n▇▇▂▁▁\n\n\nDuration\n0\n1\n25.42\n3.91\n16.30\n23.00\n24.70\n27.73\n37.42\n▁▇▆▂▁\n\n\nCar_perc\n0\n1\n59.00\n21.56\n0.00\n45.40\n62.62\n75.72\n99.27\n▁▃▆▇▅\n\n\nN_INDIVIDUOS\n0\n1\n24323.80\n16438.04\n1566.00\n11060.00\n20855.00\n36079.00\n68649.00\n▇▆▅▃▁\n\n\nMale_perc\n0\n1\n47.54\n1.57\n44.61\n46.58\n47.50\n48.29\n55.94\n▅▇▂▁▁\n\n\nIncomeHH\n0\n1\n1732.55\n453.11\n884.46\n1417.76\n1594.73\n1953.50\n3462.32\n▃▇▃▁▁\n\n\nNvehicles\n0\n1\n1.53\n0.24\n1.02\n1.35\n1.54\n1.67\n2.42\n▃▇▆▂▁\n\n\nDrivingLic\n0\n1\n62.50\n8.12\n37.04\n57.67\n63.00\n68.84\n80.79\n▁▂▇▇▂\n\n\nCarParkFree_Work\n0\n1\n49.30\n14.30\n5.47\n40.39\n50.51\n57.92\n87.60\n▁▃▇▇▁\n\n\nPTpass\n0\n1\n23.82\n12.87\n0.00\n13.41\n22.72\n32.94\n60.45\n▅▇▇▂▁\n\n\nArea_km2\n0\n1\n25.55\n42.58\n1.49\n5.04\n11.60\n28.51\n282.13\n▇▁▁▁▁",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#missing-data",
    "href": "eda.html#missing-data",
    "title": "5  Exploratory Data Analysis",
    "section": "5.5 Missing data",
    "text": "5.5 Missing data\nIs there missing data (NA)? How many?\n\ntable(is.na(data))\n\n\nFALSE \n 4720",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#outliers",
    "href": "eda.html#outliers",
    "title": "5  Exploratory Data Analysis",
    "section": "5.6 Outliers",
    "text": "5.6 Outliers\nInspect outliers and distributions\n\nboxplot(data) # This does now work if variables are not all continuous\n\n\ndata_continuous = data |&gt; select(-Origin_dicofre16, -internal, -Lisboa) # Exclude categorical variables\nboxplot(data_continuous) # Exclude categorical variables\n\n\n\n\n\n\n\nhist(data_continuous$Total) # histogram\n\n\n\n\n\n\n\nboxplot(data_continuous$Total) # outliers detected\n\n\n\n\n\n\n\n\n\n5.6.1 Treating outliers\nBased on the theory, we can create a function to detect the outliers.\n\noutlier = function(x) {\n  q = quantile(x, probs = c(0.25, 0.75), na.rm = TRUE)  # Q1 and Q3\n  caps = quantile(x, probs = c(0.05, 0.95), na.rm = TRUE) # 5th and 95th percentile\n  H = 1.5 * IQR(x, na.rm = TRUE) # interquartile range\n  \n  case_when(\n    x &lt; (q[1] - H) ~ caps[1], # replace values that are LESS than Q1-1.5*IQR with the P5 value\n    x &gt; (q[2] + H) ~ caps[2], # replace values that are MORE than Q3+1.5*IQR with the P95 value\n    TRUE ~ x # otherwise, return the original value\n  )\n}\n\nNow, use it in a copy of the table\n\ndata_outliers = data_continuous # duplicate the table\ndata_outliers$Total = outlier(data_outliers$Total) # Use the function to the same variable\n\nTake a look again at the boxplot\n\nboxplot(data_outliers$Total)\n\n\n\n\n\n\n\n\nCompare the variable with and without the outliers\n\n# Mean\nmean(data$Total)\n\n[1] 22457\n\nmean(data_outliers$Total)\n\n[1] 22013.51\n\n# Median\nmedian(data$Total)\n\n[1] 17474\n\nmedian(data_outliers$Total)\n\n[1] 17474\n\n# Standard deviation\nsd(data$Total)\n\n[1] 19084.45\n\nsd(data_outliers$Total)\n\n[1] 17739.39\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere are many methods to treat outliers. This is just one of them.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#histograms",
    "href": "eda.html#histograms",
    "title": "5  Exploratory Data Analysis",
    "section": "5.7 Histograms",
    "text": "5.7 Histograms\nPlot histograms of all the continuous variables\n\nplot_histogram(data, ncol = 3) # histograms with 3 columns\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot_density(data, ncol = 3) # density plots\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck how other variables are distributed regarding Car_perc\n\nplot_boxplot(data, by = \"Car_perc\", ncol = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen Car_perc increases, PTpass decreases.\n\n\n\n\n\n\n\n\nExercise\n\n\n\nTry plotting the boxplots of each independent variable with Car_perc.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#correlations",
    "href": "eda.html#correlations",
    "title": "5  Exploratory Data Analysis",
    "section": "5.8 Correlations",
    "text": "5.8 Correlations\nPlot correlation heatmaps, between continuous variables.\n\n# estimate correlation matrix\ncorrmat = cor(data_continuous, method = \"pearson\") |&gt; round(2)\ncorrmat\n\n                 Total  Walk  Bike   Car PTransit Other Distance Duration\nTotal             1.00  0.49  0.44  0.93     0.78  0.48    -0.36     0.10\nWalk              0.49  1.00  0.27  0.21     0.09 -0.01    -0.34     0.03\nBike              0.44  0.27  1.00  0.41     0.26  0.11     0.00     0.08\nCar               0.93  0.21  0.41  1.00     0.70  0.47    -0.29    -0.03\nPTransit          0.78  0.09  0.26  0.70     1.00  0.58    -0.20     0.36\nOther             0.48 -0.01  0.11  0.47     0.58  1.00    -0.09     0.22\nDistance         -0.36 -0.34  0.00 -0.29    -0.20 -0.09     1.00     0.16\nDuration          0.10  0.03  0.08 -0.03     0.36  0.22     0.16     1.00\nCar_perc          0.01 -0.53  0.00  0.27     0.01  0.09     0.26    -0.40\nN_INDIVIDUOS      0.72  0.56  0.29  0.68     0.28  0.20    -0.46    -0.11\nMale_perc        -0.38 -0.32 -0.14 -0.33    -0.20 -0.08     0.39    -0.01\nIncomeHH          0.25  0.04  0.22  0.26     0.23  0.10    -0.09     0.30\nNvehicles        -0.16 -0.25  0.00 -0.02    -0.21 -0.14     0.27    -0.26\nDrivingLic       -0.01 -0.12  0.08  0.08    -0.10 -0.09     0.23    -0.12\nCarParkFree_Work -0.26 -0.29 -0.03 -0.08    -0.38 -0.25     0.30    -0.53\nPTpass            0.25  0.23  0.01  0.09     0.39  0.21    -0.36     0.55\nArea_km2         -0.15 -0.15  0.03 -0.08    -0.19 -0.09     0.23    -0.25\n                 Car_perc N_INDIVIDUOS Male_perc IncomeHH Nvehicles DrivingLic\nTotal                0.01         0.72     -0.38     0.25     -0.16      -0.01\nWalk                -0.53         0.56     -0.32     0.04     -0.25      -0.12\nBike                 0.00         0.29     -0.14     0.22      0.00       0.08\nCar                  0.27         0.68     -0.33     0.26     -0.02       0.08\nPTransit             0.01         0.28     -0.20     0.23     -0.21      -0.10\nOther                0.09         0.20     -0.08     0.10     -0.14      -0.09\nDistance             0.26        -0.46      0.39    -0.09      0.27       0.23\nDuration            -0.40        -0.11     -0.01     0.30     -0.26      -0.12\nCar_perc             1.00        -0.09      0.18    -0.07      0.40       0.33\nN_INDIVIDUOS        -0.09         1.00     -0.46     0.14     -0.14       0.00\nMale_perc            0.18        -0.46      1.00    -0.34      0.19      -0.11\nIncomeHH            -0.07         0.14     -0.34     1.00      0.23       0.22\nNvehicles            0.40        -0.14      0.19     0.23      1.00       0.44\nDrivingLic           0.33         0.00     -0.11     0.22      0.44       1.00\nCarParkFree_Work     0.53        -0.19      0.26    -0.01      0.54       0.39\nPTpass              -0.52         0.20     -0.34     0.16     -0.44      -0.45\nArea_km2             0.25        -0.19      0.30    -0.18      0.12       0.10\n                 CarParkFree_Work PTpass Area_km2\nTotal                       -0.26   0.25    -0.15\nWalk                        -0.29   0.23    -0.15\nBike                        -0.03   0.01     0.03\nCar                         -0.08   0.09    -0.08\nPTransit                    -0.38   0.39    -0.19\nOther                       -0.25   0.21    -0.09\nDistance                     0.30  -0.36     0.23\nDuration                    -0.53   0.55    -0.25\nCar_perc                     0.53  -0.52     0.25\nN_INDIVIDUOS                -0.19   0.20    -0.19\nMale_perc                    0.26  -0.34     0.30\nIncomeHH                    -0.01   0.16    -0.18\nNvehicles                    0.54  -0.44     0.12\nDrivingLic                   0.39  -0.45     0.10\nCarParkFree_Work             1.00  -0.66     0.32\nPTpass                      -0.66   1.00    -0.41\nArea_km2                     0.32  -0.41     1.00\n\n# store the results so you can call the p-value at the corrplot\nres = cor.mtest(data_continuous, conf.level = .95) \n\ncorrplot(\n  corrmat,\n  method = \"color\", # or \"circle\"\n  p.mat = res$p,\n  sig.level = 0.05,\n  type = \"upper\", # display only the upper triangular\n  # order = \"hclust\", # order by hierarchical clustering\n  tl.col = \"black\" # text label color\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe pairwise correlations that are crossed are statistically not significant. The null hypothesis (H0) is that correlation is zero. The alternative hypothesis (HA) is that correlation is not zero.\nThis means that the correlations are only significant when you reject the null hypothesis (p-value &lt; 0.05).\n\n\nOther method, using the DataExplorer package\n\nplot_correlation(data_continuous)\n\n\n\n\n\n\n\n\nCheck the p-value of a crossed pair correlation:\n\ncor.test(data$IncomeHH, data$Bike)\n\n\n    Pearson's product-moment correlation\n\ndata:  data$IncomeHH and data$Bike\nt = 3.5261, df = 234, p-value = 0.0005074\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.09977779 0.34249284\nsample estimates:\n      cor \n0.2246163 \n\ncor.test(data$IncomeHH, data$Duration)\n\n\n    Pearson's product-moment correlation\n\ndata:  data$IncomeHH and data$Duration\nt = 4.817, df = 234, p-value = 2.62e-06\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1795420 0.4122446\nsample estimates:\n     cor \n0.300356 \n\ncor.test(data$Distance, data$Duration)\n\n\n    Pearson's product-moment correlation\n\ndata:  data$Distance and data$Duration\nt = 2.4605, df = 234, p-value = 0.0146\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.0317533 0.2808153\nsample estimates:\n      cor \n0.1588098 \n\n\n\n\n\n\n\n\nThe default for cor.test is Pearson, two-sided, with a 95% confident level. Check ?cor.test for more options.\n\n\n\n\n\n\n\n\n\nINE. 2018. “Mobilidade e Funcionalidade Do Território Nas Áreas Metropolitanas do Porto e de Lisboa: 2017.” Lisboa: Instituto National de Estatística. https://www.ine.pt/xportal/xmain?xpid=INE&xpgid=ine_publicacoes&PUBLICACOESpub_boui=349495406&PUBLICACOESmodo=2&xlang=pt.\n\n\n———. 2022. “Censos 2021- XVI Recenseamento Geral da População. VI Recenseamento Geral da Habitação.” Lisboa: Instituto National de Estatística. https://censos.ine.pt/xurl/pub/65586079.\n\n\nWickham, Hadley, Mine Çetinaka-Rundel, and Garrett Grolemund. 2017. R for Data Science. 2nd ed. O’Reilly Sebastopol. https://r4ds.hadley.nz/.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#load-packages",
    "href": "eda.html#load-packages",
    "title": "5  Exploratory Data Analysis",
    "section": "5.2 Load packages",
    "text": "5.2 Load packages\n\nlibrary(tidyverse) # Pack of most used libraries for data science\nlibrary(skimr) # summary of the data\nlibrary(DataExplorer) # exploratory data analysis\nlibrary(corrplot) # correlation plots",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#dataset",
    "href": "linear-regression.html#dataset",
    "title": "6  Multiple Linear Regression",
    "section": "6.2 Dataset",
    "text": "6.2 Dataset\nThe database used in this example is a treated database from the Mobility Survey for the metropolitan areas of Lisbon in 2018 (INE 2018).\nIncluded variables:\n\nOrigin_dicofre16 - Code of Freguesia (district) as set by INE after 2016 (Distrito + Concelho + Freguesia), for trip origin\nTotal - number of trips with origin at each district\nWalk - number of walking trips\nBike - number of bike trips\nCar - number of car trips. Includes taxi and motorcycle.\nPTransit - number of Public Transit trips\nOther - number of other trips (truck, van, tractor, aviation)\nDistance - average trip distance (km)\nDuration - average trip duration (minutes)\nCar_perc - percentage of car trips\nN_INDIVIDUOS - number of residents (INE 2022)\nMale_perc - percentage of male residents (INE 2022)\nIncomeHH - average household income\nNvehicles - average number of car/motorcycle vehicles in the household\nDrivingLic - percentage of car driving licence holders\nCarParkFree_Work - percentage of respondents with free car parking at the work location\nPTpass - percentage of public transit monthly pass holders\ninternal - binary variable (factor). Yes: trip with same TAZ origin and destination, No: trips with different destination\nLisboa - binary variable (factor). Yes: the district is part of Lisbon municipality, No: otherwise\nArea_km2 - area of in Origin_dicofre16, in km2\n\n\nImport dataset\n\ndata = readRDS(\"../data/IMOBmodel.Rds\")\ndata_continuous = data |&gt; select(-Origin_dicofre16, -internal, -Lisboa) # Exclude categorical variables\n\nShow summary statistics\n\nskim(data)\n\n\nData summary\n\n\nName\ndata\n\n\nNumber of rows\n236\n\n\nNumber of columns\n20\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n2\n\n\nnumeric\n17\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nOrigin_dicofre16\n0\n1\n6\n6\n0\n118\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ninternal\n0\n1\nFALSE\n2\nYes: 118, No: 118\n\n\nLisboa\n0\n1\nFALSE\n2\nNo: 188, Yes: 48\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nTotal\n0\n1\n22457.00\n19084.45\n361.00\n5917.75\n17474.00\n33377.50\n112186.00\n▇▃▂▁▁\n\n\nWalk\n0\n1\n5383.44\n6224.84\n0.00\n763.25\n3125.00\n8298.50\n32646.00\n▇▂▁▁▁\n\n\nBike\n0\n1\n107.03\n248.65\n0.00\n0.00\n13.50\n98.75\n2040.00\n▇▁▁▁▁\n\n\nCar\n0\n1\n13289.24\n12351.61\n0.00\n3243.00\n9008.00\n21248.75\n52631.00\n▇▃▂▁▁\n\n\nPTransit\n0\n1\n3473.79\n5467.82\n0.00\n249.00\n1057.00\n4853.00\n41672.00\n▇▁▁▁▁\n\n\nOther\n0\n1\n203.45\n336.04\n0.00\n2.00\n44.00\n281.50\n2391.00\n▇▁▁▁▁\n\n\nDistance\n0\n1\n11.14\n2.66\n6.84\n9.54\n10.32\n12.10\n22.66\n▇▇▂▁▁\n\n\nDuration\n0\n1\n25.42\n3.91\n16.30\n23.00\n24.70\n27.73\n37.42\n▁▇▆▂▁\n\n\nCar_perc\n0\n1\n59.00\n21.56\n0.00\n45.40\n62.62\n75.72\n99.27\n▁▃▆▇▅\n\n\nN_INDIVIDUOS\n0\n1\n24323.80\n16438.04\n1566.00\n11060.00\n20855.00\n36079.00\n68649.00\n▇▆▅▃▁\n\n\nMale_perc\n0\n1\n47.54\n1.57\n44.61\n46.58\n47.50\n48.29\n55.94\n▅▇▂▁▁\n\n\nIncomeHH\n0\n1\n1732.55\n453.11\n884.46\n1417.76\n1594.73\n1953.50\n3462.32\n▃▇▃▁▁\n\n\nNvehicles\n0\n1\n1.53\n0.24\n1.02\n1.35\n1.54\n1.67\n2.42\n▃▇▆▂▁\n\n\nDrivingLic\n0\n1\n62.50\n8.12\n37.04\n57.67\n63.00\n68.84\n80.79\n▁▂▇▇▂\n\n\nCarParkFree_Work\n0\n1\n49.30\n14.30\n5.47\n40.39\n50.51\n57.92\n87.60\n▁▃▇▇▁\n\n\nPTpass\n0\n1\n23.82\n12.87\n0.00\n13.41\n22.72\n32.94\n60.45\n▅▇▇▂▁\n\n\nArea_km2\n0\n1\n25.55\n42.58\n1.49\n5.04\n11.60\n28.51\n282.13\n▇▁▁▁▁\n\n\n\n\n\nThe dependent variable is continuous.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#check-the-assumptions",
    "href": "linear-regression.html#check-the-assumptions",
    "title": "6  Multiple Linear Regression",
    "section": "6.3 Check the assumptions",
    "text": "6.3 Check the assumptions\nBefore running the model, you need to check if the assumptions are met.\n\nThe dependent variable is normally distributed\nLinear relationship between the dependent variable and the independent variables\nNo multicollinearity between independent variables (or only very little)\nThe observations are independent\nConstant Variance (Assumption of Homoscedasticity)\nResiduals are normally distributed",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#assumption-1---normal-distribution",
    "href": "linear-regression.html#assumption-1---normal-distribution",
    "title": "6  Multiple Linear Regression",
    "section": "6.4 Assumption 1 - Normal distribution",
    "text": "6.4 Assumption 1 - Normal distribution\nThe Dependent Variable is be normally distributed.\nCheck the histogram of Car_perc:\n\nhist(data$Car_perc)\n\n\n\n\n\n\n\n\nIf the sample is small (&lt; 50 observations), we use Shapiro-Wilk test:\n\nshapiro.test(data$Car_perc)\n\n\n    Shapiro-Wilk normality test\n\ndata:  data$Car_perc\nW = 0.97284, p-value = 0.0001709\n\n\nIf not, use the Kolmogorov-Smirnov test:\n\nks.test(\n  data$Car_perc,\n  \"pnorm\",\n  mean = mean(data$Car_perc),\n  sd = sd(data$Car_perc)\n)\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  data$Car_perc\nD = 0.072477, p-value = 0.1675\nalternative hypothesis: two-sided\n\n\nThe null hypothesis for both tests is that the distribution is normal. Therefore, for the distribution to be normal, the pvalue must be &gt; 0.05 and the null hypothesis is not rejected. From the output obtained we can assume normality.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#assumption-2---linear-relationship",
    "href": "linear-regression.html#assumption-2---linear-relationship",
    "title": "6  Multiple Linear Regression",
    "section": "6.5 Assumption 2 - Linear relationship",
    "text": "6.5 Assumption 2 - Linear relationship\nThere is a linear relationship between dependent variable (DV) and independent variables (IV).\nWe can check this assumption by plotting scatterplots of the DV against each IV:\n\nplot(x = data$Car_perc, y = data$Total, xlab = \"Car_perc (%)\", ylab = \"Total (number of trips)\")  \nplot(x = data$Car_perc, y = data$Walk, xlab = \"Car_perc\", ylab = \"Walk\")  \nplot(x = data$Car_perc, y = data$Bike, xlab = \"Car_perc\", ylab = \"Bike\")  \nplot(x = data$Car_perc, y = data$Car, xlab = \"Car_perc\", ylab = \"Car\")  \nplot(x = data$Car_perc, y = data$PTransit, xlab = \"Car_perc\", ylab = \"PTransit\")\nplot(x = data$Car_perc, y = data$Other, xlab = \"Car_perc\", ylab = \"Other\")\nplot(x = data$Car_perc, y = data$Distance, xlab = \"Car_perc\", ylab = \"Distance\")\nplot(x = data$Car_perc, y = data$Duration, xlab = \"Car_perc\", ylab = \"Duration\")\nplot(x = data$Car_perc, y = data$N_INDIVIDUOS, xlab = \"Car_perc\", ylab = \"N_INDIVIDUOS\")\nplot(x = data$Car_perc, y = data$Male_perc, xlab = \"Car_perc\", ylab = \"Male_perc\")\nplot(x = data$Car_perc, y = data$IncomeHH, xlab = \"Car_perc\", ylab = \"IncomeHH\")\nplot(x = data$Car_perc, y = data$Nvehicles, xlab = \"Car_perc\", ylab = \"Nvehicles\")\nplot(x = data$Car_perc, y = data$DrivingLic, xlab = \"Car_perc\", ylab = \"Driving License\")\nplot(x = data$Car_perc, y = data$CarParkFree_Work, xlab = \"Car_perc\", ylab = \"Free car parking at work\")\nplot(x = data$Car_perc, y = data$PTpass, xlab = \"Car_perc\", ylab = \"PTpass\")\nplot(x = data$Car_perc, y = data$internal, xlab = \"Car_perc\", ylab = \"internal trips\")\nplot(x = data$Car_perc, y = data$Lisboa, xlab = \"Car_perc\", ylab = \"Lisboa\")\nplot(x = data$Car_perc, y = data$Area_km2, xlab = \"Car_perc\", ylab = \"Area_km2\")\n\nOr you can make a pairwise scatterplot matrix, that compares every variable with each other:\n\n# pairs(data_continuous, pch = 19, lower.panel = NULL) # we have too many variables, let's split the plots\npairs(data_continuous[,1:6], pch = 19, lower.panel = NULL)\n\n\n\n\n\n\n\npairs(data_continuous[,7:12], pch = 19, lower.panel = NULL)\n\n\n\n\n\n\n\npairs(data_continuous[,13:17], pch = 19, lower.panel = NULL)",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#assumption-3---no-multicollinearity",
    "href": "linear-regression.html#assumption-3---no-multicollinearity",
    "title": "6  Multiple Linear Regression",
    "section": "6.6 Assumption 3 - No multicollinearity",
    "text": "6.6 Assumption 3 - No multicollinearity\nCheck the correlation plot before choosing the variables.\n\n6.6.1 Declare the model\n\n\n\n\n\n\nUse CTRL + SHIFT + C to comment/uncomment lines (variables)\n\n\n\n\n# names(data) # to see the names of the variables\n\nmodel = lm(\n  Car_perc ~ Total +\n    Walk +\n    Bike +\n    Car +\n    PTransit +\n    Other +\n    Distance + \n    Duration + \n    N_INDIVIDUOS + \n    Male_perc + \n    IncomeHH + \n    Nvehicles + \n    DrivingLic + \n    CarParkFree_Work + \n    PTpass + \n    internal + \n    Lisboa + \n    Area_km2,\n  data = data\n)\n\nsummary(model)\n\n\nCall:\nlm(formula = Car_perc ~ Total + Walk + Bike + Car + PTransit + \n    Other + Distance + Duration + N_INDIVIDUOS + Male_perc + \n    IncomeHH + Nvehicles + DrivingLic + CarParkFree_Work + PTpass + \n    internal + Lisboa + Area_km2, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-37.952  -4.907  -0.232   5.305  39.157 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       4.047e+00  3.192e+01   0.127  0.89923    \nTotal             1.633e-01  3.671e-01   0.445  0.65700    \nWalk             -1.640e-01  3.671e-01  -0.447  0.65552    \nBike             -1.614e-01  3.669e-01  -0.440  0.66044    \nCar              -1.627e-01  3.671e-01  -0.443  0.65806    \nPTransit         -1.640e-01  3.672e-01  -0.447  0.65552    \nOther            -1.622e-01  3.671e-01  -0.442  0.65897    \nDistance          6.004e-01  3.629e-01   1.654  0.09948 .  \nDuration         -3.244e-01  3.325e-01  -0.976  0.33032    \nN_INDIVIDUOS     -6.627e-05  8.876e-05  -0.747  0.45612    \nMale_perc         2.723e-01  6.151e-01   0.443  0.65849    \nIncomeHH         -5.218e-04  2.199e-03  -0.237  0.81264    \nNvehicles         7.301e+00  3.959e+00   1.844  0.06650 .  \nDrivingLic        2.910e-01  1.161e-01   2.506  0.01294 *  \nCarParkFree_Work  1.943e-01  7.882e-02   2.465  0.01449 *  \nPTpass           -8.254e-02  9.989e-02  -0.826  0.40956    \ninternalNo        1.962e+01  2.136e+00   9.182  &lt; 2e-16 ***\nLisboaYes        -9.647e+00  3.182e+00  -3.031  0.00273 ** \nArea_km2          1.689e-02  1.860e-02   0.908  0.36486    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.59 on 217 degrees of freedom\nMultiple R-squared:  0.7772,    Adjusted R-squared:  0.7587 \nF-statistic: 42.06 on 18 and 217 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n6.6.2 Assessing the model\n\nFirst check the pvalue and the F statistics of the model to see if there is any statistical relation between the dependent variable and the independent variables. If pvalue &lt; 0.05 and the F statistics &gt; Fcritical = 2,39, then the model is statistically acceptable.\nThe R-square and Adjusted R-square evaluate the amount of variance that is explained by the model. The difference between one and another is that the R-square does not consider the number of variables. If you increase the number of variables in the model, the R-square will tend to increase which can lead to overfitting. On the other hand, the Adjusted R-square adjust to the number of independent variables.\nTake a look at the t-value and the Pr(&gt;|t|). If the t-value &gt; 1,96 or Pr(&gt;|t|) &lt; 0,05, then the IV is statistically significant to the model.\nTo analyze the estimates of the variables, you should first check the signal and assess if the independent variable has a direct or inverse relationship with the dependent variable. It is only possible to evaluate the magnitude of the estimate if all variables are continuous and standardized or by calculating the elasticities.\n\nWe can see from the output that the R-squared value for the model (with ALL variables) is 0.7772. We can also see that the overall F-statistic is 42.06 and the corresponding p-value is &lt;2.2e-16, which indicates that the overall regression model is significant. Also, the predictor variables DrivingLic and CarParkFree_Work, internal(No) and Lisboa(Yes) are statistically significant at the 0.05 significance level.\n\n\n\n\n\n\nYour turn\n\n\n\nNow try to remove some variables from the model and assess it again. Elaborate a justification to exclude those variables.\n\n\n\n\n6.6.3 Calculate the Variance Inflation Factor (VIF)\nWe use the vif() function from the car package to calculate the VIF for each predictor variable in the model:\n\ncar::vif(model)\n\n           Total             Walk             Bike              Car \n    1.028324e+08     1.094004e+07     1.743544e+04     4.307486e+07 \n        PTransit            Other         Distance         Duration \n    8.443136e+06     3.186972e+04     1.957062e+00     3.544018e+00 \n    N_INDIVIDUOS        Male_perc         IncomeHH        Nvehicles \n    4.459671e+00     1.961472e+00     2.079348e+00     1.882275e+00 \n      DrivingLic CarParkFree_Work           PTpass         internal \n    1.863227e+00     2.660375e+00     3.464699e+00     2.400178e+00 \n          Lisboa         Area_km2 \n    3.451852e+00     1.313708e+00 \n\n\nA common rule of thumb is that a VIF value greater than 5 indicates a high level of multicollinearity among the predictor variables, which is potentially concerning.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#assumption-4-independence-of-observations",
    "href": "linear-regression.html#assumption-4-independence-of-observations",
    "title": "6  Multiple Linear Regression",
    "section": "6.7 Assumption 4: independence of observations",
    "text": "6.7 Assumption 4: independence of observations\nMultiple linear regression assumes that each observation in the dataset is independent.\nThe error (E) is independent across observations and the error variance is constant across IV\nThe simplest way to determine if this assumption is met is to perform a Durbin-Watson test, which is a formal statistical test that tells us whether or not the residuals (and thus the observations) exhibit autocorrelation.\n\ndurbinWatsonTest(model)\n\n lag Autocorrelation D-W Statistic p-value\n   1      0.07881007      1.840798    0.11\n Alternative hypothesis: rho != 0\n\n\nH0 (null hypothesis): There is no correlation among the residuals. Since p-value &gt; 0.05, we do not reject the null hypothesis and we can not discard that there is autocorrelation in the model.\n\n\n\n\n\n\nNote\n\n\n\nIn the Durbin-Watson test, values of the D-W Statistic vary from 0 to 4.\nIf the values are from 1.8 to 2.2 this means that there is no autocorrelation in the model.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#assumption-5-constant-variance-homoscedasticity",
    "href": "linear-regression.html#assumption-5-constant-variance-homoscedasticity",
    "title": "6  Multiple Linear Regression",
    "section": "6.8 Assumption 5: Constant Variance (Homoscedasticity)",
    "text": "6.8 Assumption 5: Constant Variance (Homoscedasticity)\nThe simplest way to determine if this assumption is met is to create a plot of standardized residuals versus predicted values.\n\nplot(model)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the Residuals, check the following assumptions:\n\nResiduals vs Fitted: This plot is used to detect non-linearity, heteroscedasticity, and outliers.\nNormal Q-Q: The quantile-quantile (Q-Q) plot is used to check if the disturbances follow a normal distribution\nScale-Location: This plot is used to verify if the residuals are spread equally (homoscedasticity) or not (heteroscedasticity) through the sample.\nResiduals vs Leverage: This plot is used to detect the impact of the outliers in the model. If the outliers are outside the Cook-distance, this may lead to serious problems in the model.\n\nTry analyzing the plots and check if the model meets the assumptions.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#assumption-6-residuals-are-normally-distributed",
    "href": "linear-regression.html#assumption-6-residuals-are-normally-distributed",
    "title": "6  Multiple Linear Regression",
    "section": "6.9 Assumption 6: Residuals are normally distributed",
    "text": "6.9 Assumption 6: Residuals are normally distributed\nAssess the Q-Q Residuals plot above. When the residuals clearly depart from a straight diagonal line, it indicates that they do not follow a normal distribution.\nUse a formal statistical test like Shapiro-Wilk, Kolmogorov-Smironov to validate those results.\n\n\n\n\n\n\nINE. 2018. “Mobilidade e Funcionalidade Do Território Nas Áreas Metropolitanas do Porto e de Lisboa: 2017.” Lisboa: Instituto National de Estatística. https://www.ine.pt/xportal/xmain?xpid=INE&xpgid=ine_publicacoes&PUBLICACOESpub_boui=349495406&PUBLICACOESmodo=2&xlang=pt.\n\n\n———. 2022. “Censos 2021- XVI Recenseamento Geral da População. VI Recenseamento Geral da Habitação.” Lisboa: Instituto National de Estatística. https://censos.ine.pt/xurl/pub/65586079.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#assumption-1-normal-distribution",
    "href": "linear-regression.html#assumption-1-normal-distribution",
    "title": "6  Multiple Linear Regression",
    "section": "6.4 Assumption 1: Normal distribution",
    "text": "6.4 Assumption 1: Normal distribution\nThe Dependent Variable is be normally distributed.\nCheck the histogram of Car_perc:\n\nhist(data$Car_perc)\n\n\n\n\n\n\n\n\nIf the sample is small (&lt; 50 observations), we use Shapiro-Wilk test:\n\nshapiro.test(data$Car_perc)\n\n\n    Shapiro-Wilk normality test\n\ndata:  data$Car_perc\nW = 0.97284, p-value = 0.0001709\n\n\nIf not, use the Kolmogorov-Smirnov test:\n\nks.test(\n  data$Car_perc,\n  \"pnorm\",\n  mean = mean(data$Car_perc),\n  sd = sd(data$Car_perc)\n)\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  data$Car_perc\nD = 0.072477, p-value = 0.1675\nalternative hypothesis: two-sided\n\n\nThe null hypothesis for both tests is that the distribution is normal. Therefore, for the distribution to be normal, the pvalue must be &gt; 0.05 and the null hypothesis is not rejected. From the output obtained we can assume normality.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#assumption-2-linear-relationship",
    "href": "linear-regression.html#assumption-2-linear-relationship",
    "title": "6  Multiple Linear Regression",
    "section": "6.5 Assumption 2: Linear relationship",
    "text": "6.5 Assumption 2: Linear relationship\nThere is a linear relationship between dependent variable (DV) and independent variables (IV).\nWe can check this assumption by plotting scatterplots of the DV against each IV:\n\nplot(x = data$Car_perc, y = data$Total, xlab = \"Car_perc (%)\", ylab = \"Total (number of trips)\")  \nplot(x = data$Car_perc, y = data$Walk, xlab = \"Car_perc\", ylab = \"Walk\")  \nplot(x = data$Car_perc, y = data$Bike, xlab = \"Car_perc\", ylab = \"Bike\")  \nplot(x = data$Car_perc, y = data$Car, xlab = \"Car_perc\", ylab = \"Car\")  \nplot(x = data$Car_perc, y = data$PTransit, xlab = \"Car_perc\", ylab = \"PTransit\")\nplot(x = data$Car_perc, y = data$Other, xlab = \"Car_perc\", ylab = \"Other\")\nplot(x = data$Car_perc, y = data$Distance, xlab = \"Car_perc\", ylab = \"Distance\")\nplot(x = data$Car_perc, y = data$Duration, xlab = \"Car_perc\", ylab = \"Duration\")\nplot(x = data$Car_perc, y = data$N_INDIVIDUOS, xlab = \"Car_perc\", ylab = \"N_INDIVIDUOS\")\nplot(x = data$Car_perc, y = data$Male_perc, xlab = \"Car_perc\", ylab = \"Male_perc\")\nplot(x = data$Car_perc, y = data$IncomeHH, xlab = \"Car_perc\", ylab = \"IncomeHH\")\nplot(x = data$Car_perc, y = data$Nvehicles, xlab = \"Car_perc\", ylab = \"Nvehicles\")\nplot(x = data$Car_perc, y = data$DrivingLic, xlab = \"Car_perc\", ylab = \"Driving License\")\nplot(x = data$Car_perc, y = data$CarParkFree_Work, xlab = \"Car_perc\", ylab = \"Free car parking at work\")\nplot(x = data$Car_perc, y = data$PTpass, xlab = \"Car_perc\", ylab = \"PTpass\")\nplot(x = data$Car_perc, y = data$internal, xlab = \"Car_perc\", ylab = \"internal trips\")\nplot(x = data$Car_perc, y = data$Lisboa, xlab = \"Car_perc\", ylab = \"Lisboa\")\nplot(x = data$Car_perc, y = data$Area_km2, xlab = \"Car_perc\", ylab = \"Area_km2\")\n\nOr you can make a pairwise scatterplot matrix, that compares every variable with each other:\n\n# pairs(data_continuous, pch = 19, lower.panel = NULL) # we have too many variables, let's split the plots\npairs(data_continuous[,1:6], pch = 19, lower.panel = NULL)\n\n\n\n\n\n\n\npairs(data_continuous[,7:12], pch = 19, lower.panel = NULL)\n\n\n\n\n\n\n\npairs(data_continuous[,13:17], pch = 19, lower.panel = NULL)",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#assumption-3-no-multicollinearity",
    "href": "linear-regression.html#assumption-3-no-multicollinearity",
    "title": "6  Multiple Linear Regression",
    "section": "6.6 Assumption 3: No multicollinearity",
    "text": "6.6 Assumption 3: No multicollinearity\nCheck the correlation plot before choosing the variables.\n\nDeclare the model\n\n\n\n\n\n\nUse CTRL + SHIFT + C to comment/uncomment lines (variables)\n\n\n\n\n# names(data) # to see the names of the variables\n\nmodel = lm(\n  Car_perc ~ Total +\n    Walk +\n    Bike +\n    Car +\n    PTransit +\n    Other +\n    Distance + \n    Duration + \n    N_INDIVIDUOS + \n    Male_perc + \n    IncomeHH + \n    Nvehicles + \n    DrivingLic + \n    CarParkFree_Work + \n    PTpass + \n    internal + \n    Lisboa + \n    Area_km2,\n  data = data\n)\n\nsummary(model)\n\n\nCall:\nlm(formula = Car_perc ~ Total + Walk + Bike + Car + PTransit + \n    Other + Distance + Duration + N_INDIVIDUOS + Male_perc + \n    IncomeHH + Nvehicles + DrivingLic + CarParkFree_Work + PTpass + \n    internal + Lisboa + Area_km2, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-37.952  -4.907  -0.232   5.305  39.157 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       4.047e+00  3.192e+01   0.127  0.89923    \nTotal             1.633e-01  3.671e-01   0.445  0.65700    \nWalk             -1.640e-01  3.671e-01  -0.447  0.65552    \nBike             -1.614e-01  3.669e-01  -0.440  0.66044    \nCar              -1.627e-01  3.671e-01  -0.443  0.65806    \nPTransit         -1.640e-01  3.672e-01  -0.447  0.65552    \nOther            -1.622e-01  3.671e-01  -0.442  0.65897    \nDistance          6.004e-01  3.629e-01   1.654  0.09948 .  \nDuration         -3.244e-01  3.325e-01  -0.976  0.33032    \nN_INDIVIDUOS     -6.627e-05  8.876e-05  -0.747  0.45612    \nMale_perc         2.723e-01  6.151e-01   0.443  0.65849    \nIncomeHH         -5.218e-04  2.199e-03  -0.237  0.81264    \nNvehicles         7.301e+00  3.959e+00   1.844  0.06650 .  \nDrivingLic        2.910e-01  1.161e-01   2.506  0.01294 *  \nCarParkFree_Work  1.943e-01  7.882e-02   2.465  0.01449 *  \nPTpass           -8.254e-02  9.989e-02  -0.826  0.40956    \ninternalNo        1.962e+01  2.136e+00   9.182  &lt; 2e-16 ***\nLisboaYes        -9.647e+00  3.182e+00  -3.031  0.00273 ** \nArea_km2          1.689e-02  1.860e-02   0.908  0.36486    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.59 on 217 degrees of freedom\nMultiple R-squared:  0.7772,    Adjusted R-squared:  0.7587 \nF-statistic: 42.06 on 18 and 217 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nAssessing the model\n\nFirst check the pvalue and the F statistics of the model to see if there is any statistical relation between the dependent variable and the independent variables. If pvalue &lt; 0.05 and the F statistics &gt; Fcritical = 2.39, then the model is statistically acceptable.\nThe R-square and Adjusted R-square evaluate the amount of variance that is explained by the model. The difference between one and another is that the R-square does not consider the number of variables. If you increase the number of variables in the model, the R-square will tend to increase which can lead to overfitting. On the other hand, the Adjusted R-square adjust to the number of independent variables.\nTake a look at the t-value and the Pr(&gt;|t|). If the t-value &gt; 1.96 or Pr(&gt;|t|) &lt; 0.05, then the IV is statistically significant to the model.\nTo analyze the estimates of the variables, you should first check the signal and assess if the independent variable has a direct or inverse relationship with the dependent variable. It is only possible to evaluate the magnitude of the estimate if all variables are continuous and standardized or by calculating the elasticities. Do not forget to access the Intercept…\n\nWe can see from the output that the R-squared value for the model (with ALL variables) is 0.7772. We can also see that the overall F-statistic is 42.06 and the corresponding p-value is &lt;2.2e-16, which indicates that the overall regression model is significant. Also, the predictor variables DrivingLic and CarParkFree_Work, internal(No) and Lisboa(Yes) are statistically significant at the 0.05 significance level.\n\n\n\n\n\n\nYour turn\n\n\n\nNow try to remove some variables from the model and assess it again. Elaborate a justification to exclude those variables.\n\n\n\n\nCalculate the Variance Inflation Factor (VIF)\nWe use the vif() function from the car package to calculate the VIF for each predictor variable in the model:\n\ncar::vif(model)\n\n           Total             Walk             Bike              Car \n    1.028324e+08     1.094004e+07     1.743544e+04     4.307486e+07 \n        PTransit            Other         Distance         Duration \n    8.443136e+06     3.186972e+04     1.957062e+00     3.544018e+00 \n    N_INDIVIDUOS        Male_perc         IncomeHH        Nvehicles \n    4.459671e+00     1.961472e+00     2.079348e+00     1.882275e+00 \n      DrivingLic CarParkFree_Work           PTpass         internal \n    1.863227e+00     2.660375e+00     3.464699e+00     2.400178e+00 \n          Lisboa         Area_km2 \n    3.451852e+00     1.313708e+00 \n\n\nA common rule of thumb is that a VIF value greater than 5 indicates a high level of multicollinearity among the predictor variables, which is potentially concerning.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  }
]