[
  {
    "objectID": "r-basics.html",
    "href": "r-basics.html",
    "title": "3  R basics",
    "section": "",
    "text": "3.1 Math operations\nIn this chapter we will introduce to the R basics and some exercises to get familiar to how R works.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#math-operations",
    "href": "r-basics.html#math-operations",
    "title": "3  R basics",
    "section": "",
    "text": "Sum\n\n1+1\n\n[1] 2\n\n\n\n\nSubtraction\n\n5-2\n\n[1] 3\n\n\n\n\nMultiplication\n\n2*2\n\n[1] 4\n\n\n\n\nDivision\n\n8/2\n\n[1] 4\n\n\n\n\nRound the number\n\nround(3.14)\n\n[1] 3\n\nround(3.14, 1) # The \"1\" indicates to round it up to 1 decimal digit.\n\n[1] 3.1\n\n\nYou can use help ?round in the console to see the description of the function, and the default arguments.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#basic-shortpaths",
    "href": "r-basics.html#basic-shortpaths",
    "title": "3  R basics",
    "section": "3.2 Basic shortpaths",
    "text": "3.2 Basic shortpaths\n\nPerform Combinations\n\nc(1, 2, 3)\n\n[1] 1 2 3\n\nc(1:3) # The \":\" indicates a range between the first and second numbers. \n\n[1] 1 2 3\n\n\n\n\nCreate a comment with ctrl + shift + c\n\n# Comments help you organize your code. The software will not run the comment. \n\n\n\nCreate a table\nA simple table with the number of trips by car, PT, walking, and cycling in a hypothetical street segment at a certain period.\nDefine variables\n\nmodes &lt;- c(\"car\", \"PT\", \"walking\", \"cycling\") # you can use \"=\" or \"&lt;-\"\nTrips = c(200, 50, 300, 150) # key sensitive (uppercase and lowercase letters are different)\n\nJoin the variables to create a table\n\ntable_example = data.frame(modes, Trips)\n\nTake a look at the table\nVisualize the table by clicking on the “Data” in the “Environment” page or use :\n\nView(table_example)\n\nLook at the first row\n\ntable_example[1,] #rows and columns start from 1 in R, differently from Python which starts from 0.\n\n  modes Trips\n1   car   200\n\n\nLook at first row and column\n\ntable_example[1,1]\n\n[1] \"car\"",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#practical-exercise",
    "href": "r-basics.html#practical-exercise",
    "title": "3  R basics",
    "section": "3.3 Practical exercise",
    "text": "3.3 Practical exercise\nDataset: the number of trips between all municipalities in the Lisbon Metropolitan Area, Portugal (INE 2018).\n\nImport dataset\nYou can click directly in the file under the “Files” pan, or:\n\ndata = readRDS(\"data/TRIPSmode_mun.Rds\")\n\n\n\n\n\n\n\nAfter you type \" you can use tab to navigate between folders and files and enter to autocomplete.\n\n\n\n\n\nTake a first look at the data\nSummary statistics\n\nsummary(data)\n\n  Origin_mun        Destination_mun        Total             Walk       \n Length:315         Length:315         Min.   :     7   Min.   :     0  \n Class :character   Class :character   1st Qu.:   330   1st Qu.:     0  \n Mode  :character   Mode  :character   Median :  1090   Median :     0  \n                                       Mean   : 16825   Mean   :  4033  \n                                       3rd Qu.:  5374   3rd Qu.:     0  \n                                       Max.   :875144   Max.   :306289  \n      Bike              Car            PTransit            Other        \n Min.   :   0.00   Min.   :     0   Min.   :     0.0   Min.   :    0.0  \n 1st Qu.:   0.00   1st Qu.:   263   1st Qu.:     5.0   1st Qu.:    0.0  \n Median :   0.00   Median :   913   Median :   134.0   Median :    0.0  \n Mean   :  80.19   Mean   :  9956   Mean   :  2602.6   Mean   :  152.4  \n 3rd Qu.:   0.00   3rd Qu.:  4408   3rd Qu.:   975.5   3rd Qu.:   62.5  \n Max.   :5362.00   Max.   :349815   Max.   :202428.0   Max.   :11647.0  \n\n\nCheck the structure of the data\n\nstr(data)\n\nClasses 'tbl_df', 'tbl' and 'data.frame':   315 obs. of  8 variables:\n $ Origin_mun     : chr  \"Alcochete\" \"Alcochete\" \"Alcochete\" \"Alcochete\" ...\n $ Destination_mun: chr  \"Alcochete\" \"Almada\" \"Amadora\" \"Barreiro\" ...\n $ Total          : num  20478 567 188 867 114 ...\n $ Walk           : num  6833 0 0 0 0 ...\n $ Bike           : num  320 0 0 0 0 0 0 0 91 0 ...\n $ Car            : num  12484 353 107 861 114 ...\n $ PTransit       : num  833 0 81 5 0 ...\n $ Other          : num  7 214 0 0 0 0 0 0 0 0 ...\n\n\nCheck the first values of each variable\n\ndata\n\n\nhead(data, 3) # first 3 values\n\n  Origin_mun Destination_mun Total Walk Bike   Car PTransit Other\n1  Alcochete       Alcochete 20478 6833  320 12484      833     7\n2  Alcochete          Almada   567    0    0   353        0   214\n3  Alcochete         Amadora   188    0    0   107       81     0\n\n\nCheck the number of rows (observations) and columns (variables)\n\nnrow(data)\n\n[1] 315\n\nncol(data)\n\n[1] 8\n\n\nOpen the dataset\n\nView(data)\n\n\n\nExplore the data\nCheck the total number of trips\nUse $ to select a variable of the data\n\nsum(data$Total)\n\n[1] 5299853\n\n\nPercentage of car trips related to the total\n\nsum(data$Car)/sum(data$Total) * 100\n\n[1] 59.17638\n\n\nPercentage of active trips related to the total\n\n(sum(data$Walk) + sum(data$Bike)) / sum(data$Total) * 100\n\n[1] 24.44883\n\n\n\n\nModify original data\nCreate a column with the sum of the number of trips for active modes\n\ndata$Active = data$Walk + data$Bike\n\nFilter by condition (create new tables)\nFilter trips only with origin from Lisbon\n\ndata_Lisbon = data[data$Origin == \"Lisboa\",]\n\nFilter trips with origin different from Lisbon\n\ndata_out_Lisbon = data[data$Origin != \"Lisboa\",]\n\nFilter trips with origin and destination in Lisbon\n\ndata_in_Out_Lisbon = data[data$Origin == \"Lisboa\" & data$Destination == \"Lisboa\",]\n\nRemove the first column\n\ndata = data[ ,-1] #first column\n\nCreate a table only with origin, destination and walking trips\nThere are many ways to do the same operation.\n\nnames(data)\n\n[1] \"Destination_mun\" \"Total\"           \"Walk\"            \"Bike\"           \n[5] \"Car\"             \"PTransit\"        \"Other\"           \"Active\"         \n\n\n\ndata_walk2 = data[ ,c(1,2,4)]\n\n\ndata_walk3 = data[ ,-c(3,5:9)]\n\n\n\nExport data\nSave data in .csv and .Rds\n\nwrite.csv(data, 'data/dataset.csv', row.names = FALSE)\nsaveRDS(data, 'data/dataset.Rds') #Choose a different file. \n\n\n\nImport data\n\ncsv_file = read.csv(\"data/dataset.csv\")\nrds_file = readRDS(\"data/dataset.Rds\")\n\n\n\n\n\n\n\nINE. 2018. “Mobilidade e Funcionalidade Do Território Nas Áreas Metropolitanas do Porto e de Lisboa: 2017.” Lisboa: Instituto National de Estatística. https://www.ine.pt/xportal/xmain?xpid=INE&xpgid=ine_publicacoes&PUBLICACOESpub_boui=349495406&PUBLICACOESmodo=2&xlang=pt.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R basics</span>"
    ]
  },
  {
    "objectID": "spatial-data.html",
    "href": "spatial-data.html",
    "title": "9  Introduction to spatial data",
    "section": "",
    "text": "9.1 Import vector data\nSpatial data is data that is associated with a geometry. This geometry can be a point, a line, a polygon, or a grid.\nSpatial data can be represented in many ways, such as vector data and raster data. In this tutorial, we will learn how to work with spatial data in R.\nWe will use the sf package to work with vector data, and the dplyr package to manipulate data.\nThe sf package is a powerful package for working with spatial data in R. It includes hundreds of functions to deal with spatial data (Pebesma and Bivand 2023).\nDownload and open Municipalities_geo.gpkg under EITcourse/data repository.\nWithin the sf package, we use the st_read() to read spatial features.\nCode\nMunicipalities_geo = st_read(\"data/Municipalities_geo.gpkg\")",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to spatial data</span>"
    ]
  },
  {
    "objectID": "spatial-data.html#import-vector-data",
    "href": "spatial-data.html#import-vector-data",
    "title": "9  Introduction to spatial data",
    "section": "",
    "text": "You can also open directly from url from github. Example:\nurl = \"https://github.com/U-Shift/EITcourse/raw/main/data/Municipalities_geo.gpkg\" Municipalities_geo = st_read(url)\n\n\n\n\n9.1.1 Projected vs Geographic Coordinate Systems\nA projected coordinate system is a flat representation of the Earth’s surface. A geographic coordinate system is a spherical representation of the Earth’s surface.\n\n\n\nSource: ESRI\n\n\nThe st_crs() function can be used to check the coordinate reference system of a spatial object.\n\n\nCode\nst_crs(Municipalities_geo)\n\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        MEMBER[\"World Geodetic System 1984 (G2296)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\nWGS84 is the most common geographic coordinate system, used in GPS, and EPSG:4326 is code for it.\nIf we want to project the data to a projected coordinate system, to use metric units instead of degrees, we can use the st_transform() function.\nIn this case, the EPGS:3857 is the code for the Pseudo-Mercator coordinate system.\n\n\nCode\nMunicipalities_projected = st_transform(Municipalities_geo, crs = 3857)\n\n\nNow see the differences when calling Municipalities_geo and Municipalities_projected.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to spatial data</span>"
    ]
  },
  {
    "objectID": "spatial-data.html#join-geometries-to-data-frames",
    "href": "spatial-data.html#join-geometries-to-data-frames",
    "title": "9  Introduction to spatial data",
    "section": "9.2 Join geometries to data frames",
    "text": "9.2 Join geometries to data frames\nImport TRIPSmun.Rds file and check data class\n\n\nCode\nTRIPSmun = readRDS(\"../data/TRIPSmun.Rds\")\nclass(TRIPSmun)\n\n\n[1] \"data.frame\"\n\n\nCode\nclass(Municipalities_geo)\n\n\n[1] \"sf\"         \"data.frame\"\n\n\nTo join the geometries from the Municipalities_geo to the data frame, we can use the left_join() function from the dplyr package.\n\n\nCode\nTRIPSgeo =\n  TRIPSmun |&gt; \n  left_join(Municipalities_geo)\n\nclass(TRIPSgeo)\n\n\n[1] \"data.frame\"\n\n\nAs you can see, this does not make the object a spatial feature. To do this, we need to use the st_as_sf() function.\n\n\nCode\nTRIPSgeo = TRIPSgeo |&gt; st_as_sf()\nclass(TRIPSgeo)\n\n\n[1] \"sf\"         \"data.frame\"\n\n\nNow we have a spatial feature with the data frame.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to spatial data</span>"
    ]
  },
  {
    "objectID": "spatial-data.html#create-spatial-data-from-coordinates",
    "href": "spatial-data.html#create-spatial-data-from-coordinates",
    "title": "9  Introduction to spatial data",
    "section": "9.3 Create spatial data from coordinates",
    "text": "9.3 Create spatial data from coordinates\nThe st_as_sf() function can also be used to create a spatial feature from a data frame with coordinates. In that case, we need to specify the columns with the coordinates.\nWe will use survey data (in .txt) with the participants’ home latitude/longitude coordinates to create a spatial feature.\n\n\nCode\nSURVEY = read.csv(\"../data/SURVEY.txt\", sep = \"\\t\") # tab delimiter\nclass(SURVEY)\n\n\n[1] \"data.frame\"\n\n\nCode\nSURVEYgeo = st_as_sf(SURVEY, coords = c(\"lon\", \"lat\"), crs = 4326) # create spatial feature\nclass(SURVEYgeo)\n\n\n[1] \"sf\"         \"data.frame\"\n\n\nWe can also set the crs of the spatial feature on the fly.\nCheck the differences between both data variables.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to spatial data</span>"
    ]
  },
  {
    "objectID": "spatial-data.html#visuzlize-spatial-data",
    "href": "spatial-data.html#visuzlize-spatial-data",
    "title": "9  Introduction to spatial data",
    "section": "9.4 Visuzlize spatial data",
    "text": "9.4 Visuzlize spatial data\nRepresent Transport Zones with Total and Car, using plot().\n\n\nCode\nplot(TRIPSgeo) # all variables\n\n\n\n\n\n\n\n\n\nCode\nplot(TRIPSgeo[\"Municipality\"])\n\n\n\n\n\n\n\n\n\nCode\nplot(TRIPSgeo[\"Total\"])\n\n\n\n\n\n\n\n\n\nCode\nplot(TRIPSgeo[\"Car\"])\n\n\n\n\n\n\n\n\n\nCode\n# plot pointy data\nplot(SURVEYgeo)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the next chapter we will learn how to create interactive maps.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to spatial data</span>"
    ]
  },
  {
    "objectID": "spatial-data.html#export-spatial-data",
    "href": "spatial-data.html#export-spatial-data",
    "title": "9  Introduction to spatial data",
    "section": "9.5 Export spatial data",
    "text": "9.5 Export spatial data\nYou can save your spatial data in different formats using the function st_write(), such as shapefiles (ESRI), GeoJSON, and GeoPackage.\nThis is also useful to convert spatial data between formats.\n\n\nCode\nst_write(TRIPSgeo, \"data/TRIPSgeo.gpkg\") # as geopackage\nst_write(TRIPSgeo, \"data/TRIPSgeo.shp\") # as shapefile\nst_write(TRIPSgeo, \"data/TRIPSgeo.geojson\") # as geojson\nst_write(TRIPSgeo, \"data/TRIPSgeo.csv\", layer_options = \"GEOMETRY=AS_WKT\") # as csv, with WKT geometry\n\n\n\n\n\n\n\n\nIf you already have a file with the same name, you can use the delete_dns = TRUE argument to overwrite it.\n\n\n\n\n\n\n\n\n\nPebesma, Edzer, and Roger Bivand. 2023. Spatial Data Science: With Applications in R. Boca Raton: Chapman; Hall/CRC. https://doi.org/10.1201/9780429459016.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to spatial data</span>"
    ]
  },
  {
    "objectID": "interactive-maps.html",
    "href": "interactive-maps.html",
    "title": "15  Interactive maps",
    "section": "",
    "text": "15.1 Mapview\nYou can plot a static map using plof(sf), but you can also create interactive maps.\nInteractive maps are useful to explore the data, as you can zoom in and out, and click on the points to see the data associated with them.\nThere are several R packages to create interactive maps. For instance, the tmap package, the leaflet package, and the mapview package.\nMapview allows to create quick interactive maps, only by declaring the function mapview().\nCode\nlibrary(mapview)\nmapview(TRIPSgeo)\nTo color the points by a variable, you can use the zcol argument.\nCode\nmapview(TRIPSgeo, zcol = \"Total\")\nAs you can see, a color palette is automatically assigned to the continuous variable.\nTry to use a categorical variable.\nCode\nmapview(TRIPSgeo,\n        zcol = \"Municipality\", # depending on the variable\n        alpha.regions = 0.4, # also add transparency\n        color = \"white\" # border color\n        )\nYou can go crazy with all the options that mapview offers. Please refer to the documentation to see all the options.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Interactive maps</span>"
    ]
  },
  {
    "objectID": "interactive-maps.html#mapview",
    "href": "interactive-maps.html#mapview",
    "title": "15  Interactive maps",
    "section": "",
    "text": "Note that you can change the basemap, and click on the geometries to see the data associated with them.\n\n\n\n\n\n\n15.1.1 Export\nYou can directly export the map as an html file or image, using the Viewer panel.\n\n\n\n\n\n\n\nThis is the most straightforward solution.\n\n\n\nYou can also export a map as an html file or image using code.\n\n\nCode\n# install.packages(\"webshot2\") # you will need this\n\nmap = mapview(TRIPSgeo, zcol = \"Total\") # fisrt create a objet with the desired map\n\nmapshot2(map, \"data/map.html\") # as webpage\nmapshot2(map, file = \"data/map.png\") # as image",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Interactive maps</span>"
    ]
  },
  {
    "objectID": "interactive-maps.html#rmarkdown",
    "href": "interactive-maps.html#rmarkdown",
    "title": "15  Interactive maps",
    "section": "15.2 Rmarkdown",
    "text": "15.2 Rmarkdown\nTo include a map on a report, website, paper (any type), you can create an Rmarkdown file.\n\nAnd include a R code chunk (ctrl + alt + i) with a map. If the output is html, you will get an interactive map on your document!",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Interactive maps</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Engel, Claudia A. 2023. Introduction to r. cengel.github.io/R-intro/.\n\n\nINE. 2018. “Mobilidade e Funcionalidade Do Território Nas Áreas Metropolitanas do Porto e de Lisboa:\n2017.” Lisboa: Instituto National de Estatística. https://www.ine.pt/xportal/xmain?xpid=INE&xpgid=ine_publicacoes&PUBLICACOESpub_boui=349495406&PUBLICACOESmodo=2&xlang=pt.\n\n\n———. 2022. “Censos 2021- XVI Recenseamento\nGeral da População. VI Recenseamento Geral da Habitação.”\nLisboa: Instituto National de Estatística. https://censos.ine.pt/xurl/pub/65586079.\n\n\nLovelace, Robin, and Richard Ellison. 2018. “Stplanr: A Package\nfor Transport Planning.” The R Journal 10\n(2): 10. https://doi.org/10.32614/RJ-2018-053.\n\n\nLovelace, Robin, Rosa Félix, and Dustin Carlino. 2022. “Jittering:\nA Computationally Efficient Method for Generating Realistic Route\nNetworks from Origin-Destination Data.” Findings, April.\nhttps://doi.org/10.32866/001c.33873.\n\n\nLovelace, Robin, and Malcolm Morgan. 2024. Od: Manipulate and Map\nOrigin-Destination Data. https://github.com/itsleeds/od.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2024.\nGeocomputation with r. Second. Chapman; Hall/CRC. https://r.geocompx.org/.\n\n\nPebesma, Edzer, and Roger Bivand. 2023. Spatial Data Science: With\nApplications in R. Boca Raton: Chapman; Hall/CRC. https://doi.org/10.1201/9780429459016.\n\n\nPereira, Rafael H. M., Marcus Saraiva, Daniel Herszenhut, Carlos Kaue\nVieira Braga, and Matthew Wigginton Conway. 2021. “R5r: Rapid\nRealistic Routing on Multimodal Transport Networks with r5 in\nr.” Findings, March. https://doi.org/10.32866/001c.21262.\n\n\nPereira, Rafael HM, and Daniel Herszenhut. 2023. Introduction to\nUrban Accessibility: A Practical Guide with r. Instituto de\nPesquisa Econômica Aplicada (Ipea). https://ipeagit.github.io/intro_access_book/.\n\n\nZomorrodi, Ryan. 2024. Centr: Weighted and Unweighted Spatial\nCenters. https://ryanzomorrodi.github.io/centr/.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "data-manipulation.html",
    "href": "data-manipulation.html",
    "title": "4  Data manipulation",
    "section": "",
    "text": "4.1 Select variables\nIn this chapter we will use some very useful dplyr functions to handle and manipulate data.\nYou can load the dplyr package directly, or load the entire tidy universe (tidyverse).\nUsing the same dataset as in R basics but with slightly differences1.\nWe will do the same operations but in a simplified way.\nYou don’t need to know everything! And you don’t need to know by heart. The following functions are the ones you will probably use most of the time to handle data.\nHave a look at your dataset. You can open using View(), look at the information at the “Environment” panel, or even print the same information using glimpse()\nglimpse(TRIPS)\nWe will create a new dataset with Origin, Walk, Bike and Total. This time we will use the select() function.\nTRIPS_new = select(TRIPS, Origin, Walk, Bike, Total) # the first argument is the dataset\nThe first argument, as usually in R, is the dataset, and the remaining ones are the columns to select.\nWith most of the dplyr functions you don’t need to refer to data$... you can simply type the variable names (and even without the \"...\"!). This makes coding in R simpler :)\nYou can also remove columns that you don’t need.\nTRIPS_new = select(TRIPS_new, -Total) # dropping the Total column",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html#select-variables",
    "href": "data-manipulation.html#select-variables",
    "title": "4  Data manipulation",
    "section": "",
    "text": "4.1.1 Using pipes!\nNow, let’s introduce pipes. Pipes are a rule as: “With this, do this.”\nThis is useful to skip the first argument of the functions (usually the dataset to apply the function).\nApplying a pipe to the select() function, we can write as:\n\nTRIPS_new = TRIPS |&gt; select(Origin, Walk, Bike, Total)\n\nTwo things to note:\n\nThe pipe symbol can be written as |&gt; or %&gt;%. 2 To write it you may also use the ctrl+shift+m shortcut.\nAfter typing select( you can press tab and the list of available variables of that dataset will show up! Enter to select. With this you prevent typo errors.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html#filter-observations",
    "href": "data-manipulation.html#filter-observations",
    "title": "4  Data manipulation",
    "section": "4.2 Filter observations",
    "text": "4.2 Filter observations\nYou can filter observations based on a condition using the filter() function.\n\nTRIPS2 = TRIPS[TRIPS$Total &gt; 25000,] # using r-base, you cant forget the comma\nTRIPS2 = TRIPS2 |&gt; filter(Total &gt; 25000) # using dplyr, it's easier\n\nYou can have other conditions inside the condition.\n\nsummary(TRIPS$Total)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    361    5918   17474   22457   33378  112186 \n\nTRIPS3 = TRIPS |&gt; filter(Total &gt; median(Total)) \n\nOther filter conditions:\n\n== equal, != different\n&lt; smaller, &gt; greater, &lt;= smaller or equal, &gt;= greater or equal\n& and, | or\nis.na, !is.na is not NA\n%in%, !%in% not in",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html#create-new-variables",
    "href": "data-manipulation.html#create-new-variables",
    "title": "4  Data manipulation",
    "section": "4.3 Create new variables",
    "text": "4.3 Create new variables\nYou can also try again to create a variable of Car percentage using pipes! To create a new variable or change an existing one (overwriting), you can use the mutate() function.\n\nTRIPS$Car_perc = TRIPS$Car/TRIPS$Total * 100 # using r-base\n\nTRIPS = TRIPS |&gt; mutate(Car_perc = Car/Total * 100) # using dplyr",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html#change-data-type",
    "href": "data-manipulation.html#change-data-type",
    "title": "4  Data manipulation",
    "section": "4.4 Change data type",
    "text": "4.4 Change data type\nData can be in different formats. For example, the variable Origin is a character, but we can convert it to a numeric variable.\n\nclass(TRIPS$Origin)\n\n[1] \"character\"\n\nTRIPS = TRIPS |&gt; \n  mutate(Origin_num = as.integer(Origin)) # you can use as.numeric() as well\nclass(TRIPS$Origin_num)\n\n[1] \"integer\"\n\n\nMost used data types are:\n\ninteger (int)\nnumeric (num)\ncharacter (chr)\nlogical (logical)\ndate (Date)\nfactor (factor)\n\n\n4.4.1 Factors\nFactors are useful to deal with categorical data. You can convert a character to a factor using as.factor(), and also use labels and levels for categorical ordinal data.\nWe can change the Lisbon variable to a factor, and Internal too.\n\nTRIPS = TRIPS |&gt; \n  mutate(Lisbon_factor = factor(Lisbon, labels = c(\"No\", \"Yes\")),\n         Internal_factor = factor(Internal, labels = c(\"Inter\", \"Intra\")))\n\nBut how do we know which levels come first? A simple way is to use table() or unique() functions.\n\nunique(TRIPS$Lisbon) # this will show all the different values\n\n[1] 0 1\n\ntable(TRIPS$Lisbon) # this will show the frequency of each value\n\n\n  0   1 \n188  48 \n\ntable(TRIPS$Lisbon_factor)\n\n\n No Yes \n188  48 \n\n\nThe first number to appear is the first level, and so on.\nYou can see the difference between using a continuous variable (in this case Lisbon` has 0 and 1) and a categorical variable (Lisbon_factor).\n\nplot(TRIPS$Lisbon) # the values range between 0 and 1\n\n\n\n\n\n\n\nplot(TRIPS$Lisbon_factor) # the values are categorical and labeled with Yes/No",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html#join-data-tables",
    "href": "data-manipulation.html#join-data-tables",
    "title": "4  Data manipulation",
    "section": "4.5 Join data tables",
    "text": "4.5 Join data tables\nWhen having relational tables - i.e. with a common identifier - it is useful to be able to join them in a very efficient way.\nleft_join is a function that joins two tables by a common column. The first table is the one that will be kept, and the second one will be joined to it. How left_join works:\n\n\n\n\n\n\n\nA visual representation of the left join where every row in x appears in the output.Source: R for Data Science.\n\n\nLet’s join the municipalities to this table with a supporting table that includes all the relation between neighbourhoods and municipalities, and the respective names and codes.\n\nMunicipalities = readRDS(\"data/Municipalities_names.Rds\")\n\n\nhead(TRIPS)\n\n# A tibble: 6 × 13\n  Origin Total  Walk  Bike   Car PTransit Other Internal Lisbon Car_perc\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n1 110501 35539 11325  1309 21446     1460     0        0      0     60.3\n2 110501 47602  3502   416 37727     5519   437        1      0     79.3\n3 110506 37183 12645    40 22379     2057    63        0      0     60.2\n4 110506 42313  1418   163 37337     3285   106        1      0     88.2\n5 110507 30725  9389  1481 19654      201     0        0      0     64.0\n6 110507 54586  2630   168 44611     6963   215        1      0     81.7\n# ℹ 3 more variables: Origin_num &lt;int&gt;, Lisbon_factor &lt;fct&gt;,\n#   Internal_factor &lt;fct&gt;\n\ntail(Municipalities)\n\n    Mun_code Neighborhood_code        Municipality\n113     1109            110913               Mafra\n114     1114            111409 Vila Franca de Xira\n115     1109            110918               Mafra\n116     1109            110904               Mafra\n117     1502            150202           Alcochete\n118     1109            110911               Mafra\n                                             Neighborhood\n113                                         Santo Isidoro\n114                                   Vila Franca de Xira\n115 União das freguesias de Azueira e Sobral da Abelheira\n116                                            Encarnação\n117                                               Samouco\n118                                             Milharado\n\n\nWe can see that we have a common variable: Origin in TRIPS and Neighborhood_code in Municipalities.\nTo join these two tables we need to specify the common variable in each table, using the by argument.\n\nTRIPSjoin = TRIPS |&gt; left_join(Municipalities, by = c(\"Origin\" = \"Neighborhood_code\"))\n\nIf you prefer, you can mutate or rename a variable so both tables have the same name. When both tables have the same name, you don’t need to specify the by argument.\n\nMunicipalities = Municipalities |&gt; rename(Origin = \"Neighborhood_code\") # change name\nTRIPSjoin = TRIPS |&gt; left_join(Municipalities) # automatic detects common variable\n\nAs you can see, both tables don’t need to be the same length. The left_join function will keep all the observations from the first table, and join the second table to it. If there is no match, the variables from the second table will be filled with NA.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html#group_by-and-summarize",
    "href": "data-manipulation.html#group_by-and-summarize",
    "title": "4  Data manipulation",
    "section": "4.6 group_by and summarize",
    "text": "4.6 group_by and summarize\nWe have a very large table with all the neighbourhoods and their respective municipalities. We want to know the total number of trips with origin in each municipality.\nTo make it easier to understand, let’s keep only the variables we need.\n\nTRIPSredux = TRIPSjoin |&gt; select(Origin, Municipality, Internal, Car, Total)\nhead(TRIPSredux)\n\n# A tibble: 6 × 5\n  Origin Municipality Internal   Car Total\n  &lt;chr&gt;  &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 110501 Cascais             0 21446 35539\n2 110501 Cascais             1 37727 47602\n3 110506 Cascais             0 22379 37183\n4 110506 Cascais             1 37337 42313\n5 110507 Cascais             0 19654 30725\n6 110507 Cascais             1 44611 54586\n\n\nWe can group this table by the Municipality variable and summarize the number of trips with origin in each municipality.\n\nTRIPSsum = TRIPSredux |&gt; \n  group_by(Municipality) |&gt; # you won't notice any chagne with only this\n  summarize(Total = sum(Total))\nhead(TRIPSsum)\n\n# A tibble: 6 × 2\n  Municipality   Total\n  &lt;chr&gt;          &lt;dbl&gt;\n1 Alcochete      36789\n2 Almada        289834\n3 Amadora       344552\n4 Barreiro      133658\n5 Cascais       373579\n6 Lisboa       1365111\n\n\nWe summed the total number of trips in each municipality.\nIf we want to group by more than one variable, we can add more group_by() functions.\n\nTRIPSsum2 = TRIPSredux |&gt; \n  group_by(Municipality, Internal) |&gt; \n  summarize(Total = sum(Total),\n            Car = sum(Car))\nhead(TRIPSsum2)\n\n# A tibble: 6 × 4\n# Groups:   Municipality [3]\n  Municipality Internal  Total    Car\n  &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Alcochete           0  16954   9839\n2 Alcochete           1  19835  15632\n3 Almada              0 105841  49012\n4 Almada              1 183993 125091\n5 Amadora             0 117727  33818\n6 Amadora             1 226825 142386\n\n\nWe summed the total number of trips and car trips in each municipality, separated by inter and intra municipal trips.\n\n\n\n\n\n\nIt is a good practice to use the ungroup() function after the group_by() function. This will remove the grouping. If you don’t do this, the grouping will be kept and you may have unexpected results in the next time you use that dataset.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html#arrange-data",
    "href": "data-manipulation.html#arrange-data",
    "title": "4  Data manipulation",
    "section": "4.7 Arrange data",
    "text": "4.7 Arrange data\nYou can sort a dataset by one or more variables.\nFor instance, arrange() by Total trips, ascending or descending order.\n\nTRIPS2 = TRIPSsum2 |&gt; arrange(Total)\nTRIPS2 = TRIPSsum2 |&gt; arrange(-Total) # descending\n\nTRIPS2 = TRIPSsum2 |&gt; arrange(Municipality) # alphabetic\n\nTRIPS4 = TRIPS |&gt; arrange(Lisbon_factor, Total) # more than one variable\n\nThis is not the same as opening the view table and click on the arrows. When you do that, the order is not saved in the dataset. If you want to save the order, you need to use the arrange() function.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html#all-together-now",
    "href": "data-manipulation.html#all-together-now",
    "title": "4  Data manipulation",
    "section": "4.8 All together now!",
    "text": "4.8 All together now!\nThis is the pipes magic. It takes the last result and applies the next function to it. “With this, do this.”. You can chain as many functions as you want.\n\nTRIPS_pipes = TRIPS |&gt; \n  select(Origin, Internal, Car, Total) |&gt; \n  \n  mutate(Origin_num = as.integer(Origin)) |&gt; \n  mutate(Internal_factor = factor(Internal, labels = c(\"Inter\", \"Intra\"))) |&gt; \n  \n  filter(Internal_factor == \"Inter\")|&gt;\n  \n  left_join(Municipalities) |&gt;\n  \n  group_by(Municipality) |&gt;\n  summarize(Total = sum(Total),\n            Car = sum(Car),\n            Car_perc = Car/Total * 100) |&gt; \n  ungroup() |&gt; \n  \n  arrange(desc(Car_perc))\n\nWith this code we will have a table with the total number of intercity trips, by municipality, with their names instead of codes, arranged by the percentage of car trips.\n\nTRIPS_pipes\n\n# A tibble: 18 × 4\n   Municipality         Total    Car Car_perc\n   &lt;chr&gt;                &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 Mafra                65811  46329     70.4\n 2 Sesimbra             49370  31975     64.8\n 3 Cascais             161194  96523     59.9\n 4 Palmela              66428  39688     59.7\n 5 Alcochete            16954   9839     58.0\n 6 Setúbal             129059  70318     54.5\n 7 Montijo              57164  30900     54.1\n 8 Seixal              120747  63070     52.2\n 9 Sintra              237445 123408     52.0\n10 Oeiras              134862  66972     49.7\n11 Almada              105841  49012     46.3\n12 Loures              132310  60478     45.7\n13 Barreiro             52962  24160     45.6\n14 Odivelas             93709  39151     41.8\n15 Vila Franca de Xira 115152  47201     41.0\n16 Moita                51040  17394     34.1\n17 Amadora             117727  33818     28.7\n18 Lisboa              280079  69038     24.6",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html#other-dplyr-functions",
    "href": "data-manipulation.html#other-dplyr-functions",
    "title": "4  Data manipulation",
    "section": "4.9 Other dplyr functions",
    "text": "4.9 Other dplyr functions\nYou can explore other dplyr functions and variations to manipulate data in the dplyr cheat sheet:\n\nTake a particular attention to pivot_wider and pivot_longer (tidyr package) to transform OD matrices in wide and long formats.\n\n\n\nOD matrix in long format\n\n\nOrigins\nDestinations\nTrips\n\n\n\n\nA\nB\n20\n\n\nA\nC\n45\n\n\nB\nA\n10\n\n\nC\nC\n5\n\n\nC\nA\n30\n\n\n\n\n\n\n\n\nOD matrix in wide format\n\n\nTrips\nA\nB\nC\n\n\n\n\nA\nNA\n20\n45\n\n\nB\n10\nNA\nNA\n\n\nC\n30\nNA\n5",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html#footnotes",
    "href": "data-manipulation.html#footnotes",
    "title": "4  Data manipulation",
    "section": "",
    "text": "This dataset includes the number of trips with origin in each neighborhood, divided by mode of transport, and inter or intra municipal trips.↩︎\nYou can change this in RStudio &gt; Tools &gt; Global Options &gt; Code.↩︎",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "5  Exploratory Data Analysis",
    "section": "",
    "text": "5.1 Do it yourself with R\nCopy the R code above in your session.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#do-it-yourself-with-r",
    "href": "eda.html#do-it-yourself-with-r",
    "title": "5  Exploratory Data Analysis",
    "section": "",
    "text": "# EXPLORATORY DATA ANALYSIS\n\n# The database used in this example is a treated database from the Mobility Survey.   \n# executed for the metropolitan areas of Lisbon and Porto in 2018 (IMOB 2018).\n# We will only focus on trips within the metropolitan area of Lisbon. \n\n# Variables\n\n  #'*Origin_dicofre16* - Code of Freguesias as set by INE after 2016 (Distrito + Concelho + Freguesia)\n  #'*Total* - number of trips with origin in Origin_dicofre16\n  #'*Walk* - number of walking trips with origin in Origin_dicofre16\n  #'*Bike* - number of bike trips with origin in Origin_dicofre16\n  #'*Car* - number of car trips with origin in Origin_dicofre16. Includes taxi and motorcycle.\n  #'*PTransit* - number of Public Transit trips with origin in Origin_dicofre16\n  #'*Other* - number of other trips (truck, van, tractor, aviation) with origin in Origin_dicofre16\n  #'*Distance* - average trip distance (km) with origin in Origin_dicofre16\n  #'*Duration* - average trip duration (minutes) with origin in Origin_dicofre16\n  #'*Car_perc* - percentage of car trips with origin in Origin_dicofre16\n  #'*N_INDIVIDUOS* - number of residents in Origin_dicofre16 (Censos 2021)\n  #'*Male_perc* - percentage of male residents in Origin_dicofre16 (Censos 2021)\n  #'*IncomeHH* - average household income in Origin_dicofre16\n  #'*Nvehicles* - average number of car/motorcycle vehicles in the household in Origin_dicofre16\n  #'*DrivingLic* - percentage of car driving licence holders in Origin_dicofre16\n  #'*CarParkFree_Work* - percentage of respondents with free car parking at the work location, in Origin_dicofre16\n  #'*PTpass* - percentage of public transit monthly pass holders in Origin_dicofre16\n  #'*internal* - binary variable (factor). \"Yes\": internal trips in that freguesia (Origin_dicofre16), \"No\": external trips from that freguesia\n  #'*Lisboa* - binary variable (factor). \"Yes\": the freguesia is part of Lisbon municipality, \"No\": otherwise\n  #'*Area_km2* - area of in Origin_dicofre16, in km2\n\n# Initial Steps\n\n    # Install Libraries\n    #   For the first time, you will need to install some of the packages. \n    #     Step by step: \n \n    #       1. Go to Packages on the lower right display window and click install\n    #       2. Write the library you want to install and click \"install\"\n   \n    #           Or... `install.packages(\"readxl\",\"tidyverse\")` etc...\n \n    #       Depending on the version of your R, `DataExplorer` may need to be installed from source, such as\n\n    #   if (!require(devtools)) install.packages(\"devtools\")\n    #   devtools::install_github(\"boxuancui/DataExplorer\")\n\n\n    # Import Libraries\n\n      library(tidyverse) # Pack of most used libraries for data science\n      library(skimr) # Library used for providing a summary of the data\n      library(DataExplorer) # Library used in data science to perform exploratory data analysis\n      library(corrplot) # Library used for correlation plots\n\n\n# Import dataset\n \n      dataset &lt;- readRDS(\"data/IMOBmodel.Rds\")\n\n# Transform the dataset into a dataframe\n      df &lt;- data.frame(dataset)\n      \n# Take a look at the dataset\n      View(df)\n \n    # Check the summary statistics\n      summary(df)\n      \n    # Check the structure of the dataset \n      str(df)\n\n    # Take a first look at the dataset\n      head(df, 10)\n\n    # Check the type and class of the dataset\n      typeof(df)\n      class(df)\n\n\n#' *Note:* Most libraries work with dataframes. It is good practice to always transform the dataset to dataframe format.       \n\n# Compare the structure of the `dataset` with `df`\n        \n# Show summary statistics of the dataframe\n \n      skim(df)\n    \n# Identify missing data\n \n    # Is there missing data? How many?\n      table(is.na(df))\n\n    # Plot the percentage of missing data\n      plot_missing(df)\n      \n# Detect outliers  \n \n    # In order to detect outliers and do correlations (further in the exercise),it is necessary to have only continuous variables.\n    \n    # a) Create a new database only with continuous variables. \n\n      str(df)\n      names(df)\n      \n      df_continuous = df[,-c(1,18,19)]\n   \n      boxplot(df_continuous, las = 2)\n      \n      boxplot(df_continuous$Total, las = 1) #positively skewed\n\n    # b) Take out the outliers from the variable Total\n\n    # Create function \"outlier\"\n      outlier &lt;- function(x){\n        quant &lt;- quantile(x, probs=c(0.25, 0.75))  # calculates the Q1 and Q3\n        caps &lt;- quantile(x, probs=c(0.05, 0.95)) # calculates the 5th and 95th percentile\n        H &lt;- 1.5* IQR(x, na.rm = TRUE) # calculates interquartile range\n        x[x &lt; (quant[1] - H)] &lt;- caps[1]  # replace any values in x that are LESS than Q1 - 1.5 * IQR with the 5th percentile value\n        x[x &gt; (quant[2] + H)] &lt;- caps[2]  # # replace any values in x that are MORE than Q1 + 1.5 * IQR with the 95th percentile value\n        return(x)\n        }\n\n    # Assign the same database for df_outliers\n  \n      df_outliers = df_continuous\n  \n    # Replace the outliers in the variable Total\n  \n      df_outliers$Total = outlier(df_continuous$Total)\n\n    # c) Take a look again at the boxplots\n \n      boxplot(df_outliers, las = 2)\n\n        # Compare results of the dataset with and without the outliers  \n  \n          # Calculate the mean\n  \n            mean(df$Total)\n            mean(df_outliers$Total)\n\n          # Calculate the median\n            median(df$Total)\n            median(df_outliers$Total)\n\n          # standard deviation\n            var(df$Total)\n            var(df_outliers$Total)\n\n#' *Note:* There are many methods to treat outliers. This is just one of them.\n  # In the next lecture we will demonstrate other methods of detecting outliers such as the Cook distance and QQ plot.    \n\n# Histograms\n    \n      # a) Plot histograms of all the continuous variables\n      plot_histogram(df, ncol = 3) #with 3 columns\n      \n    # b) Check how other variables \n    #' How do the other variables behave regarding *Car_perc*\n    #' Plot boxplots of each independent variable with *Car_perc*\n\n      plot_boxplot(df, by = \"Car_perc\", ncol = 3)\n\n#' *Note*: If you increase the \"Car_perc\", it will decrease PTpass. Take a look at the relation with the other variables. \n#' \n\n# Correlations\n    \n    # Plot correlation heatmaps\n  \n#' *Note:* Correlations are only between continuous variables. \n  \n      res &lt;- cor.mtest(df_continuous, conf.level = .95) #store the results so you can call the p-value at the corrplot\n\n      corrplot(cor(df_continuous), p.mat = res$p, method = \"circle\", type = \"upper\", order=\"hclust\", sig.level = 0.05)\n\n#' *Note:* The pairwise correlations that are crossed are statistically insignificant. \n#  The null hypothesis is that correlation is zero. \n#  This means that the correlations are only significant when you reject the null hypothesis (pvalue &lt; 0.05).\n   \n#' See `?corrplot` for more options.  \n#  Try putting into method \"color\" or \"circle\", and see the difference.  \n\n    # Check the _pvalue_ of a crossed pair correlation: \n \n      cor.test(df_continuous$IncomeHH, df_continuous$Other)\n\n      cor.test(df_continuous$IncomeHH, df_continuous$Duration)\n\n# The default for `cor.test` is Pearson, two-sided, with a 95% confident level. Check `?cor.test` for more options.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "linear-regression.html",
    "href": "linear-regression.html",
    "title": "6  Multiple Linear Regression",
    "section": "",
    "text": "6.1 Do it yourself with R\nCopy the R code above in your session.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#do-it-yourself-with-r",
    "href": "linear-regression.html#do-it-yourself-with-r",
    "title": "6  Multiple Linear Regression",
    "section": "",
    "text": "# Example exercise: Trip production of the Metropolitan Area of Lisbon.\n\n#' **Your task**: Estimate a linear regression model that predicts the car percentage per county.  \n \n# Variables:\n#' \n#'*Origin_dicofre16* - Code of Freguesias as set by INE after 2016 (Distrito + Concelho + Freguesia)\n#'*Total* - number of trips with origin in Origin_dicofre16\n#'*Walk* - number of walking trips with origin in Origin_dicofre16\n#'*Bike* - number of bike trips with origin in Origin_dicofre16\n#'*Car* - number of car trips with origin in Origin_dicofre16. Includes taxi and motorcycle.\n#'*PTransit* - number of Public Transit trips with origin in Origin_dicofre16\n#'*Other* - number of other trips (truck, van, tractor, aviation) with origin in Origin_dicofre16\n#'*Distance* - average trip distance (km) with origin in Origin_dicofre16\n#'*Duration* - average trip duration (minutes) with origin in Origin_dicofre16\n#'*Car_perc* - percentage of car trips with origin in Origin_dicofre16\n#'*N_INDIVIDUOS* - number of residents in Origin_dicofre16 (Censos 2021)\n#'*Male_perc* - percentage of male residents in Origin_dicofre16 (Censos 2021)\n#'*IncomeHH* - average household income in Origin_dicofre16\n#'*Nvehicles* - average number of car/motorcycle vehicles in the household in Origin_dicofre16\n#'*DrivingLic* - percentage of car driving licence holders in Origin_dicofre16\n#'*CarParkFree_Work* - percentage of respondents with free car parking at the work location, in Origin_dicofre16\n#'*PTpass* - percentage of public transit monthly pass holders in Origin_dicofre16\n#'*internal* - binary variable (factor). \"Yes\": internal trips in that freguesia (Origin_dicofre16), \"No\": external trips from that freguesia\n#'*Lisboa* - binary variable (factor). \"Yes\": the freguesia is part of Lisbon municipality, \"No\": otherwise\n#'*Area_km2* - area of in Origin_dicofre16, in km2\n \n  # Let's begin!\n \n    # Import Libraries\n      library(tidyverse) # Pack of most used libraries\n      library(skimr) # Library used for providing a summary of the data\n      library(DataExplorer) # Library used in data science to perform exploratory data analysis\n      library(corrplot) # Library used for correlation plots\n      library(car) # Library used for testing autocorrelation (Durbin Watson)\n      library(olsrr) # Library used for testing multicollinearity (VIF, TOL, etc.)\n      library(corrplot) # For correlation plots\n\n    # Import dataset and transform into dataframe\n        dataset = readRDS(\"data/IMOBmodel.Rds\")\n        \n        # Check the class\n          class(dataset)\n\n \n        # Transform the dataset into a dataframe\n          df = data.frame(dataset)\n\n#'*Assumption 1:* Dependent variable is continuous. \n    \n    # Show summary statistics\n      skim(df)\n      summary(df)\n\n    # Show boxplot\n      boxplot(df$Distance)\n      \n      summary(df$Distance)\n \n# Multiple Linear Regression\n#' Equation with `Car_perc` as the dependent variable:  \n\n# Checking assumptions\n  # Before running the model, you need to check if the assumptions are met.\n \n    # Linear relation\n      \n#'*Assumption 2:* There is a linear relationship between dependent variable (DV) and independent variables (IV)\n \n    par(mfrow=c(2,3)) #set plot area as 2 rows and 3 columns\n    \n    plot(x = df$Car_perc, y = df$Total, xlab = \"Car_perc (%)\", ylab = \"Total (number of trips)\")  \n    plot(x = df$Car_perc, y = df$Walk, xlab = \"Car_perc\", ylab = \"Walk\")  \n    plot(x = df$Car_perc, y = df$Bike, xlab = \"Car_perc\", ylab = \"Bike\")  \n    plot(x = df$Car_perc, y = df$Car, xlab = \"Car_perc\", ylab = \"Car\")  \n    plot(x = df$Car_perc, y = df$PTransit, xlab = \"Car_perc\", ylab = \"PTransit\")\n    plot(x = df$Car_perc, y = df$Other, xlab = \"Car_perc\", ylab = \"Other\")\n    plot(x = df$Car_perc, y = df$Distance, xlab = \"Car_perc\", ylab = \"Distance\")\n    plot(x = df$Car_perc, y = df$Duration, xlab = \"Car_perc\", ylab = \"Duration\")\n    plot(x = df$Car_perc, y = df$N_INDIVIDUOS, xlab = \"Car_perc\", ylab = \"N_INDIVIDUOS\")\n    plot(x = df$Car_perc, y = df$Male_perc, xlab = \"Car_perc\", ylab = \"Male_perc\")\n    plot(x = df$Car_perc, y = df$IncomeHH, xlab = \"Car_perc\", ylab = \"IncomeHH\")\n    plot(x = df$Car_perc, y = df$Nvehicles, xlab = \"Car_perc\", ylab = \"Nvehicles\")\n    plot(x = df$Car_perc, y = df$DrivingLic, xlab = \"Car_perc\", ylab = \"Driving License\")\n    plot(x = df$Car_perc, y = df$CarParkFree_Work, xlab = \"Car_perc\", ylab = \"Free car parking at work\")\n    plot(x = df$Car_perc, y = df$PTpass, xlab = \"Car_perc\", ylab = \"PTpass\")\n    plot(x = df$Car_perc, y = df$internal, xlab = \"Car_perc\", ylab = \"internal trips\")\n    plot(x = df$Car_perc, y = df$Lisboa, xlab = \"Car_perc\", ylab = \"Lisboa\")\n    plot(x = df$Car_perc, y = df$Area_km2, xlab = \"Car_perc\", ylab = \"Area_km2\")\n\n\n    #' Or you could execute a pairwise scatterplot matrix, that compares every variable with each other: \n   \n    pairs(df[,c(2:17,20)], pch = 19, lower.panel = NULL) #cannot put categorical and character variables in this function\n\n    #This function is not visible with many variables. \n    #Try reducing the size. \n\n    pairs(df[,c(2:10)], pch = 19, lower.panel = NULL)\n\n#'*Assumption 3:* The Dependent Variable should be normally distributed.  \n\n  #' Check the histogram of `Car_perc`\n\n    par(mfrow=c(1,1))\n    hist(df$Duration)\n  \n  # If the sample is smaller than 50 observations, use Shapiro-Wilk test: \n\n    shapiro.test(df$Car_perc)\n\n  # If not, use the Kolmogorov-Smirnov test\n \n    ks.test(df$Car_perc, \"pnorm\", mean=mean(df$Car_perc), sd = sd(df$Car_perc))\n\n#' The null hypothesis of both tests is that the distribution is normal. \n#' Therefore, for the distribution to be normal, the pvalue &gt; 0.05 and you should not reject the null hypothesis.\n\n#'* Multiple linear regression model*\n    \n#' Check the correlation plot before choosing the variables. \n\n\n  model &lt;- lm(Car_perc ~ Total +\n               Walk +\n               Bike +\n               Car +\n               PTransit +\n               Other +\n               Distance +\n               Duration +\n               N_INDIVIDUOS +\n               Male_perc +\n               IncomeHH +\n               Nvehicles +\n               DrivingLic +\n               CarParkFree_Work +\n               PTpass +\n               internal +\n               Lisboa +\n               Area_km2,\n              data = df)\nsummary(model)\n\n#'*Tip:* Use the function `names(df)` in the console to obtain the names of the variables.  \n#'*Tip:* Use ctrl+shift+c to comment a variable\n \n#' **Assessing the model**:\n \n#' 1. First check the **pvalue** and the **F statistics** of the model to see if there is any statistical relation \n#' between the dependent variable and the independent variables. \n#' If pvalue &lt; 0.05 and the F statistics &gt; Fcritical = 2,39, then the model is statistically acceptable.  \n\n#' 2. The **R-square** and **Adjusted R-square** evaluate the amount of variance that is explained by the model. \n#' The difference between one and another is that the R-square does not consider the number of variables.\n#' If you increase the number of variables in the model, the R-square will tend to increase which can lead to overfitting. \n#' On the other hand, the Adjusted R-square adjust to the number of independent variables. \n \n#' 3. Take a look at the **t-value** and the Pr(&gt;|t|). \n#' If the t-value &gt; 1,96 or Pr(&gt;|t|) &lt; 0,05, then the IV is statistically significant to the model.\n   \n#' 4. To analyze the **estimates** of the variables, you should first check the **signal** \n#' and evaluate if the independent variable has a direct or inverse relationship with the dependent variable. \n#' It is only possible to evaluate the **magnitude** of the estimate if all variables are continuous and standardized \n#' or by calculating the elasticities. The elasticities are explained and demonstrated in chapter 4. \n\n#' Residuals\n#' Check the following assumptions\n\n#' *Assumption 4:* The error (E) is independent across observations and the error variance is constant across IV – Homoscedasticity\n#' *Assumption 5:* Disturbances are approximately normally distributed\n\n#' * **Residuals vs Fitted:** This plot is used to detect non-linearity, heteroscedasticity, and outliers. \n\n#' **Normal Q-Q:** The quantile-quantile (Q-Q) plot is used to check if the disturbances follow a normal distribution.\n\n#' * **Scale-Location:** This plot is used to verify if the residuals are spread equally (homoscedasticity) or not \n#' (heteroscedasticity) through the sample. \n#' * **Residuals vs Leverage:** This plot is used to detect the impact of the outliers in the model. \n#' If the outliers are outside the Cook-distance, this may lead to serious problems in the model. \n \n#' Try analyzing the plots and check if the model meets the assumptions. \n    par(mfrow=c(2,2))\n    plot(model)\n\n \n#' *Assumption 6:* Non-autocorrelation of disturbances\n#' Execute the Durbin-Watson test to evaluate autocorrelation of the residuals\n    durbinWatsonTest(model)\n\n#' &gt; **Note:** In the Durbin-Watson test, values of the D-W Statistic vary from 0 to 4. \n#' If the values are from 1.8 to 2.2 this means that there is no autocorrelation in the model.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "factor-analysis.html",
    "href": "factor-analysis.html",
    "title": "7  Factor Analysis",
    "section": "",
    "text": "7.1 Do it yourself with R\nCopy the R code above in your session.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Factor Analysis</span>"
    ]
  },
  {
    "objectID": "factor-analysis.html#do-it-yourself-with-r",
    "href": "factor-analysis.html#do-it-yourself-with-r",
    "title": "7  Factor Analysis",
    "section": "",
    "text": "#' #### Example exercise: \"Residential location satisfaction in the Lisbon metropolitan area\"\n#' \n#' The aim of this study was to examine the perception of households towards their residential location considering several land use and accessibility factors as well as household socioeconomic and attitudinal characteristics.\n#' \n#' _Reference:_ Martinez, L. G., de Abreu e Silva, J., & Viegas, J. M. (2010). Assessment of residential location satisfaction in the Lisbon metropolitan area, TRB (No. 10-1161).\n#' \n#' **Your task:** Analyse the data and create meaningful latent factors.  \n#' \n#' ## Data\n#' #### Variables: \n#' \n#' * `DWELCLAS`: Classification of the dwelling;\n#' * `INCOME`: Income of the household;\n#' * `CHILD13`: Number of children under 13 years old;\n#' * `H18`: Number of household members above 18 years old;\n#' * `HEMPLOY`: Number of household members employed;\n#' * `HSIZE`: Household size;\n#' * `IAGE`: Age of the respondent;\n#' * `ISEX`: Sex of the respondent;\n#' * `NCARS`: Number of cars in the household;\n#' * `AREA`: Area of the dwelling;\n#' * `BEDROOM`: Number of bedrooms in the dwelling;\n#' * `PARK`: Number of parking spaces in the dwelling; \n#' * `BEDSIZE`: BEDROOM/HSIZE;\n#' * `PARKSIZE`: PARK/NCARS;\n#' * `RAGE10`: 1 if Dwelling age &lt;= 10;\n#' * `TCBD`: Private car distance in time to CBD;\n#' * `DISTTC`: Euclidean distance to heavy public transport system stops;\n#' * `TWCBD`: Private car distance in time of workplace to CBD;\n#' * `TDWWK`: Private car distance in time of dwelling to work place;\n#' * `HEADH`: 1 if Head of the Household;\n#' * `POPDENS`: Population density per hectare;\n#' * `EQUINDEX`: Number of undergraduate students/Population over 20 years old (500m)\n#' \n#' \n#' #### Rules of thumb: \n#' * At least 10 variables\n#' * n &lt; 50 (Unacceptable); n &gt; 200 (recommended)\n#' * It is recommended to use continuous variables. If your data contains categorical variables, you should transform them to dummy variables. \n#' \n#' #### Assumptions: \n#' * Normality; \n#' * linearity; \n#' * Homogeneity;\n#' * Homoscedasticity (some multicollinearity is desirable);\n#' * Correlations between variables &lt; 0.3 (not appropriate to use Factor Analysis)\n#' \n#' ## Let's start!\n#' #### Import Libraries\nlibrary(foreign) # Library used to read SPSS files\nlibrary(nFactors) # Library used for factor analysis\nlibrary(tidyverse) # Library used in data science to perform exploratory data analysis\nlibrary(summarytools) # Library used for checking the summary of the dataset\nlibrary(psych) # Library used for factor analysis\nlibrary(GPArotation) # Library used for factor analysis\n\n#' \n#' ### Get to know your dataset\n#' ##### Import dataset\n#' \ndf &lt;- read.spss(\"Data/example_fact.sav\", to.data.frame = T) #transforms a list into a data.frame directly\n\n#' \n#' ##### Take a look at the main characteristics of the dataset\n#' \nclass(df) #type of data\nstr(df)\n\n#' \n#' ##### Check summary statistics of variables\ndescriptive_stats &lt;- dfSummary(df)\nview(descriptive_stats)\n\n#' &gt; **Note:** I used a different library of the MLR chapter for perfoming the summary statistics. \"R\" allows you to do the same or similar tasks with different packages. \n#' \n#' ##### Take a look at the first values of the dataset\n#' \nhead(df,5)\n\n#' \n#' ##### Make ID as row names or case number\n#' \ndf&lt;-data.frame(df, row.names = 1)\n\n#' \n#' ### Evaluating the assumptions for factoral analysis\n#' \n#' Let's run a random regression model in order to evaluate some assumptions\n#' \nrandom = rchisq(nrow(df), 32)\nfake = lm(random ~ ., data = df)\nstandardized = rstudent(fake)\nfitted = scale(fake$fitted.values)\n\n#' \n#' * **Normality**\n#' \nhist(standardized)\n\n#' \n#' * **Linearity**\n#' \nqqnorm(standardized)\nabline(0,1)\n\n#' \n#' * **Homogeneity** \n#' \nplot(fitted, standardized)\nabline(h=0,v=0)\n\n#' \n#' * **Correlations between variables** \n#' Correlation matrix\ncorr_matrix &lt;- cor(df, method = \"pearson\")\n\n#' \n#' The **Bartlett** test examines if there is equal variance (homogeneity) between variables. Thus, it evaluates if there is any pattern between variables. Check for correlation adequacy - Bartlett's Test.\ncortest.bartlett(corr_matrix, n = nrow(df))\n\n#' &gt; **Note:** The null hypothesis is that there is no correlation between variables. Therefore, in factor analysis you want to reject the null hypothesis.   \n#' \n#' ##### Check for sampling adequacy - KMO test\n#' \nKMO(corr_matrix)\n\n#' \n#' &gt; **Note:** We want at least 0.7 of the overall Mean Sample Adequacy (MSA). If, 0.6 &lt; MSA &lt; 0.7, it is not a good value, but acceptable in some cases.\n#' \n#' ##### Determine the number of factors to extract\n#' \n#' **1. Parallel Analysis**\n#' \nnum_factors = fa.parallel(df, fm = \"ml\", fa = \"fa\")\n\n#' \n#' &gt; **Note:** `fm` = factor math; `ml` = maximum likelihood; `fa` = factor analysis\n#' \n#' The selection of the number of factors in the Parallel analysis can be threefold:  \n#' \n#' * Detect where there is an \"elbow\" in the graph;\n#' * Detect the intersection between the \"FA Actual Data\" and the \"FA Simulated Data\";\n#' * Consider the number of factors with eigenvalue &gt; 1.\n#' \n#' \n#' **2. Kaiser Criterion**\n#' \nsum(num_factors$fa.values &gt; 1) #Determines the number of factors with eigenvalue &gt; 1\n\n#' \n#' You can also consider factors with eigenvalue &gt; 0.7, since some of the literature indicate that this value does not overestimate the number of factors as much as considering an eigenvalue = 1. \n#' \n#' **3. Principal Component Analysis (PCA)**\n#' \n#' * Print variance that explains the components\ndf_pca &lt;- princomp(df, cor=TRUE) #cor = TRUE, standardizes your dataset before running a PCA\nsummary(df_pca)  \n\n#' \n#' * Scree Plot\nplot(df_pca,type=\"lines\", npcs = 31) \n\n#' \n#' &gt; **Note:** Check the cummulative variance of the first components and the scree plot, and see if the PCA is a good approach to detect the number of factors in this case. \n#' \n#' **PCA is not the same thing as Factor Analysis!** PCA only considers the common information (variance) of the variables, while factor analysis takes into account also the unique variance of the variable. Both approaches are often mixed up. In this example we use PCA as only a first criteria for choosing the number of factors. PCA is very used in image recognition and data reduction of big data. \n#' \n#' ## Exploratory Factor Analysis\n#' \n#' * **Model 1**: No rotation\n#' * **Model 2**: Rotation Varimax\n#' * **Model 3**: Rotation Oblimin\n\n# No rotation\ndf_factor &lt;- factanal(df, factors = 4, rotation = \"none\", scores=c(\"regression\"), fm = \"ml\")\n# Rotation Varimax\ndf_factor_var &lt;- factanal(df, factors = 4, rotation = \"varimax\", scores=c(\"regression\"), fm = \"ml\")\n# Rotiation Oblimin\ndf_factor_obl &lt;- factanal(df, factors = 4, rotation = \"oblimin\", scores=c(\"regression\"), fm = \"ml\")\n\n#' \n#' Let's print out the results of `df_factor_obl`, and have a look. \nprint(df_factor, digits=2, cutoff=0.3, sort=TRUE) #cutoff of 0.3 due to the sample size is higher than 350 observations.\n\n#' \n#' &gt; **Note:** \n#' The variability contained in the factors = Communality + Uniqueness.  \n#' Varimax assigns orthogonal rotation, and oblimin assigns oblique rotation.\n#' \n#' \n#' Plot factor 1 against factor 2, and compare the results of different rotations\n#' \n#' * **No Rotation**\nplot(\n  df_factor$loadings[, 1],\n  df_factor$loadings[, 2],\n  xlab = \"Factor 1\",\n  ylab = \"Factor 2\",\n  ylim = c(-1, 1),\n  xlim = c(-1, 1),\n  main = \"No rotation\"\n)\nabline(h = 0, v = 0)\nload &lt;- df_factor$loadings[, 1:2]\ntext(\n  load,\n  names(df),\n  cex = .7,\n  col = \"blue\"\n)\n\n#' \n#' * **Varimax rotation**\nplot(\n  df_factor_var$loadings[, 1],\n  df_factor_var$loadings[, 2],\n  xlab = \"Factor 1\",\n  ylab = \"Factor 2\",\n  ylim = c(-1, 1),\n  xlim = c(-1, 1),\n  main = \"Varimax rotation\"\n)\nabline(h = 0, v = 0)\nload &lt;- df_factor_var$loadings[, 1:2]\ntext(\n  load,\n  labels = names(df),\n  cex = .7,\n  col = \"red\"\n)\n\n#' \n#' \n#' * **Oblimin Rotation**\nplot(\n  df_factor_obl$loadings[, 1],\n  df_factor_obl$loadings[, 2],\n  xlab = \"Factor 1\",\n  ylab = \"Factor 2\",\n  ylim = c(-1, 1),\n  xlim = c(-1, 1),\n  main = \"Oblimin rotation\"\n)\nabline(h = 0, v = 0)\nload &lt;- df_factor_obl$loadings[, 1:2]\ntext(\n  load,\n  labels = names(df),\n  cex = .7,\n  col = \"darkgreen\"\n)\n\n#' \n#' When you have more than two factors it is difficult to analyse the factors by the plots. Variables that have low explaining variance in the two factors analyzed, could be highly explained by the other factors not present in the graph. However, try comparing the plots with the factor loadings and plot the other graphs to get more familiar with exploratory factor analysis.  \n#'",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Factor Analysis</span>"
    ]
  },
  {
    "objectID": "clusters.html",
    "href": "clusters.html",
    "title": "8  Cluster Analysis",
    "section": "",
    "text": "8.1 Do it yourself with R\nCopy the R code above in your session.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "clusters.html#do-it-yourself-with-r",
    "href": "clusters.html#do-it-yourself-with-r",
    "title": "8  Cluster Analysis",
    "section": "",
    "text": "#' \n#' #### Example exercise: Airports\n#' \n#' **Your task**: Create and evaluate the many types of clustering methods. \n#' \n#' #### Variables: \n#' \n#' * `Code`: Code of the airport;  \n#' * `Airport`: Name of the airport;  \n#' * `Ordem`: ID of the observations;  \n#' * `Passengers`: Number of passengers;  \n#' * `Movements`: Number of flights;  \n#' * `Numberofairlines`: Number of airlines in each airport;  \n#' * `Mainairlineflightspercentage`: Percentage of flights of the main airline of each airport;  \n#' * `Maximumpercentageoftrafficpercountry`: Maximum percentage of flights per country;  \n#' * `NumberofLCCflightsweekly`: Number of weekly low cost flights`;   \n#' * `NumberofLowCostAirlines`: Number of low cost airlines of each airport;  \n#' * `LowCostAirlinespercentage`: Percentage of the number of low cost airlines in each airport;  \n#' * `Destinations`: Number of flights arriving at each airport;  \n#' * `Average_route_Distance`: Average route distance in km;  \n#' * `DistancetoclosestAirport`: Distance to closest airport in km\n#' * `DistancetoclosestSimilarAirport`: Distance to closest similar airport in km;  \n#' * `AirportRegionalRelevance`: Relevance of the airport in a regional scale (0 - 1);  \n#' * `Distancetocitykm`: Distance between the airport and the city in km;  \n#' * `Inhabitantscorrected`: Population of the city;  \n#' * `numberofvisitorscorrected`: Number of vistors that arrived in the airport;  \n#' * `GDP corrected`: Corrected value of the Gross Domestic Product;  \n#' * `Cargoton`: Cargo ton. The total number of cargo transported in a certain period multiplied by the number o flights.\n#' \n#' ## Startup\n#' ##### Import Libraries\nlibrary(readxl) # Reading excel files\nlibrary(skimr) # Summary statistics\nlibrary(tidyverse) # Pack of useful tools\nlibrary(mclust) # Model based clustering\nlibrary(cluster) # Cluster analysis\nlibrary(factoextra) # Visualizing distances\n\n#' \n#' ##### Import dataset as a dataframe\ndataset &lt;- read_excel(\"data/Data_Aeroports_Clustersv1.xlsX\")\ndf &lt;- data.frame(dataset)\n\n#' \n#' ## Get to know your data\n#' ##### Summary statistics\nskim(df)\n\n#' ##### Sneak the plot\n#' Now let us plot an example and take a look\nplot(Numberofairlines ~ Destinations, df) #plot\ntext(Numberofairlines ~ Destinations, df, label = Airport, pos = 4, cex = 0.6) #labels over the previous plot\n\n#' \n#' By looking at the plot, you may already have a clue on the number of clusters with this two variabls. However, this is not clear and it does not consider the other variables in the analysis. \n#' \n#' ### Prepare data before performing a cluster analysis\n#' ##### Deal with missing data\ntable(is.na(df))\n\n#' In this example we do not have missing values. In case you do have in the future, you can take out the missing values with _listwise deletion_ (`df &lt;- na.omit(df)`) or use other ways of treating missing values.\n#' \n#' ##### Continuous variables\n#' Leave only continuous variables, and make `Ordem` as the row ID variable\ndf_reduced = df[,!(names(df) %in% c(\"Code\",\"Airport\"))]\ndf_reduced = data.frame(df_reduced, row.names = 1) #Ordem is the 1st variable in the df\n\n#' \n#' Take a look at the scale of the variables. See how they are different!\nhead(df_reduced)\n\n#' \n#' ##### Standardize variables\n#' Z-score standardization:  $(x_{i} - x_{\\text{mean}}) / {\\sigma}$\nmean &lt;- apply(df_reduced, 2, mean) # The \"2\" in the function is used to select the columns. MARGIN: c(1,2)\nsd &lt;- apply(df_reduced, 2, sd)\ndf_scaled &lt;- scale(df_reduced, mean, sd)\n\n#'   \n#' ## Hierarchical Clustering\n#' \n#' ##### Measuring Similarity through Euclidean distances\n#' \ndistance &lt;- dist(df_scaled, method = \"euclidean\")\n\n#' \n#' &gt; **Note:** There are other forms of distance measures that can be used such as:  \n#'   i) Minkowski distance; \n#'   ii) Manhattan distance; \n#'   iii) Mahanalobis distance.\n#'   \n#' ##### Visualize distances in heatmap\n#'\nfviz_dist(distance, gradient = list(low = \"#00AFBB\", mid = \"white\", high = \"#FC4E07\"), order = FALSE)\n\n#' \n#' #### Types of hierarchical clustering  \n#' There are many types of hierarchical clustering. Let's explore some.\n#' \n#' **1. Single linkage (nearest neighbor) clustering algorithm**\n#' \n#' Based on a bottom-up approach, by linking two clusters that have the closest distance between each other. \nmodels &lt;- hclust(distance, \"single\")\nplot(models, labels = df$Airport, xlab = \"Distance - Single linkage\", cex=0.6, hang = -1)\n\nrect.hclust(models, 4, border = \"purple\") # Visualize the cut on the dendogram, with 4 clusters\n\n#' \n#' \n#' **2. Complete linkage (Farthest neighbor) clustering algorithm**\n#' \n#' Complete linkage is based on the maximum distance between observations in each cluster.\nmodelc &lt;- hclust(distance, \"complete\")\nplot(modelc, labels = df$Airport, xlab = \"Distance - Complete linkage\", cex=0.6, hang = -1)\nrect.hclust(modelc, 4, border = \"blue\") \n\n#' \n#' **3. Average linkage between groups** \n#' \n#' The average linkage considers the distance between clusters to be the average of the distances between observations in one cluster to all the members in the other cluster. \nmodela &lt;- hclust(distance, \"average\")\nplot(modela, labels = df$Airport, xlab = \"Distance - Average linkage\", cex=0.6, hang = -1)\nrect.hclust(modelc, 4, border = \"red\")\n\n#' \n#' **4. Ward`s method**\n#'   \n#' The Ward`s method considers the measures of similarity as the sum of squares within the cluster summed over all variables. \nmodelw &lt;- hclust(distance, \"ward.D2\")\nplot(modelw, labels = df$Airport, xlab = \"Distance - Ward method\", cex=0.6, hang = -1)\nrect.hclust(modelw, 4, border = \"orange\")\n\n#' \n#' **5. Centroid method**\n#' \n#' The centroid method considers the similarity between two clusters as the distance between its centroids.\nmodelcen &lt;- hclust(distance, \"centroid\")\nplot(modelcen, labels = df$Airport, xlab = \"Distance - Centroid method\", cex=0.6, hang = -1)\nrect.hclust(modelcen, 4, border = \"darkgreen\")\n\n#' \n#' ##### Comparing results from different hierarchical methods\n#' Now lets evaluate the **membership** of each observation with the `cutree` function for each method.\n#' \nmember_single &lt;- cutree(models, 4)\nmember_com &lt;- cutree(modelc, 4)\nmember_av &lt;- cutree(modela, 4)\nmember_ward &lt;- cutree(modelw, 4)\nmember_cen &lt;- cutree(modelcen, 4)\n\n#'   \n#' \n#' Compare how common each method is to each other.\ntable(member_com, member_av) # compare the complete linkage with the average linkage\n\n#' \n#' &gt; **Note:** Try comparing other methods, and evaluate how common they are.   \n#'   \n#' ##### Silhouette Plots: evaluate which method is more appropriate \n#' The silhouette plot evaluates how similar an observation is to its own cluster compared to other clusters. The clustering configuration is appropriate when most objects have high values. Low or negative values indicate that the clustering method is not appropriate or the number of clusters is not ideal. \n#' \nplot(silhouette(member_single, distance))\nplot(silhouette(member_com, distance))\nplot(silhouette(member_av, distance))\nplot(silhouette(member_ward, distance))\nplot(silhouette(member_cen, distance))\n\n#' \n#' \n#' ## Non-Hirarchical Clustering \n#' \n#' ##### K-means clustering\n#' * k-means with n=3 clusters\nkm_clust &lt;- kmeans(df_scaled, 3)\nkm_clust #print the results\n\n#' \n#' * Other ways of setting the number of clusters  \n#' \n#' This algorithm will detect how many clusters from 1 to 10 explains more variance\n#' \n  k &lt;- list()\n  for(i in 1:10){\n    k[[i]] &lt;- kmeans(df_scaled, i)\n  }\n\n#'   \n#' &gt; **Note**: Try printing the k value and take a look at the ratio `between_SS` / `total_SS`. Evaluate how it varies when you add clusters. \n#' \n#' Now, lets  plot `between_SS` / `total_SS` into a scree plot\n#' \nbetSS_totSS &lt;- list()\nfor(i in 1:10){\nbetSS_totSS[[i]] &lt;- k[[i]]$betweenss/k[[i]]$totss\n}\nplot(1:10, betSS_totSS, type = \"b\", ylab = \"Between SS / Total SS\", xlab = \"Number of clusters\")\n\n#' \n#' \n#' Let take out the outliers and see the difference in the k-means clustering:\n#'  \n#' * **Examine the boxplots**\n#'   \npar(cex.axis=0.6, mar=c(11,2,1,1))# Make labels fit in the boxplot\nboxplot(df_scaled, las = 2) #labels rotated to vertical\n\n#' \n#' * **Detect the outliers** \n#' \noutliers &lt;- boxplot.stats(df_scaled)$out\noutliers\n\n#' \n#' * **Remove rows with outliers** \n#' \nnrow(df_scaled) #32\nout_ind &lt;- which(df_scaled %in% c(outliers)) #the row.names that contain outliers\ndf_no_outliers = df_scaled[-c(out_ind),] #remove those rows from the df_scaled\nnrow(df_no_outliers) #31\n\n#' \n#' &gt; **Note:** There are many methods to treat outliers. This is just one of them. Note that it is not very appropriate, since it removes many observations that are relevant for the analysis. Try using other methods and evaluate the difference.  \n#' \n#' \n#' Execute a k-means clustering with the dataset without the outliers and see the difference. \n#' \nkm_no_outliers &lt;- kmeans(df_no_outliers, 3)\nkm_no_outliers\n\n#' \n#' #### Ploting the clusters\n#' Finally, plotting the clusters results to check if they make sense.  \n#' Let us go back to first example and take a look. \n#' \n#' * **K-means with outliers**\n#' \nplot(Numberofairlines ~ Destinations, df, col = km_clust$cluster)\nwith(df, text(Numberofairlines ~ Destinations, label = Airport, pos = 1, cex = 0.6))\n\n#' \n#' * **K-means without outliers**\n#'\nplot(Numberofairlines ~ Destinations, df, col = km_no_outliers$cluster)\nwith(df, text(Numberofairlines ~ Destinations, label = Airport, pos = 1, cex = 0.6))",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "centroids.html",
    "href": "centroids.html",
    "title": "10  Centroids of transport zones",
    "section": "",
    "text": "10.1 Geometric centroids\nIn this section we will calculate the geometric and the weighted centroids of transport zones.\nTaking the Municipalities_geo data from the previous section, we will calculate the geometric centroids, using the st_centroid() function.\nCode\nlibrary(dplyr)\nlibrary(sf)\nlibrary(mapview)\n\nMunicipalities_geo = st_read(\"../data/Municipalities_geo.gpkg\", quiet = TRUE)\n\nCentroid_geo = st_centroid(Municipalities_geo)\nThis creates points at the geometric center of each polygon.\nCode\nmapview(Centroid_geo)\n\n\n\n\n\n\nCode\nmapview(Centroid_geo) + mapview(Municipalities_geo, alpha.regions = 0) # both maps, with full transparency in polygons\nBut… is this the best way to represent the center of a transport zone?\nThese results may be biased by the shape of the polygons, and not represent where activities are. Example: lakes, forests, etc.\nTo overcome this, we can use weighted centroids.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Centroids of transport zones</span>"
    ]
  },
  {
    "objectID": "centroids.html#weighted-centroids",
    "href": "centroids.html#weighted-centroids",
    "title": "10  Centroids of transport zones",
    "section": "10.2 Weighted centroids",
    "text": "10.2 Weighted centroids\nWe will weight centroids of transport zones by population and by number of buildings.\nFor this, we will need the census data (INE 2022).\n\n\nCode\nCensus = st_read(\"../data/census.gpkg\", quiet = TRUE)\n\nmapview(Census |&gt; filter(Municipality == \"Lisboa\"), zcol = \"Population\")\n\n\n\n\n\n\nIt was not that easy to estimate weighted centroids with R, as it is with GIS software. But there is this new package centr that can help us (Zomorrodi 2024).\nWe need to specify the group we want to calculate the mean centroids, and the weight variable we want to use.\n\n\nCode\n# test\nlibrary(centr)\nCentroid_pop = Census |&gt; \n  mean_center(group = \"Municipality\", weight = \"Population\")\n\n\nWe can do the same for buildings.\n\n\nCode\nCentroid_build = Census |&gt; \n  mean_center(group = \"Municipality\", weight = \"Buildings\")",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Centroids of transport zones</span>"
    ]
  },
  {
    "objectID": "centroids.html#compare-centroids-in-a-map",
    "href": "centroids.html#compare-centroids-in-a-map",
    "title": "10  Centroids of transport zones",
    "section": "10.3 Compare centroids in a map",
    "text": "10.3 Compare centroids in a map\n\n10.3.1 Interactive map\n\n\nCode\nmapview(Centroid_geo, col.region = \"blue\") +\n  mapview(Centroid_pop, col.region = \"red\") +\n  mapview(Centroid_build, col.region = \"black\") +\n  mapview(Municipalities_geo, alpha.regions = 0) # polygon limits\n\n\n\n\n\n\nSee how the building, population and geometric centroids of Sintra are apart, from closer to Tagus, to the rural area.\n\n\n10.3.2 Static map\nTo produce the same map, using only plot() and st_geometry(), we need to make sure that the geometries have the same crs.\n\n\nCode\nst_crs(Centroid_geo) # 4326 WGS84\nst_crs(Centroid_pop) # 3763 Portugal TM06\n\n\nSo, we need to transform the geometries to the same crs.\n\n\nCode\nCentroid_pop = st_transform(Centroid_pop, crs = 4326)\nCentroid_build = st_transform(Centroid_build, crs = 4326)\n\n\nNow, to use plot() we incrementally add layers to the plot.\n\n\nCode\n# Plot the Municipalities_geo polygons first (with no fill)\nplot(st_geometry(Municipalities_geo), col = NA, border = \"black\")\n\n# Add the Centroids_geo points in blue\nplot(st_geometry(Centroid_geo), col = \"blue\", pch = 16, add = TRUE) # add!\n\n# Add the Centroid_pop points in red\nplot(st_geometry(Centroid_pop), col = \"red\", pch = 16, add = TRUE)\n\n# Add the Centroid_build points in black\nplot(st_geometry(Centroid_build), col = \"black\", pch = 16, add = TRUE)\n\n\n\n\n\nStatic map of different centroids of Municipalities\n\n\n\n\nIn the next section we will use these centroids to calculate the desire lines between them.\n\n\n\n\n\n\nINE. 2022. “Censos 2021- XVI Recenseamento Geral da População. VI Recenseamento Geral da Habitação.” Lisboa: Instituto National de Estatística. https://censos.ine.pt/xurl/pub/65586079.\n\n\nZomorrodi, Ryan. 2024. Centr: Weighted and Unweighted Spatial Centers. https://ryanzomorrodi.github.io/centr/.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Centroids of transport zones</span>"
    ]
  }
]