---
title: "Cluster Analysis"
code-fold: false
number-depth: 3
bibliography: references.bib
---

::: {.callout-tip appearance="simple"}
## Do it yourself with R

Copy the script [ClusterAnalysis.R](https://github.com/U-Shift/MQAT/blob/main/code/classroom/ClusterAnalysis.R) and paste it in your session.\
Run each line using `CTRL` + `ENTER`
:::

Based on a dataset of European **Airports**, we will create **clusters** based on the observations.

::: callout-important
## Your task

Create and assess the many types of clustering methods.
:::

## Load packages

```{r}
#| message: false
library(tidyverse) # Pack of most used libraries for data science
library(readxl) # Import excel files
library(skimr) # Summary statistics
library(mclust) # Model based clustering
library(cluster) # Cluster analysis
library(factoextra) # Visualizing distances
```

## Dataset

Included **variables**:

-   `Code` - Code of the airport
-   `Airport` - Name of the airport
-   `Ordem` - ID of the observations
-   `Passengers` - Number of passengers
-   `Movements` - Number of flights
-   `Numberofairlines` - Number of airlines at each airport
-   `Mainairlineflightspercentage` - Percentage of flights of the main airline of each airport
-   `Maximumpercentageoftrafficpercountry` - Maximum percentage of flights per country
-   `NumberofLCCflightsweekly` - Number of weekly low cost flights
-   `NumberofLowCostAirlines` - Number of low cost airlines of each airport
-   `LowCostAirlinespercentage` - Percentage of the number of low cost airlines in each airport
-   `Destinations` - Number of flights arriving at each airport
-   `Average_route_Distance` - Average route distance in km
-   `DistancetoclosestAirport` - Distance to closest airport in km
-   `DistancetoclosestSimilarAirport` - Distance to closest similar airport in km
-   `AirportRegionalRelevance` - Relevance of the airport in a regional scale (0 - 1)
-   `Distancetocitykm` - Distance between the airport and the city in km
-   `Inhabitantscorrected` - Population of the city
-   `numberofvisitorscorrected` - Number of visitors arrived in the airport
-   `GDPcorrected` - Corrected value of the Gross Domestic Product
-   `Cargoton` - The total number of cargo \[ton\] transported in a certain period multiplied by the number of flights.

### Import dataset

```{r}
data = read_excel("../data/Data_Aeroports_Clustersv1.xlsx")
data = data.frame(data) # as data frame only
```

### Get to know your dataset

Take a look at the first values of the dataset

```{r}
head(data, 5)
```

Check summary statistics of variables

```{r}
skim(data)
```

As exploring the data, we can plot the `Numberofairlines` against the `Destinations` and observe.

```{r}
plot(Numberofairlines ~ Destinations, data)
text(Numberofairlines ~ Destinations, data, # over the previous plot
  label = Airport, # label
  pos = 4, # position
  cex = 0.6 # size
)
```

By looking at the plot, you may already have a clue on the number of clusters with this two variables.
However, this is not clear and it does not consider the **other variables** in the analysis.

### Prepare data

#### Row-names

Make the `Code` variable as **row names** or case number

```{r}
data = data |> column_to_rownames(var = "Code")
```

#### Remove the non-continuous data

Leave only continuous variables and the ordered ID.

```{r}
data_continuous = data |> select(-Ordem, -Airport) # remove chr and id variables
```

#### Standardize variables

Take a look at the scale of the variables.
See how different they are!

```{r}
head(data_continuous)
```

Z-score standardization: $(x_{i} - x_{\text{mean}}) / {\sigma}$

```{r}
data_scaled = data_continuous |> 
  mutate(across(everything(), ~ ( . - mean(.) ) / sd(.)))
# Result = z-scores, same as scale()
```

## Hierarchical Clustering

### Distance measures

**Similarity** of observations can be measured through different **distance measures**, including:

-   Euclidean distance
-   Minkowski distance
-   Manhattan distance
-   Mahanalobis distance

Let's measure the **euclidean distances** of our standartize data and visualize them on a heatmap

```{r}
#| warning: false
# measure
distance = dist(data_scaled, method = "euclidean")

# heatmap
fviz_dist(
  distance, 
  gradient = list(
    low = "#00AFBB",
    mid = "white",
    high = "#FC4E07"
  ),
  order = FALSE
)
```

By the color codes, you can have a clue of the airports that are more similar.

### Types of hierarchical clustering

There are many types of hierarchical clustering.
We will explore some of them:

-   Single linkage (nearest neighbour) clustering algorithm
-   Complete linkage (Farthest neighbour) clustering algorithm
-   Average linkage between groups
-   Ward\`s method
-   Centroid method

#### Single linkage

This clustering algorithm is based on a bottom-up approach, by linking two clusters that have the **closest distance** between each other.

```{r}
cluster_single = hclust(distance, "single")

# dendogram
plot(
  cluster_single,
  hang = -1, # all to the bottom
  cex = 0.6 # label text size
)
rect.hclust(cluster_single, k = 4, border = "purple") # cut on the dendogram at 4 clusters
```

This results in 4 clusters, with *Heathrow Airport* and *Kaunas Airport* at their own cluster.

#### Complete linkage

Complete linkage is based on the **maximizing distance** between observations in each cluster.

```{r}
cluster_complete = hclust(distance, "complete")

# dendogram
plot(
  cluster_complete,
  hang = -1, # all to the bottom
  cex = 0.6 # label text size
)
rect.hclust(cluster_complete, k = 4, border = "blue") # cut on the dendogram at 4 clusters
```

#### Average linkage

The average linkage considers the distance between clusters to be the average of the distances between observations in one cluster to all the members in the other cluster.

```{r}
cluster_average = hclust(distance, "average")
plot(cluster_average, labels = data$Airport, xlab = "Distance - Average linkage", cex=0.6, hang = -1)
rect.hclust(cluster_average, 4, border = "red")
```

#### Ward\`s method

The Ward\`s method considers the measures of similarity as the sum of squares within the cluster summed over all variables.

```{r}
cluster_ward = hclust(distance, "ward.D2")
plot(cluster_ward, labels = data$Airport, xlab = "Distance - Ward method", cex=0.6, hang = -1)
rect.hclust(cluster_ward, 4, border = "orange")
```

#### Centroid method

The centroid method considers the similarity between two clusters as the distance between its centroids.

```{r}
cluster_centroid = hclust(distance, "centroid")
plot(cluster_centroid, labels = data$Airport, xlab = "Distance - Centroid method", cex=0.6, hang = -1)
rect.hclust(cluster_centroid, 4, border = "darkgreen")
```

### Comparing results from different hierarchical methods

Now lets evaluate the **membership** of each observation with the `cutree` function for each method.

```{r}
member_single = cutree(cluster_single, 4)
member_complete = cutree(cluster_complete, 4)
member_average = cutree(cluster_average, 4)
member_ward = cutree(cluster_ward, 4)
member_centroid = cutree(cluster_centroid, 4)
```

Compare how common each method is to each other.

```{r}
table(member_complete, member_average) # compare the complete linkage with the average linkage
```

> **Note:** Try comparing other methods, and evaluate how common they are.

### Silhouette Plots

evaluate which method is more appropriate

The silhouette plot evaluates how similar an observation is to its own cluster compared to other clusters.
The clustering configuration is appropriate when most objects have high values.
Low or negative values indicate that the clustering method is not appropriate or the number of clusters is not ideal.

```{r}
plot(silhouette(member_single, distance))
plot(silhouette(member_complete, distance))
plot(silhouette(member_average, distance))
plot(silhouette(member_ward, distance))
plot(silhouette(member_centroid, distance))
```

## Non-Hirarchical Clustering

##### K-means clustering

-   k-means with n=3 clusters

```{r}
km_clust = kmeans(data_scaled, 3)
km_clust #print the results
```

-   Other ways of setting the number of clusters

This algorithm will detect how many clusters from 1 to 10 explains more variance

```{r}
  k = list()
  for(i in 1:10){
    k[[i]] = kmeans(data_scaled, i)
  }
```

> **Note**: Try printing the k value and take a look at the ratio `between_SS` / `total_SS`.
> Evaluate how it varies when you add clusters.

Now, lets plot `between_SS` / `total_SS` into a scree plot

```{r}
betSS_totSS = list()
for(i in 1:10){
betSS_totSS[[i]] = k[[i]]$betweenss/k[[i]]$totss
}
plot(1:10, betSS_totSS, type = "b", ylab = "Between SS / Total SS", xlab = "Number of clusters")
```

Let take out the outliers and see the difference in the k-means clustering:

-   **Examine the boxplots**

```{r}
par(cex.axis=0.6, mar=c(11,2,1,1))# Make labels fit in the boxplot
boxplot(data_scaled, las = 2) #labels rotated to vertical
```

-   **Detect the outliers**

```{r}
outliers = boxplot.stats(data_scaled)$out
outliers
```

-   **Remove rows with outliers**

```{r}
nrow(data_scaled) #32
out_ind = which(data_scaled %in% c(outliers)) #the row.names that contain outliers
data_no_outliers = data_scaled[-c(out_ind),] #remove those rows from the data_scaled
nrow(data_no_outliers) #31
```

> **Note:** There are many methods to treat outliers.
> This is just one of them.
> Note that it is not very appropriate, since it removes many observations that are relevant for the analysis.
> Try using other methods and evaluate the difference.

Execute a k-means clustering with the dataset without the outliers and see the difference.

```{r}
km_no_outliers = kmeans(data_no_outliers, 3)
km_no_outliers
```

#### Ploting the clusters

Finally, plotting the clusters results to check if they make sense.\
Let us go back to first example and take a look.

-   **K-means with outliers**

```{r}
plot(Numberofairlines ~ Destinations, data, col = km_clust$cluster)
with(data, text(Numberofairlines ~ Destinations, label = Airport, pos = 1, cex = 0.6))
```

-   **K-means without outliers**

```{r}
plot(Numberofairlines ~ Destinations, data, col = km_no_outliers$cluster)
with(data, text(Numberofairlines ~ Destinations, label = Airport, pos = 1, cex = 0.6))
```
