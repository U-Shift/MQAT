[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MQAT",
    "section": "",
    "text": "1 Introduction\nMaterials, data and tutorials for the Quantitative Methods of Analysis in Transportation course of the MSc in Transportation Systems at Instituto Superior Técnico - University of Lisbon.\nThis website aims to provide tools to deal with exploring and treating transportation datasets using R programming, an open-source and widely used tool for data analytics in urban mobility.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#recommended-readings",
    "href": "index.html#recommended-readings",
    "title": "MQAT",
    "section": "Recommended readings",
    "text": "Recommended readings\n\nEngel (2023) Introduction to R.\nWickham, Çetinaka-Rundel, and Grolemund (2017) R for Data Science.\nLovelace, Nowosad, and Muenchow (2024) Geocomputation with R.\n\n\n\n\n\n\n\nThis website was developed entirely in R with Quarto.\n\n\n\n\n\n\n\n\n\nEngel, Claudia A. 2023. Introduction to r. cengel.github.io/R-intro/.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2024. Geocomputation with r. 2nd ed. Chapman; Hall/CRC. https://r.geocompx.org/.\n\n\nWickham, Hadley, Mine Çetinaka-Rundel, and Garrett Grolemund. 2017. R for Data Science. 2nd ed. O’Reilly Sebastopol. https://r4ds.hadley.nz/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "2  Software",
    "section": "",
    "text": "2.1 R\nIn this chapter we will guide you through the installation of R, RStudio and the packages you will need for this course.\nR and RStudio1 are separate downloads.\nYou will need R installed on your computer. R stats (how it is also known) is a programming language and free software environment for statistical computing and graphics supported by the R Foundation for Statistical Computing.\nThe download links live at The Comprehensive R Archive Network (aka CRAN). he most recent version is 4.5.1, but you can use &gt;= 4.1.x if you already have it installed.\nAfter this installation, you don’t need to open R base. Please proceed to install RStudio.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Software</span>"
    ]
  },
  {
    "objectID": "software.html#r",
    "href": "software.html#r",
    "title": "2  Software",
    "section": "",
    "text": "WindowsMacUbuntu\n\n\nDownload R-4.5.1 for Windows and run the executable file.\n\n\n\n\n\n\nImportant\n\n\n\nYou will also need to install Rtools, which is a collection of tools necessary to build R packages in Windows.\n\n\n\n\nDownload R-4.5.1 for MacOX. You will have to choose between the arm64 or the x86-64 version.\nDownload the .pkg file and install it as usual.\n\n\n\nThese are instructions for Ubuntu. If you use other linux distribution, please follow the instructions on The Comprehensive R Archive Network - CRAN.\n\nYou can look for R in the Ubuntu Software Center or install it via the terminal:\n\n# sudo apt update && sudo apt upgrade -y\nsudo apt install r-base\n\nOr, if you prefer, you can install the latest version of R from CRAN:\n\n# update indices\nsudo apt update -qq\n# install two helper packages we need\nsudo apt install --no-install-recommends software-properties-common dirmngr\n# add the signing key (by Michael Rutter) for these repos\nwget -qO- https://cloud.r-project.org/bin/linux/ubuntu/marutter_pubkey.asc | sudo tee -a /etc/apt/trusted.gpg.d/cran_ubuntu_key.asc\n# add the R 4.0 repo from CRAN -- adjust 'focal' to 'groovy' or 'bionic' as needed\nsudo add-apt-repository \"deb https://cloud.r-project.org/bin/linux/ubuntu $(lsb_release -cs)-cran40/\"\n\nThen run:\n\nsudo apt install r-base r-base-core r-recommended r-base-dev\n\n[Optional] To keep up-to-date r version and packages, you can follow the instructions at r2u",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Software</span>"
    ]
  },
  {
    "objectID": "software.html#rstudio",
    "href": "software.html#rstudio",
    "title": "2  Software",
    "section": "2.2 RStudio",
    "text": "2.2 RStudio\nRStudio Desktop is an integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management.\nRStudio is available for free download from Posit RStudio.\n\nWindows 10/11MacOSUbuntu\n\n\nDownload RStudio 2025.09 and run the executable file.\n\n\nDownload RStudio 2025.09 and install it as usual.\n\n\n\nThese are instructions for Ubuntu 24. If you use other linux distribution, please follow the instructions on Posit RStudio.\n\nInstall it via the terminal:\n\nsudo apt install libssl-dev libclang-dev\nwget https://download1.rstudio.org/electron/jammy/amd64/rstudio-2025.09.0-387-amd64.deb\nsudo dpkg -i rstudio*\nrm -v rstudio*",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Software</span>"
    ]
  },
  {
    "objectID": "software.html#r-packages",
    "href": "software.html#r-packages",
    "title": "2  Software",
    "section": "2.3 R packages",
    "text": "2.3 R packages\nYou will need to install some packages to work with the data and scripts in this course.\nYou can install them in RStudio by searching for them in the Packages tab:\n\nor by running the following code in the console:\n\ninstall.packages(\"tidyverse\")\ninstall.packages(\"readxl\")\n\ninstall.packages(\"sf\")\ninstall.packages(\"mapview\")\ninstall.packages(\"rmarkdown\")\ninstall.packages(\"centr\")\ninstall.packages(\"od\")\ninstall.packages(\"openrouteservice\")\n\ninstall.packages(c(\"remotes\", \"devtools\", \"usethis\")) # optional\ninstall.packages(\"osmextract\") # optional\ninstall.packages(\"stplanr\") # optional",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Software</span>"
    ]
  },
  {
    "objectID": "software.html#tinytex",
    "href": "software.html#tinytex",
    "title": "2  Software",
    "section": "2.4 TinyTex",
    "text": "2.4 TinyTex\nTo be able to export your Rmakdown document to a pdf file, you will need a LaTeX processor. tinytex is a lightweight R package with the most used features of LaTeX that you can use with R.\nYou can install it from RStudio console by running:\n\n# you need to run both lines\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()\n\nTo use some LaTeX packages, you also need to install tlmgr.\n\ntinytex::tlmgr_install()\ntinytex::tlmgr_update() # updates all latex pkgs",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Software</span>"
    ]
  },
  {
    "objectID": "software.html#ors",
    "href": "software.html#ors",
    "title": "2  Software",
    "section": "2.5 Open Route Service",
    "text": "2.5 Open Route Service\nFor some GIS exercises, we will use the Open Route Service to calculate routes and travel times.\nIn RStudio console, run:\n\n# install.packages(\"openrouteservice\")\n\n# instructions to setup at the U-Shift server\noptions(openrouteservice.url = \"https://...\") # the server address provided to you\nopenrouteservice::ors_api_key(\"\") # empty key",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Software</span>"
    ]
  },
  {
    "objectID": "software.html#footnotes",
    "href": "software.html#footnotes",
    "title": "2  Software",
    "section": "",
    "text": "We will use RStudio, although if you already use other studio such as VScode, that’s also fine.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Software</span>"
    ]
  },
  {
    "objectID": "r-basics.html",
    "href": "r-basics.html",
    "title": "3  R basics",
    "section": "",
    "text": "3.1 Math operations\nIn this chapter we will introduce to the R basics and some exercises to get familiar to how R works.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#math-operations",
    "href": "r-basics.html#math-operations",
    "title": "3  R basics",
    "section": "",
    "text": "Sum\n\n1+1\n\n[1] 2\n\n\n\n\nSubtraction\n\n5-2\n\n[1] 3\n\n\n\n\nMultiplication\n\n2*2\n\n[1] 4\n\n\n\n\nDivision\n\n8/2\n\n[1] 4\n\n\n\n\nRound the number\n\nround(3.14)\n\n[1] 3\n\nround(3.14, 1) # The \"1\" indicates to round it up to 1 decimal digit.\n\n[1] 3.1\n\n\nYou can use help ?round in the console to see the description of the function, and the default arguments.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#basics",
    "href": "r-basics.html#basics",
    "title": "3  R basics",
    "section": "3.2 Basics",
    "text": "3.2 Basics\n\nCombinations\n\nc(1, 2, 3)\n\n[1] 1 2 3\n\nc(1:3) # The \":\" indicates a range between the first and second numbers. \n\n[1] 1 2 3\n\n\n\n\n\n\n\n\nTry it yourself\n\n\n\nTry to write a combination with the numbers 10, 11, 56, 57, 58.\n\n\n\n\nCreate a comment with ctrl + shift + c\n\n# Comments help you organize your code. \n# A line with a comment will not be executed. \n\n\n\nCreate a table\nA simple table with the number of trips by car, PT, walking, and cycling in a hypothetical street segment at a certain period.\nDefine variables\n\nmodes &lt;- c(\"car\", \"PT\", \"walking\", \"cycling\") # you can use \"=\" or \"&lt;-\"\ntrips = c(200, 50, 300, 150) # key sensitive (uppercase and lowercase letters are different)\n\nJoin the variables to create a table\n\ntable_example = data.frame(modes, trips)\n\nTake a look at the table\nVisualize the table by clicking on the “Data” in the “Environment” page or use :\n\nView(table_example)\n\nLook at the first row\n\ntable_example[1,] #rows and columns start from 1 in R, differently from Python which starts from 0.\n\n  modes trips\n1   car   200\n\n\nLook at first row and column\n\ntable_example[1,1]\n\n[1] \"car\"\n\ntable_example[1,4]\n\nNULL",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#practical-exercise",
    "href": "r-basics.html#practical-exercise",
    "title": "3  R basics",
    "section": "3.3 Practical exercise",
    "text": "3.3 Practical exercise\nDataset: the number of trips between all municipalities in the Lisbon Metropolitan Area, Portugal (INE 2018).\n\nImport dataset\nYou can click directly in the file under the “Files” pan, or:\n\ndata = readRDS(\"data/TRIPSmode_mun.Rds\")\n\n\n\n\n\n\n\nAfter you type \" you can use tab to navigate between folders and files and enter to autocomplete.\n\n\n\n\n\nTake a first look at the data\nSummary statistics\n\nsummary(data)\n\n  Origin_mun        Destination_mun        Total             Walk       \n Length:315         Length:315         Min.   :     7   Min.   :     0  \n Class :character   Class :character   1st Qu.:   330   1st Qu.:     0  \n Mode  :character   Mode  :character   Median :  1090   Median :     0  \n                                       Mean   : 16825   Mean   :  4033  \n                                       3rd Qu.:  5374   3rd Qu.:     0  \n                                       Max.   :875144   Max.   :306289  \n      Bike              Car            PTransit            Other        \n Min.   :   0.00   Min.   :     0   Min.   :     0.0   Min.   :    0.0  \n 1st Qu.:   0.00   1st Qu.:   263   1st Qu.:     5.0   1st Qu.:    0.0  \n Median :   0.00   Median :   913   Median :   134.0   Median :    0.0  \n Mean   :  80.19   Mean   :  9956   Mean   :  2602.6   Mean   :  152.4  \n 3rd Qu.:   0.00   3rd Qu.:  4408   3rd Qu.:   975.5   3rd Qu.:   62.5  \n Max.   :5362.00   Max.   :349815   Max.   :202428.0   Max.   :11647.0  \n\n\nCheck the structure of the data\n\nstr(data)\n\nClasses 'tbl_df', 'tbl' and 'data.frame':   315 obs. of  8 variables:\n $ Origin_mun     : chr  \"Alcochete\" \"Alcochete\" \"Alcochete\" \"Alcochete\" ...\n $ Destination_mun: chr  \"Alcochete\" \"Almada\" \"Amadora\" \"Barreiro\" ...\n $ Total          : num  20478 567 188 867 114 ...\n $ Walk           : num  6833 0 0 0 0 ...\n $ Bike           : num  320 0 0 0 0 0 0 0 91 0 ...\n $ Car            : num  12484 353 107 861 114 ...\n $ PTransit       : num  833 0 81 5 0 ...\n $ Other          : num  7 214 0 0 0 0 0 0 0 0 ...\n\n\nCheck the number of rows (observations) and columns (variables)\n\nnrow(data)\n\n[1] 315\n\nncol(data)\n\n[1] 8\n\n\nOpen the dataset\n\nView(data)\n\n\n\n\nThe following three rules make a dataset tidy: variables are columns, observations are rows, and values are cells. [@wickham17r]\n\n\n\n\nExplore the data\nCheck the total number of trips\nUse $ to select a variable of the data\n\nsum(data$Total)\n\n[1] 5299853\n\n\nPercentage of car trips related to the total\n\nsum(data$Car)/sum(data$Total) * 100\n\n[1] 59.17638\n\n\nPercentage of active trips related to the total\n\n(sum(data$Walk) + sum(data$Bike)) / sum(data$Total) * 100\n\n[1] 24.44883\n\n\n\n\nModify original data\nCreate a column with the sum of the number of trips for active modes\n\ndata$Active = data$Walk + data$Bike\n\nFilter by condition (create new tables)\nFilter trips only with origin from Lisbon\n\ndata_Lisbon = data[data$Origin == \"Lisboa\",]\n\nFilter trips with origin different from Lisbon\n\ndata_out_Lisbon = data[data$Origin != \"Lisboa\",]\n\nFilter trips with origin and destination in Lisbon\n\ndata_in_Out_Lisbon = data[data$Origin == \"Lisboa\" & data$Destination == \"Lisboa\",]\n\nRemove the first column\n\ndata = data[ ,-1] #first column\n\nCreate a table only with origin, destination and walking trips\nThere are many ways to do the same operation.\n\nnames(data)\n\n[1] \"Destination_mun\" \"Total\"           \"Walk\"            \"Bike\"           \n[5] \"Car\"             \"PTransit\"        \"Other\"           \"Active\"         \n\n\n\ndata_walk2 = data[ ,c(1,2,4)]\n\n\ndata_walk3 = data[ ,-c(3,5:9)]\n\n\n\nExport data\nSave data in .csv and .Rds\n\nwrite.csv(data, 'data/dataset.csv', row.names = FALSE)\nsaveRDS(data, 'data/dataset.Rds') #Choose a different file. \n\n\n\nImport data\n\ncsv_file = read.csv(\"data/dataset.csv\")\nrds_file = readRDS(\"data/dataset.Rds\")\n\n\n\n\n\n\n\nINE. 2018. “Mobilidade e Funcionalidade Do Território Nas Áreas Metropolitanas do Porto e de Lisboa: 2017.” Lisboa: Instituto National de Estatística. https://www.ine.pt/xportal/xmain?xpid=INE&xpgid=ine_publicacoes&PUBLICACOESpub_boui=349495406&PUBLICACOESmodo=2&xlang=pt.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R basics</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html",
    "href": "data-manipulation.html",
    "title": "4  Data manipulation",
    "section": "",
    "text": "4.1 Select variables\nIn this chapter we will use some very useful dplyr functions to handle and manipulate data.\nYou can load the dplyr package directly, or load the entire tidy universe (tidyverse).\nUsing the same dataset as in R basics but with slightly differences1.\nWe will do the same operations but in a simplified way.\nYou don’t need to know everything! And you don’t need to know by heart. The following functions are the ones you will probably use most of the time to handle data.\nHave a look at your dataset. You can open using View(), look at the information at the “Environment” panel, or even print the same information using glimpse()\nglimpse(TRIPS)\nWe will create a new dataset with Origin, Walk, Bike and Total. This time we will use the select() function.\nTRIPS_new = select(TRIPS, Origin, Walk, Bike, Total) # the first argument is the dataset\nThe first argument, as usually in R, is the dataset, and the remaining ones are the columns to select.\nWith most of the dplyr functions you don’t need to refer to data$... you can simply type the variable names (and even without the \"...\"!). This makes coding in R simpler :)\nYou can also remove columns that you don’t need.\nTRIPS_new = select(TRIPS_new, -Total) # dropping the Total column",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html#select-variables",
    "href": "data-manipulation.html#select-variables",
    "title": "4  Data manipulation",
    "section": "",
    "text": "Using pipes!\nNow, let’s introduce pipes. Pipes are a rule as: “With this, do this.”\nThis is useful to skip the first argument of the functions (usually the dataset to apply the function).\nApplying a pipe to the select() function, we can write as:\n\nTRIPS_new = TRIPS |&gt; select(Origin, Walk, Bike, Total)\n\nTwo things to note:\n\nThe pipe symbol can be written as |&gt; or %&gt;%. 2 To write it you may also use the ctrl+shift+m shortcut.\nAfter typing select( you can press tab and the list of available variables of that dataset will show up! Enter to select. With this you prevent typo errors.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html#filter-observations",
    "href": "data-manipulation.html#filter-observations",
    "title": "4  Data manipulation",
    "section": "4.2 Filter observations",
    "text": "4.2 Filter observations\nYou can filter observations based on a condition using the filter() function.\n\nTRIPS2 = TRIPS[TRIPS$Total &gt; 25000,] # using r-base, you cant forget the comma\nTRIPS2 = TRIPS2 |&gt; filter(Total &gt; 25000) # using dplyr, it's easier\n\nYou can have other conditions inside the condition.\n\nsummary(TRIPS$Total)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    361    5918   17474   22457   33378  112186 \n\nTRIPS3 = TRIPS |&gt; filter(Total &gt; median(Total)) \n\nOther filter conditions:\n\n== equal, != different\n&lt; smaller, &gt; greater, &lt;= smaller or equal, &gt;= greater or equal\n& and, | or\nis.na, !is.na is not NA\n%in%, !%in% not in",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html#create-new-variables",
    "href": "data-manipulation.html#create-new-variables",
    "title": "4  Data manipulation",
    "section": "4.3 Create new variables",
    "text": "4.3 Create new variables\nYou can also try again to create a variable of Car percentage using pipes! To create a new variable or change an existing one (overwriting), you can use the mutate() function.\n\nTRIPS$Car_perc = TRIPS$Car/TRIPS$Total * 100 # using r-base\n\nTRIPS = TRIPS |&gt; mutate(Car_perc = Car/Total * 100) # using dplyr",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html#change-data-type",
    "href": "data-manipulation.html#change-data-type",
    "title": "4  Data manipulation",
    "section": "4.4 Change data type",
    "text": "4.4 Change data type\nData can be in different formats. For example, the variable Origin is a character, but we can convert it to a numeric variable.\n\nclass(TRIPS$Origin)\n\n[1] \"character\"\n\nTRIPS = TRIPS |&gt; \n  mutate(Origin_num = as.integer(Origin)) # you can use as.numeric() as well\nclass(TRIPS$Origin_num)\n\n[1] \"integer\"\n\n\nMost used data types are:\n\ninteger (int)\nnumeric (num)\ncharacter (chr)\nlogical (logical)\ndate (Date)\nfactor (factor)\n\n\nFactors\nFactors are useful to deal with categorical data. You can convert a character to a factor using as.factor(), and also use labels and levels for categorical ordinal data.\nWe can change the Lisbon variable to a factor, and Internal too.\n\nTRIPS = TRIPS |&gt; \n  mutate(Lisbon_factor = factor(Lisbon, labels = c(\"No\", \"Yes\")),\n         Internal_factor = factor(Internal, labels = c(\"Inter\", \"Intra\")))\n\nBut how do we know which levels come first? A simple way is to use table() or unique() functions.\n\nunique(TRIPS$Lisbon) # this will show all the different values\n\n[1] 0 1\n\ntable(TRIPS$Lisbon) # this will show the frequency of each value\n\n\n  0   1 \n188  48 \n\ntable(TRIPS$Lisbon_factor)\n\n\n No Yes \n188  48 \n\n\nThe first number to appear is the first level, and so on.\nYou can see the difference between using a continuous variable (in this case Lisbon` has 0 and 1) and a categorical variable (Lisbon_factor).\n\nplot(TRIPS$Lisbon) # the values range between 0 and 1\n\n\n\n\n\n\n\nplot(TRIPS$Lisbon_factor) # the values are categorical and labeled with Yes/No",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html#join-data-tables",
    "href": "data-manipulation.html#join-data-tables",
    "title": "4  Data manipulation",
    "section": "4.5 Join data tables",
    "text": "4.5 Join data tables\nWhen having relational tables - i.e. with a common identifier - it is useful to be able to join them in a very efficient way.\nleft_join is a function that joins two tables by a common column. The first table is the one that will be kept, and the second one will be joined to it. How left_join works:\n\n\n\n\n\n\n\nA visual representation of the left join where every row in x appears in the output.Source: R for Data Science.\n\n\nLet’s join the municipalities to this table with a supporting table that includes all the relation between neighbourhoods and municipalities, and the respective names and codes.\n\nMunicipalities = readRDS(\"data/Municipalities_names.Rds\")\n\n\nhead(TRIPS)\n\n# A tibble: 6 × 13\n  Origin Total  Walk  Bike   Car PTransit Other Internal Lisbon Car_perc\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n1 110501 35539 11325  1309 21446     1460     0        0      0     60.3\n2 110501 47602  3502   416 37727     5519   437        1      0     79.3\n3 110506 37183 12645    40 22379     2057    63        0      0     60.2\n4 110506 42313  1418   163 37337     3285   106        1      0     88.2\n5 110507 30725  9389  1481 19654      201     0        0      0     64.0\n6 110507 54586  2630   168 44611     6963   215        1      0     81.7\n# ℹ 3 more variables: Origin_num &lt;int&gt;, Lisbon_factor &lt;fct&gt;,\n#   Internal_factor &lt;fct&gt;\n\ntail(Municipalities)\n\n    Mun_code Neighborhood_code        Municipality\n113     1109            110913               Mafra\n114     1114            111409 Vila Franca de Xira\n115     1109            110918               Mafra\n116     1109            110904               Mafra\n117     1502            150202           Alcochete\n118     1109            110911               Mafra\n                                             Neighborhood\n113                                         Santo Isidoro\n114                                   Vila Franca de Xira\n115 União das freguesias de Azueira e Sobral da Abelheira\n116                                            Encarnação\n117                                               Samouco\n118                                             Milharado\n\n\nWe can see that we have a common variable: Origin in TRIPS and Neighborhood_code in Municipalities.\nTo join these two tables we need to specify the common variable in each table, using the by argument.\n\nTRIPSjoin = TRIPS |&gt; left_join(Municipalities, by = c(\"Origin\" = \"Neighborhood_code\"))\n\nIf you prefer, you can mutate or rename a variable so both tables have the same name. When both tables have the same name, you don’t need to specify the by argument.\n\nMunicipalities = Municipalities |&gt; rename(Origin = \"Neighborhood_code\") # change name\nTRIPSjoin = TRIPS |&gt; left_join(Municipalities) # automatic detects common variable\n\nAs you can see, both tables don’t need to be the same length. The left_join function will keep all the observations from the first table, and join the second table to it. If there is no match, the variables from the second table will be filled with NA.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html#group_by-and-summarize",
    "href": "data-manipulation.html#group_by-and-summarize",
    "title": "4  Data manipulation",
    "section": "4.6 group_by and summarize",
    "text": "4.6 group_by and summarize\nWe have a very large table with all the neighbourhoods and their respective municipalities. We want to know the total number of trips with origin in each municipality.\nTo make it easier to understand, let’s keep only the variables we need.\n\nTRIPSredux = TRIPSjoin |&gt; select(Origin, Municipality, Internal, Car, Total)\nhead(TRIPSredux)\n\n# A tibble: 6 × 5\n  Origin Municipality Internal   Car Total\n  &lt;chr&gt;  &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 110501 Cascais             0 21446 35539\n2 110501 Cascais             1 37727 47602\n3 110506 Cascais             0 22379 37183\n4 110506 Cascais             1 37337 42313\n5 110507 Cascais             0 19654 30725\n6 110507 Cascais             1 44611 54586\n\n\nWe can group this table by the Municipality variable and summarize the number of trips with origin in each municipality.\n\nTRIPSsum = TRIPSredux |&gt; \n  group_by(Municipality) |&gt; # you won't notice any chagne with only this\n  summarize(Total = sum(Total))\nhead(TRIPSsum)\n\n# A tibble: 6 × 2\n  Municipality   Total\n  &lt;chr&gt;          &lt;dbl&gt;\n1 Alcochete      36789\n2 Almada        289834\n3 Amadora       344552\n4 Barreiro      133658\n5 Cascais       373579\n6 Lisboa       1365111\n\n\nWe summed the total number of trips in each municipality.\nIf we want to group by more than one variable, we can add more group_by() functions.\n\nTRIPSsum2 = TRIPSredux |&gt; \n  group_by(Municipality, Internal) |&gt; \n  summarize(Total = sum(Total),\n            Car = sum(Car))\nhead(TRIPSsum2)\n\n# A tibble: 6 × 4\n# Groups:   Municipality [3]\n  Municipality Internal  Total    Car\n  &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Alcochete           0  16954   9839\n2 Alcochete           1  19835  15632\n3 Almada              0 105841  49012\n4 Almada              1 183993 125091\n5 Amadora             0 117727  33818\n6 Amadora             1 226825 142386\n\n\nWe summed the total number of trips and car trips in each municipality, separated by inter and intra municipal trips.\n\n\n\n\n\n\nIt is a good practice to use the ungroup() function after the group_by() function. This will remove the grouping. If you don’t do this, the grouping will be kept and you may have unexpected results in the next time you use that dataset.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html#arrange-data",
    "href": "data-manipulation.html#arrange-data",
    "title": "4  Data manipulation",
    "section": "4.7 Arrange data",
    "text": "4.7 Arrange data\nYou can sort a dataset by one or more variables.\nFor instance, arrange() by Total trips, ascending or descending order.\n\nTRIPS2 = TRIPSsum2 |&gt; arrange(Total)\nTRIPS2 = TRIPSsum2 |&gt; arrange(-Total) # descending\n\nTRIPS2 = TRIPSsum2 |&gt; arrange(Municipality) # alphabetic\n\nTRIPS4 = TRIPS |&gt; arrange(Lisbon_factor, Total) # more than one variable\n\nThis is not the same as opening the view table and click on the arrows. When you do that, the order is not saved in the dataset. If you want to save the order, you need to use the arrange() function.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html#all-together-now",
    "href": "data-manipulation.html#all-together-now",
    "title": "4  Data manipulation",
    "section": "4.8 All together now!",
    "text": "4.8 All together now!\nThis is the pipes magic. It takes the last result and applies the next function to it. “With this, do this.”. You can chain as many functions as you want.\n\nTRIPS_pipes = TRIPS |&gt; \n  select(Origin, Internal, Car, Total) |&gt; \n  \n  mutate(Origin_num = as.integer(Origin)) |&gt; \n  mutate(Internal_factor = factor(Internal, labels = c(\"Inter\", \"Intra\"))) |&gt; \n  \n  filter(Internal_factor == \"Inter\")|&gt;\n  \n  left_join(Municipalities) |&gt;\n  \n  group_by(Municipality) |&gt;\n  summarize(Total = sum(Total),\n            Car = sum(Car),\n            Car_perc = Car/Total * 100) |&gt; \n  ungroup() |&gt; \n  \n  arrange(desc(Car_perc))\n\nWith this code we will have a table with the total number of intercity trips, by municipality, with their names instead of codes, arranged by the percentage of car trips.\n\nTRIPS_pipes\n\n# A tibble: 18 × 4\n   Municipality         Total    Car Car_perc\n   &lt;chr&gt;                &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 Mafra                65811  46329     70.4\n 2 Sesimbra             49370  31975     64.8\n 3 Cascais             161194  96523     59.9\n 4 Palmela              66428  39688     59.7\n 5 Alcochete            16954   9839     58.0\n 6 Setúbal             129059  70318     54.5\n 7 Montijo              57164  30900     54.1\n 8 Seixal              120747  63070     52.2\n 9 Sintra              237445 123408     52.0\n10 Oeiras              134862  66972     49.7\n11 Almada              105841  49012     46.3\n12 Loures              132310  60478     45.7\n13 Barreiro             52962  24160     45.6\n14 Odivelas             93709  39151     41.8\n15 Vila Franca de Xira 115152  47201     41.0\n16 Moita                51040  17394     34.1\n17 Amadora             117727  33818     28.7\n18 Lisboa              280079  69038     24.6",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html#other-dplyr-functions",
    "href": "data-manipulation.html#other-dplyr-functions",
    "title": "4  Data manipulation",
    "section": "4.9 Other dplyr functions",
    "text": "4.9 Other dplyr functions\nYou can explore other dplyr functions and variations to manipulate data in the dplyr cheat sheet:\n\nTake a particular attention to pivot_wider and pivot_longer (tidyr package) to transform OD matrices in wide and long formats.\n\n\n\nOD matrix in long format\n\n\nOrigins\nDestinations\nTrips\n\n\n\n\nA\nB\n20\n\n\nA\nC\n45\n\n\nB\nA\n10\n\n\nC\nC\n5\n\n\nC\nA\n30\n\n\n\n\n\n\n\n\nOD matrix in wide format\n\n\nTrips\nA\nB\nC\n\n\n\n\nA\nNA\n20\n45\n\n\nB\n10\nNA\nNA\n\n\nC\n30\nNA\n5",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html#footnotes",
    "href": "data-manipulation.html#footnotes",
    "title": "4  Data manipulation",
    "section": "",
    "text": "This dataset includes the number of trips with origin in each neighborhood, divided by mode of transport, and inter or intra municipal trips.↩︎\nYou can change this in RStudio &gt; Tools &gt; Global Options &gt; Code.↩︎",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "5  Exploratory Data Analysis",
    "section": "",
    "text": "5.1 Intro\nThis chapter will show you how to use visualization and transformation to explore your data in a systematic way, a task that statisticians call exploratory data analysis, or EDA for short. EDA is an iterative cycle, where you should feel free to “fall in love” with your data.\nYou:\nSee Wickham, Çetinaka-Rundel, and Grolemund (2017) for more, in particular the EDA chapter.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#intro",
    "href": "eda.html#intro",
    "title": "5  Exploratory Data Analysis",
    "section": "",
    "text": "Generate questions about your data.\nSearch for answers by visualizing, transforming, and modelling your data.\nUse what you learn to refine your questions and/or generate new questions.\n\n\nNote: EDA is not a formal process with a strict set of rules!\n\n\n\n\n\n\n\n\nDo it yourself with R\n\n\n\nCopy the script ExploratoryDataAnalysis.R and paste it in your session.\nRun each line using CTRL + ENTER",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#load-packages",
    "href": "eda.html#load-packages",
    "title": "5  Exploratory Data Analysis",
    "section": "5.2 Load packages",
    "text": "5.2 Load packages\n\nlibrary(tidyverse) # Pack of most used libraries for data science\nlibrary(skimr) # summary of the data\nlibrary(DataExplorer) # exploratory data analysis\nlibrary(corrplot) # correlation plots",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#dataset",
    "href": "eda.html#dataset",
    "title": "5  Exploratory Data Analysis",
    "section": "5.3 Dataset",
    "text": "5.3 Dataset\nThe database used in this example is a treated database from the Mobility Survey for the metropolitan areas of Lisbon and Porto in 2018 (INE 2018). We will only focus on trips within the metropolitan area of Lisbon.\nIncluded variables:\n\nOrigin_dicofre16 - Code of Freguesia (district) as set by INE after 2016 (Distrito + Concelho + Freguesia), for trip origin\nTotal - number of trips with origin at each district\nWalk - number of walking trips\nBike - number of bike trips\nCar - number of car trips. Includes taxi and motorcycle.\nPTransit - number of Public Transit trips\nOther - number of other trips (truck, van, tractor, aviation)\nDistance - average trip distance (km)\nDuration - average trip duration (minutes)\nCar_perc - percentage of car trips\nN_INDIVIDUOS - number of residents (INE 2022)\nMale_perc - percentage of male residents (INE 2022)\nIncomeHH - average household income\nNvehicles - average number of car/motorcycle vehicles in the household\nDrivingLic - percentage of car driving licence holders\nCarParkFree_Work - percentage of respondents with free car parking at the work location\nPTpass - percentage of public transit monthly pass holders\ninternal - binary variable (factor). Yes: trip with same TAZ origin and destination, No: trips with different destination\nLisboa - binary variable (factor). Yes: the district is part of Lisbon municipality, No: otherwise\nArea_km2 - area of in Origin_dicofre16, in km2\n\n\nImport dataset\n\ndata = readRDS(\"../data/IMOBmodel.Rds\")\n\nTake a look at the dataset\n\nView(data) # open in table\nglimpse(data) # glimpse of the dataset\n\nRows: 236\nColumns: 20\n$ Origin_dicofre16 &lt;chr&gt; \"110501\", \"110501\", \"110506\", \"110506\", \"110507\", \"11…\n$ Total            &lt;dbl&gt; 35539, 47602, 37183, 42313, 30725, 54586, 57747, 6788…\n$ Walk             &lt;dbl&gt; 11325, 3502, 12645, 1418, 9389, 2630, 20423, 3573, 62…\n$ Bike             &lt;dbl&gt; 1309, 416, 40, 163, 1481, 168, 1406, 809, 13, 37, 3, …\n$ Car              &lt;dbl&gt; 21446, 37727, 22379, 37337, 19654, 44611, 33044, 5225…\n$ PTransit         &lt;dbl&gt; 1460, 5519, 2057, 3285, 201, 6963, 2477, 10534, 110, …\n$ Other            &lt;dbl&gt; 0, 437, 63, 106, 0, 215, 396, 717, 2, 169, 3, 824, 0,…\n$ Distance         &lt;dbl&gt; 11.779, 11.779, 9.868, 9.868, 9.600, 9.600, 12.875, 1…\n$ Duration         &lt;dbl&gt; 23.96, 23.96, 22.78, 22.78, 23.39, 23.39, 26.89, 26.8…\n$ Car_perc         &lt;dbl&gt; 60.34497, 79.25507, 60.18611, 88.24002, 63.96745, 81.…\n$ N_INDIVIDUOS     &lt;dbl&gt; 44165, 44165, 59238, 59238, 46529, 46529, 64192, 6419…\n$ Male_perc        &lt;dbl&gt; 47.66, 47.66, 47.12, 47.12, 45.54, 45.54, 46.14, 46.1…\n$ IncomeHH         &lt;dbl&gt; 2204.32, 2204.32, 1896.05, 1896.05, 2430.43, 2430.43,…\n$ Nvehicles        &lt;dbl&gt; 1.86, 1.86, 1.73, 1.73, 1.60, 1.60, 1.64, 1.64, 1.20,…\n$ DrivingLic       &lt;dbl&gt; 63.03, 63.03, 65.42, 65.42, 67.05, 67.05, 72.75, 72.7…\n$ CarParkFree_Work &lt;dbl&gt; 49.97, 49.97, 57.09, 57.09, 47.43, 47.43, 56.47, 56.4…\n$ PTpass           &lt;dbl&gt; 11.14, 11.14, 17.12, 17.12, 18.25, 18.25, 19.73, 19.7…\n$ internal         &lt;fct&gt; Yes, No, Yes, No, Yes, No, Yes, No, Yes, No, Yes, No,…\n$ Lisboa           &lt;fct&gt; No, No, No, No, No, No, No, No, Yes, Yes, Yes, Yes, Y…\n$ Area_km2         &lt;dbl&gt; 39.767414, 39.767414, 20.364310, 20.364310, 8.109861,…\n\nstr(data) # Structure of the dataset \n\n'data.frame':   236 obs. of  20 variables:\n $ Origin_dicofre16: chr  \"110501\" \"110501\" \"110506\" \"110506\" ...\n $ Total           : num  35539 47602 37183 42313 30725 ...\n $ Walk            : num  11325 3502 12645 1418 9389 ...\n $ Bike            : num  1309 416 40 163 1481 ...\n $ Car             : num  21446 37727 22379 37337 19654 ...\n $ PTransit        : num  1460 5519 2057 3285 201 ...\n $ Other           : num  0 437 63 106 0 215 396 717 2 169 ...\n $ Distance        : num  11.78 11.78 9.87 9.87 9.6 ...\n $ Duration        : num  24 24 22.8 22.8 23.4 ...\n $ Car_perc        : num  60.3 79.3 60.2 88.2 64 ...\n $ N_INDIVIDUOS    : num  44165 44165 59238 59238 46529 ...\n $ Male_perc       : num  47.7 47.7 47.1 47.1 45.5 ...\n $ IncomeHH        : num  2204 2204 1896 1896 2430 ...\n $ Nvehicles       : num  1.86 1.86 1.73 1.73 1.6 1.6 1.64 1.64 1.2 1.2 ...\n $ DrivingLic      : num  63 63 65.4 65.4 67 ...\n $ CarParkFree_Work: num  50 50 57.1 57.1 47.4 ...\n $ PTpass          : num  11.1 11.1 17.1 17.1 18.2 ...\n $ internal        : Factor w/ 2 levels \"Yes\",\"No\": 1 2 1 2 1 2 1 2 1 2 ...\n $ Lisboa          : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 2 2 ...\n $ Area_km2        : num  39.77 39.77 20.36 20.36 8.11 ...",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#summary-statistics",
    "href": "eda.html#summary-statistics",
    "title": "5  Exploratory Data Analysis",
    "section": "5.4 Summary statistics",
    "text": "5.4 Summary statistics\nHave an overview of the variables and their statistics’ summary.\n\nsummary(data) # Check the summary statistics\n\n Origin_dicofre16       Total             Walk              Bike        \n Length:236         Min.   :   361   Min.   :    0.0   Min.   :   0.00  \n Class :character   1st Qu.:  5918   1st Qu.:  763.2   1st Qu.:   0.00  \n Mode  :character   Median : 17474   Median : 3125.0   Median :  13.50  \n                    Mean   : 22457   Mean   : 5383.4   Mean   : 107.03  \n                    3rd Qu.: 33378   3rd Qu.: 8298.5   3rd Qu.:  98.75  \n                    Max.   :112186   Max.   :32646.0   Max.   :2040.00  \n      Car           PTransit         Other           Distance     \n Min.   :    0   Min.   :    0   Min.   :   0.0   Min.   : 6.835  \n 1st Qu.: 3243   1st Qu.:  249   1st Qu.:   2.0   1st Qu.: 9.539  \n Median : 9008   Median : 1057   Median :  44.0   Median :10.323  \n Mean   :13289   Mean   : 3474   Mean   : 203.4   Mean   :11.139  \n 3rd Qu.:21249   3rd Qu.: 4853   3rd Qu.: 281.5   3rd Qu.:12.097  \n Max.   :52631   Max.   :41672   Max.   :2391.0   Max.   :22.660  \n    Duration        Car_perc      N_INDIVIDUOS     Male_perc    \n Min.   :16.30   Min.   : 0.00   Min.   : 1566   Min.   :44.61  \n 1st Qu.:23.00   1st Qu.:45.40   1st Qu.:11060   1st Qu.:46.58  \n Median :24.70   Median :62.62   Median :20855   Median :47.51  \n Mean   :25.42   Mean   :59.00   Mean   :24324   Mean   :47.54  \n 3rd Qu.:27.73   3rd Qu.:75.72   3rd Qu.:36079   3rd Qu.:48.29  \n Max.   :37.42   Max.   :99.27   Max.   :68649   Max.   :55.94  \n    IncomeHH        Nvehicles       DrivingLic    CarParkFree_Work\n Min.   : 884.5   Min.   :1.020   Min.   :37.04   Min.   : 5.47   \n 1st Qu.:1417.8   1st Qu.:1.350   1st Qu.:57.67   1st Qu.:40.39   \n Median :1594.7   Median :1.545   Median :63.00   Median :50.51   \n Mean   :1732.6   Mean   :1.530   Mean   :62.50   Mean   :49.30   \n 3rd Qu.:1953.5   3rd Qu.:1.670   3rd Qu.:68.84   3rd Qu.:57.92   \n Max.   :3462.3   Max.   :2.420   Max.   :80.79   Max.   :87.60   \n     PTpass      internal  Lisboa       Area_km2      \n Min.   : 0.00   Yes:118   No :188   Min.   :  1.494  \n 1st Qu.:13.41   No :118   Yes: 48   1st Qu.:  5.044  \n Median :22.71                       Median : 11.598  \n Mean   :23.82                       Mean   : 25.553  \n 3rd Qu.:32.94                       3rd Qu.: 28.511  \n Max.   :60.45                       Max.   :282.125  \n\nskim(data) # In a more organized way\n\n\nData summary\n\n\nName\ndata\n\n\nNumber of rows\n236\n\n\nNumber of columns\n20\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n2\n\n\nnumeric\n17\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nOrigin_dicofre16\n0\n1\n6\n6\n0\n118\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ninternal\n0\n1\nFALSE\n2\nYes: 118, No: 118\n\n\nLisboa\n0\n1\nFALSE\n2\nNo: 188, Yes: 48\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nTotal\n0\n1\n22457.00\n19084.45\n361.00\n5917.75\n17474.00\n33377.50\n112186.00\n▇▃▂▁▁\n\n\nWalk\n0\n1\n5383.44\n6224.84\n0.00\n763.25\n3125.00\n8298.50\n32646.00\n▇▂▁▁▁\n\n\nBike\n0\n1\n107.03\n248.65\n0.00\n0.00\n13.50\n98.75\n2040.00\n▇▁▁▁▁\n\n\nCar\n0\n1\n13289.24\n12351.61\n0.00\n3243.00\n9008.00\n21248.75\n52631.00\n▇▃▂▁▁\n\n\nPTransit\n0\n1\n3473.79\n5467.82\n0.00\n249.00\n1057.00\n4853.00\n41672.00\n▇▁▁▁▁\n\n\nOther\n0\n1\n203.45\n336.04\n0.00\n2.00\n44.00\n281.50\n2391.00\n▇▁▁▁▁\n\n\nDistance\n0\n1\n11.14\n2.66\n6.84\n9.54\n10.32\n12.10\n22.66\n▇▇▂▁▁\n\n\nDuration\n0\n1\n25.42\n3.91\n16.30\n23.00\n24.70\n27.73\n37.42\n▁▇▆▂▁\n\n\nCar_perc\n0\n1\n59.00\n21.56\n0.00\n45.40\n62.62\n75.72\n99.27\n▁▃▆▇▅\n\n\nN_INDIVIDUOS\n0\n1\n24323.80\n16438.04\n1566.00\n11060.00\n20855.00\n36079.00\n68649.00\n▇▆▅▃▁\n\n\nMale_perc\n0\n1\n47.54\n1.57\n44.61\n46.58\n47.50\n48.29\n55.94\n▅▇▂▁▁\n\n\nIncomeHH\n0\n1\n1732.55\n453.11\n884.46\n1417.76\n1594.73\n1953.50\n3462.32\n▃▇▃▁▁\n\n\nNvehicles\n0\n1\n1.53\n0.24\n1.02\n1.35\n1.54\n1.67\n2.42\n▃▇▆▂▁\n\n\nDrivingLic\n0\n1\n62.50\n8.12\n37.04\n57.67\n63.00\n68.84\n80.79\n▁▂▇▇▂\n\n\nCarParkFree_Work\n0\n1\n49.30\n14.30\n5.47\n40.39\n50.51\n57.92\n87.60\n▁▃▇▇▁\n\n\nPTpass\n0\n1\n23.82\n12.87\n0.00\n13.41\n22.72\n32.94\n60.45\n▅▇▇▂▁\n\n\nArea_km2\n0\n1\n25.55\n42.58\n1.49\n5.04\n11.60\n28.51\n282.13\n▇▁▁▁▁",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#missing-data",
    "href": "eda.html#missing-data",
    "title": "5  Exploratory Data Analysis",
    "section": "5.5 Missing data",
    "text": "5.5 Missing data\nIs there missing data (NA)? How many?\n\ntable(is.na(data))\n\n\nFALSE \n 4720",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#outliers",
    "href": "eda.html#outliers",
    "title": "5  Exploratory Data Analysis",
    "section": "5.6 Outliers",
    "text": "5.6 Outliers\nInspect outliers and distributions\n\nboxplot(data) # This does now work if variables are not all continuous\n\n\ndata_continuous = data |&gt; select(-Origin_dicofre16, -internal, -Lisboa) # Exclude categorical variables\nboxplot(data_continuous) # Exclude categorical variables\n\n\n\n\n\n\n\nhist(data_continuous$Total) # histogram\n\n\n\n\n\n\n\nboxplot(data_continuous$Total) # outliers detected\n\n\n\n\n\n\n\n\n\nTreating outliers\nBased on the theory, we can create a function to detect the outliers.\n\noutlier = function(x) {\n  q = quantile(x, probs = c(0.25, 0.75), na.rm = TRUE)  # Q1 and Q3\n  caps = quantile(x, probs = c(0.05, 0.95), na.rm = TRUE) # 5th and 95th percentile\n  H = 1.5 * IQR(x, na.rm = TRUE) # interquartile range\n  \n  case_when(\n    x &lt; (q[1] - H) ~ caps[1], # replace values that are LESS than Q1-1.5*IQR with the P5 value\n    x &gt; (q[2] + H) ~ caps[2], # replace values that are MORE than Q3+1.5*IQR with the P95 value\n    TRUE ~ x # otherwise, return the original value\n  )\n}\n\nNow, use it in a copy of the table\n\ndata_outliers = data_continuous # duplicate the table\ndata_outliers$Total = outlier(data_outliers$Total) # Use the function to the same variable\n\nTake a look again at the boxplot\n\nboxplot(data_outliers$Total)\n\n\n\n\n\n\n\n\nCompare the variable with and without the outliers\n\n# Mean\nmean(data$Total)\n\n[1] 22457\n\nmean(data_outliers$Total)\n\n[1] 22013.51\n\n# Median\nmedian(data$Total)\n\n[1] 17474\n\nmedian(data_outliers$Total)\n\n[1] 17474\n\n# Standard deviation\nsd(data$Total)\n\n[1] 19084.45\n\nsd(data_outliers$Total)\n\n[1] 17739.39\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere are many methods to treat outliers. This is just one of them.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#histograms",
    "href": "eda.html#histograms",
    "title": "5  Exploratory Data Analysis",
    "section": "5.7 Histograms",
    "text": "5.7 Histograms\nPlot histograms of all the continuous variables\n\nplot_histogram(data, ncol = 3) # histograms with 3 columns\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot_density(data, ncol = 3) # density plots\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck how other variables are distributed regarding Car_perc\n\nplot_boxplot(data, by = \"Car_perc\", ncol = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen Car_perc increases, PTpass decreases.\n\n\n\n\n\n\n\n\nExercise\n\n\n\nTry plotting the boxplots of each independent variable with Car_perc.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#correlations",
    "href": "eda.html#correlations",
    "title": "5  Exploratory Data Analysis",
    "section": "5.8 Correlations",
    "text": "5.8 Correlations\nPlot correlation heatmaps, between continuous variables.\n\n# estimate correlation matrix\ncorrmat = cor(data_continuous, method = \"pearson\") |&gt; round(2)\ncorrmat\n\n                 Total  Walk  Bike   Car PTransit Other Distance Duration\nTotal             1.00  0.49  0.44  0.93     0.78  0.48    -0.36     0.10\nWalk              0.49  1.00  0.27  0.21     0.09 -0.01    -0.34     0.03\nBike              0.44  0.27  1.00  0.41     0.26  0.11     0.00     0.08\nCar               0.93  0.21  0.41  1.00     0.70  0.47    -0.29    -0.03\nPTransit          0.78  0.09  0.26  0.70     1.00  0.58    -0.20     0.36\nOther             0.48 -0.01  0.11  0.47     0.58  1.00    -0.09     0.22\nDistance         -0.36 -0.34  0.00 -0.29    -0.20 -0.09     1.00     0.16\nDuration          0.10  0.03  0.08 -0.03     0.36  0.22     0.16     1.00\nCar_perc          0.01 -0.53  0.00  0.27     0.01  0.09     0.26    -0.40\nN_INDIVIDUOS      0.72  0.56  0.29  0.68     0.28  0.20    -0.46    -0.11\nMale_perc        -0.38 -0.32 -0.14 -0.33    -0.20 -0.08     0.39    -0.01\nIncomeHH          0.25  0.04  0.22  0.26     0.23  0.10    -0.09     0.30\nNvehicles        -0.16 -0.25  0.00 -0.02    -0.21 -0.14     0.27    -0.26\nDrivingLic       -0.01 -0.12  0.08  0.08    -0.10 -0.09     0.23    -0.12\nCarParkFree_Work -0.26 -0.29 -0.03 -0.08    -0.38 -0.25     0.30    -0.53\nPTpass            0.25  0.23  0.01  0.09     0.39  0.21    -0.36     0.55\nArea_km2         -0.15 -0.15  0.03 -0.08    -0.19 -0.09     0.23    -0.25\n                 Car_perc N_INDIVIDUOS Male_perc IncomeHH Nvehicles DrivingLic\nTotal                0.01         0.72     -0.38     0.25     -0.16      -0.01\nWalk                -0.53         0.56     -0.32     0.04     -0.25      -0.12\nBike                 0.00         0.29     -0.14     0.22      0.00       0.08\nCar                  0.27         0.68     -0.33     0.26     -0.02       0.08\nPTransit             0.01         0.28     -0.20     0.23     -0.21      -0.10\nOther                0.09         0.20     -0.08     0.10     -0.14      -0.09\nDistance             0.26        -0.46      0.39    -0.09      0.27       0.23\nDuration            -0.40        -0.11     -0.01     0.30     -0.26      -0.12\nCar_perc             1.00        -0.09      0.18    -0.07      0.40       0.33\nN_INDIVIDUOS        -0.09         1.00     -0.46     0.14     -0.14       0.00\nMale_perc            0.18        -0.46      1.00    -0.34      0.19      -0.11\nIncomeHH            -0.07         0.14     -0.34     1.00      0.23       0.22\nNvehicles            0.40        -0.14      0.19     0.23      1.00       0.44\nDrivingLic           0.33         0.00     -0.11     0.22      0.44       1.00\nCarParkFree_Work     0.53        -0.19      0.26    -0.01      0.54       0.39\nPTpass              -0.52         0.20     -0.34     0.16     -0.44      -0.45\nArea_km2             0.25        -0.19      0.30    -0.18      0.12       0.10\n                 CarParkFree_Work PTpass Area_km2\nTotal                       -0.26   0.25    -0.15\nWalk                        -0.29   0.23    -0.15\nBike                        -0.03   0.01     0.03\nCar                         -0.08   0.09    -0.08\nPTransit                    -0.38   0.39    -0.19\nOther                       -0.25   0.21    -0.09\nDistance                     0.30  -0.36     0.23\nDuration                    -0.53   0.55    -0.25\nCar_perc                     0.53  -0.52     0.25\nN_INDIVIDUOS                -0.19   0.20    -0.19\nMale_perc                    0.26  -0.34     0.30\nIncomeHH                    -0.01   0.16    -0.18\nNvehicles                    0.54  -0.44     0.12\nDrivingLic                   0.39  -0.45     0.10\nCarParkFree_Work             1.00  -0.66     0.32\nPTpass                      -0.66   1.00    -0.41\nArea_km2                     0.32  -0.41     1.00\n\n# store the results so you can call the p-value at the corrplot\nres = cor.mtest(data_continuous, conf.level = .95) \n\ncorrplot(\n  corrmat,\n  method = \"color\", # or \"circle\"\n  p.mat = res$p,\n  sig.level = 0.05,\n  type = \"upper\", # display only the upper triangular\n  # order = \"hclust\", # order by hierarchical clustering\n  tl.col = \"black\" # text label color\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe pairwise correlations that are crossed are statistically not significant. The null hypothesis (H0) is that correlation is zero. The alternative hypothesis (HA) is that correlation is not zero.\nThis means that the correlations are only significant when you reject the null hypothesis (p-value &lt; 0.05).\n\n\nOther method, using the DataExplorer package\n\nplot_correlation(data_continuous)\n\n\n\n\n\n\n\n\nCheck the p-value of a crossed pair correlation:\n\ncor.test(data$IncomeHH, data$Bike)\n\n\n    Pearson's product-moment correlation\n\ndata:  data$IncomeHH and data$Bike\nt = 3.5261, df = 234, p-value = 0.0005074\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.09977779 0.34249284\nsample estimates:\n      cor \n0.2246163 \n\ncor.test(data$IncomeHH, data$Duration)\n\n\n    Pearson's product-moment correlation\n\ndata:  data$IncomeHH and data$Duration\nt = 4.817, df = 234, p-value = 2.62e-06\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1795420 0.4122446\nsample estimates:\n     cor \n0.300356 \n\ncor.test(data$Distance, data$Duration)\n\n\n    Pearson's product-moment correlation\n\ndata:  data$Distance and data$Duration\nt = 2.4605, df = 234, p-value = 0.0146\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.0317533 0.2808153\nsample estimates:\n      cor \n0.1588098 \n\n\n\n\n\n\n\n\nThe default for cor.test is Pearson, two-sided, with a 95% confident level. Check ?cor.test for more options.\n\n\n\n\n\n\n\n\n\nINE. 2018. “Mobilidade e Funcionalidade Do Território Nas Áreas Metropolitanas do Porto e de Lisboa: 2017.” Lisboa: Instituto National de Estatística. https://www.ine.pt/xportal/xmain?xpid=INE&xpgid=ine_publicacoes&PUBLICACOESpub_boui=349495406&PUBLICACOESmodo=2&xlang=pt.\n\n\n———. 2022. “Censos 2021- XVI Recenseamento Geral da População. VI Recenseamento Geral da Habitação.” Lisboa: Instituto National de Estatística. https://censos.ine.pt/xurl/pub/65586079.\n\n\nWickham, Hadley, Mine Çetinaka-Rundel, and Garrett Grolemund. 2017. R for Data Science. 2nd ed. O’Reilly Sebastopol. https://r4ds.hadley.nz/.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "linear-regression.html",
    "href": "linear-regression.html",
    "title": "6  Multiple Linear Regression",
    "section": "",
    "text": "6.1 Load packages\nlibrary(tidyverse) # Pack of most used libraries for data science\nlibrary(skimr) # summary of the data\nlibrary(DataExplorer) # exploratory data analysis\nlibrary(corrplot) # correlation plots\n\nlibrary(car) # Testing autocorrelation (Durbin Watson)\nlibrary(olsrr) # Testing multicollinearity (VIF, TOL, etc.)",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#dataset",
    "href": "linear-regression.html#dataset",
    "title": "6  Multiple Linear Regression",
    "section": "6.2 Dataset",
    "text": "6.2 Dataset\nThe database used in this example is a treated database from the Mobility Survey for the metropolitan areas of Lisbon in 2018 (INE 2018).\nIncluded variables:\n\nOrigin_dicofre16 - Code of Freguesia (district) as set by INE after 2016 (Distrito + Concelho + Freguesia), for trip origin\nTotal - number of trips with origin at each district\nWalk - number of walking trips\nBike - number of bike trips\nCar - number of car trips. Includes taxi and motorcycle.\nPTransit - number of Public Transit trips\nOther - number of other trips (truck, van, tractor, aviation)\nDistance - average trip distance (km)\nDuration - average trip duration (minutes)\nCar_perc - percentage of car trips\nN_INDIVIDUOS - number of residents (INE 2022)\nMale_perc - percentage of male residents (INE 2022)\nIncomeHH - average household income\nNvehicles - average number of car/motorcycle vehicles in the household\nDrivingLic - percentage of car driving licence holders\nCarParkFree_Work - percentage of respondents with free car parking at the work location\nPTpass - percentage of public transit monthly pass holders\ninternal - binary variable (factor). Yes: trip with same TAZ origin and destination, No: trips with different destination\nLisboa - binary variable (factor). Yes: the district is part of Lisbon municipality, No: otherwise\nArea_km2 - area of in Origin_dicofre16, in km2\n\n\nImport dataset\n\ndata = readRDS(\"../data/IMOBmodel.Rds\")\ndata_continuous = data |&gt; select(-Origin_dicofre16, -internal, -Lisboa) # Exclude categorical variables\n\nShow summary statistics\n\nskim(data)\n\n\nData summary\n\n\nName\ndata\n\n\nNumber of rows\n236\n\n\nNumber of columns\n20\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n2\n\n\nnumeric\n17\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nOrigin_dicofre16\n0\n1\n6\n6\n0\n118\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ninternal\n0\n1\nFALSE\n2\nYes: 118, No: 118\n\n\nLisboa\n0\n1\nFALSE\n2\nNo: 188, Yes: 48\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nTotal\n0\n1\n22457.00\n19084.45\n361.00\n5917.75\n17474.00\n33377.50\n112186.00\n▇▃▂▁▁\n\n\nWalk\n0\n1\n5383.44\n6224.84\n0.00\n763.25\n3125.00\n8298.50\n32646.00\n▇▂▁▁▁\n\n\nBike\n0\n1\n107.03\n248.65\n0.00\n0.00\n13.50\n98.75\n2040.00\n▇▁▁▁▁\n\n\nCar\n0\n1\n13289.24\n12351.61\n0.00\n3243.00\n9008.00\n21248.75\n52631.00\n▇▃▂▁▁\n\n\nPTransit\n0\n1\n3473.79\n5467.82\n0.00\n249.00\n1057.00\n4853.00\n41672.00\n▇▁▁▁▁\n\n\nOther\n0\n1\n203.45\n336.04\n0.00\n2.00\n44.00\n281.50\n2391.00\n▇▁▁▁▁\n\n\nDistance\n0\n1\n11.14\n2.66\n6.84\n9.54\n10.32\n12.10\n22.66\n▇▇▂▁▁\n\n\nDuration\n0\n1\n25.42\n3.91\n16.30\n23.00\n24.70\n27.73\n37.42\n▁▇▆▂▁\n\n\nCar_perc\n0\n1\n59.00\n21.56\n0.00\n45.40\n62.62\n75.72\n99.27\n▁▃▆▇▅\n\n\nN_INDIVIDUOS\n0\n1\n24323.80\n16438.04\n1566.00\n11060.00\n20855.00\n36079.00\n68649.00\n▇▆▅▃▁\n\n\nMale_perc\n0\n1\n47.54\n1.57\n44.61\n46.58\n47.50\n48.29\n55.94\n▅▇▂▁▁\n\n\nIncomeHH\n0\n1\n1732.55\n453.11\n884.46\n1417.76\n1594.73\n1953.50\n3462.32\n▃▇▃▁▁\n\n\nNvehicles\n0\n1\n1.53\n0.24\n1.02\n1.35\n1.54\n1.67\n2.42\n▃▇▆▂▁\n\n\nDrivingLic\n0\n1\n62.50\n8.12\n37.04\n57.67\n63.00\n68.84\n80.79\n▁▂▇▇▂\n\n\nCarParkFree_Work\n0\n1\n49.30\n14.30\n5.47\n40.39\n50.51\n57.92\n87.60\n▁▃▇▇▁\n\n\nPTpass\n0\n1\n23.82\n12.87\n0.00\n13.41\n22.72\n32.94\n60.45\n▅▇▇▂▁\n\n\nArea_km2\n0\n1\n25.55\n42.58\n1.49\n5.04\n11.60\n28.51\n282.13\n▇▁▁▁▁\n\n\n\n\n\nThe dependent variable is continuous.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#check-the-assumptions",
    "href": "linear-regression.html#check-the-assumptions",
    "title": "6  Multiple Linear Regression",
    "section": "6.3 Check the assumptions",
    "text": "6.3 Check the assumptions\nBefore running the model, you need to check if the assumptions are met.\n\nThe dependent variable is normally distributed\nLinear relationship between the dependent variable and the independent variables\nNo multicollinearity between independent variables (or only very little)\nThe observations are independent\nConstant Variance (Assumption of Homoscedasticity)\nResiduals are normally distributed",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#assumption-1-normal-distribution",
    "href": "linear-regression.html#assumption-1-normal-distribution",
    "title": "6  Multiple Linear Regression",
    "section": "6.4 Assumption 1: Normal distribution",
    "text": "6.4 Assumption 1: Normal distribution\nThe Dependent Variable is be normally distributed.\nCheck the histogram of Car_perc:\n\nhist(data$Car_perc)\n\n\n\n\n\n\n\n\nIf the sample is small (&lt; 50 observations), we use Shapiro-Wilk test:\n\nshapiro.test(data$Car_perc)\n\n\n    Shapiro-Wilk normality test\n\ndata:  data$Car_perc\nW = 0.97284, p-value = 0.0001709\n\n\nIf not, use the Kolmogorov-Smirnov test:\n\nks.test(\n  data$Car_perc,\n  \"pnorm\",\n  mean = mean(data$Car_perc),\n  sd = sd(data$Car_perc)\n)\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  data$Car_perc\nD = 0.072477, p-value = 0.1675\nalternative hypothesis: two-sided\n\n\nThe null hypothesis for both tests is that the distribution is normal. Therefore, for the distribution to be normal, the pvalue must be &gt; 0.05 and the null hypothesis is not rejected. From the output obtained we can assume normality.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#assumption-2-linear-relationship",
    "href": "linear-regression.html#assumption-2-linear-relationship",
    "title": "6  Multiple Linear Regression",
    "section": "6.5 Assumption 2: Linear relationship",
    "text": "6.5 Assumption 2: Linear relationship\nThere is a linear relationship between dependent variable (DV) and independent variables (IV).\nWe can check this assumption by plotting scatterplots of the DV against each IV:\n\nplot(x = data$Car_perc, y = data$Total, xlab = \"Car_perc (%)\", ylab = \"Total (number of trips)\")  \nplot(x = data$Car_perc, y = data$Walk, xlab = \"Car_perc\", ylab = \"Walk\")  \nplot(x = data$Car_perc, y = data$Bike, xlab = \"Car_perc\", ylab = \"Bike\")  \nplot(x = data$Car_perc, y = data$Car, xlab = \"Car_perc\", ylab = \"Car\")  \nplot(x = data$Car_perc, y = data$PTransit, xlab = \"Car_perc\", ylab = \"PTransit\")\nplot(x = data$Car_perc, y = data$Other, xlab = \"Car_perc\", ylab = \"Other\")\nplot(x = data$Car_perc, y = data$Distance, xlab = \"Car_perc\", ylab = \"Distance\")\nplot(x = data$Car_perc, y = data$Duration, xlab = \"Car_perc\", ylab = \"Duration\")\nplot(x = data$Car_perc, y = data$N_INDIVIDUOS, xlab = \"Car_perc\", ylab = \"N_INDIVIDUOS\")\nplot(x = data$Car_perc, y = data$Male_perc, xlab = \"Car_perc\", ylab = \"Male_perc\")\nplot(x = data$Car_perc, y = data$IncomeHH, xlab = \"Car_perc\", ylab = \"IncomeHH\")\nplot(x = data$Car_perc, y = data$Nvehicles, xlab = \"Car_perc\", ylab = \"Nvehicles\")\nplot(x = data$Car_perc, y = data$DrivingLic, xlab = \"Car_perc\", ylab = \"Driving License\")\nplot(x = data$Car_perc, y = data$CarParkFree_Work, xlab = \"Car_perc\", ylab = \"Free car parking at work\")\nplot(x = data$Car_perc, y = data$PTpass, xlab = \"Car_perc\", ylab = \"PTpass\")\nplot(x = data$Car_perc, y = data$internal, xlab = \"Car_perc\", ylab = \"internal trips\")\nplot(x = data$Car_perc, y = data$Lisboa, xlab = \"Car_perc\", ylab = \"Lisboa\")\nplot(x = data$Car_perc, y = data$Area_km2, xlab = \"Car_perc\", ylab = \"Area_km2\")\n\nOr you can make a pairwise scatterplot matrix, that compares every variable with each other:\n\n# pairs(data_continuous, pch = 19, lower.panel = NULL) # we have too many variables, let's split the plots\npairs(data_continuous[,1:6], pch = 19, lower.panel = NULL)\n\n\n\n\n\n\n\npairs(data_continuous[,7:12], pch = 19, lower.panel = NULL)\n\n\n\n\n\n\n\npairs(data_continuous[,13:17], pch = 19, lower.panel = NULL)",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#assumption-3-no-multicollinearity",
    "href": "linear-regression.html#assumption-3-no-multicollinearity",
    "title": "6  Multiple Linear Regression",
    "section": "6.6 Assumption 3: No multicollinearity",
    "text": "6.6 Assumption 3: No multicollinearity\nCheck the correlation plot before choosing the variables.\n\nDeclare the model\n\n\n\n\n\n\nUse CTRL + SHIFT + C to comment/uncomment lines (variables)\n\n\n\n\n# names(data) # to see the names of the variables\n\nmodel = lm(\n  Car_perc ~\n    Total +\n    Walk +\n    Bike +\n    Car +\n    PTransit +\n    Other +\n    Distance + \n    Duration + \n    N_INDIVIDUOS + \n    Male_perc + \n    IncomeHH + \n    Nvehicles + \n    DrivingLic + \n    CarParkFree_Work + \n    PTpass + \n    internal + \n    Lisboa + \n    Area_km2,\n  data = data\n)\n\nsummary(model)\n\n\nCall:\nlm(formula = Car_perc ~ Total + Walk + Bike + Car + PTransit + \n    Other + Distance + Duration + N_INDIVIDUOS + Male_perc + \n    IncomeHH + Nvehicles + DrivingLic + CarParkFree_Work + PTpass + \n    internal + Lisboa + Area_km2, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-37.952  -4.907  -0.232   5.305  39.157 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       4.047e+00  3.192e+01   0.127  0.89923    \nTotal             1.633e-01  3.671e-01   0.445  0.65700    \nWalk             -1.640e-01  3.671e-01  -0.447  0.65552    \nBike             -1.614e-01  3.669e-01  -0.440  0.66044    \nCar              -1.627e-01  3.671e-01  -0.443  0.65806    \nPTransit         -1.640e-01  3.672e-01  -0.447  0.65552    \nOther            -1.622e-01  3.671e-01  -0.442  0.65897    \nDistance          6.004e-01  3.629e-01   1.654  0.09948 .  \nDuration         -3.244e-01  3.325e-01  -0.976  0.33032    \nN_INDIVIDUOS     -6.627e-05  8.876e-05  -0.747  0.45612    \nMale_perc         2.723e-01  6.151e-01   0.443  0.65849    \nIncomeHH         -5.218e-04  2.199e-03  -0.237  0.81264    \nNvehicles         7.301e+00  3.959e+00   1.844  0.06650 .  \nDrivingLic        2.910e-01  1.161e-01   2.506  0.01294 *  \nCarParkFree_Work  1.943e-01  7.882e-02   2.465  0.01449 *  \nPTpass           -8.254e-02  9.989e-02  -0.826  0.40956    \ninternalNo        1.962e+01  2.136e+00   9.182  &lt; 2e-16 ***\nLisboaYes        -9.647e+00  3.182e+00  -3.031  0.00273 ** \nArea_km2          1.689e-02  1.860e-02   0.908  0.36486    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.59 on 217 degrees of freedom\nMultiple R-squared:  0.7772,    Adjusted R-squared:  0.7587 \nF-statistic: 42.06 on 18 and 217 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nAssessing the model\n\nFirst check the pvalue and the F statistics of the model to see if there is any statistical relation between the dependent variable and the independent variables. If pvalue &lt; 0.05 and the F statistics &gt; Fcritical = 2.39, then the model is statistically acceptable.\nThe R-square and Adjusted R-square evaluate the amount of variance that is explained by the model. The difference between one and another is that the R-square does not consider the number of variables. If you increase the number of variables in the model, the R-square will tend to increase which can lead to overfitting. On the other hand, the Adjusted R-square adjust to the number of independent variables.\nTake a look at the t-value and the Pr(&gt;|t|). If the t-value &gt; 1.96 or Pr(&gt;|t|) &lt; 0.05, then the IV is statistically significant to the model.\nTo analyse the estimates of the variables, you should first check the signal and assess if the independent variable has a direct or inverse relationship with the dependent variable. It is only possible to evaluate the magnitude of the estimate if all variables are continuous and standardized or by calculating the elasticities. Do not forget to access the Intercept…\n\nWe can see from the output that the R-squared value for the model (with ALL variables) is 0.7772. We can also see that the overall F-statistic is 42.06 and the corresponding p-value is &lt;2.2e-16, which indicates that the overall regression model is significant. Also, the predictor variables DrivingLic and CarParkFree_Work, internal(No) and Lisboa(Yes) are statistically significant at the 0.05 significance level.\n\n\n\n\n\n\nYour turn\n\n\n\nNow try to remove some variables from the model and assess it again. Elaborate a justification to exclude those variables.\n\n\n\n\nCalculate the Variance Inflation Factor (VIF)\nWe use the vif() function from the car package to calculate the VIF for each predictor variable in the model:\n\ncar::vif(model)\n\n           Total             Walk             Bike              Car \n    1.028324e+08     1.094004e+07     1.743544e+04     4.307486e+07 \n        PTransit            Other         Distance         Duration \n    8.443136e+06     3.186972e+04     1.957062e+00     3.544018e+00 \n    N_INDIVIDUOS        Male_perc         IncomeHH        Nvehicles \n    4.459671e+00     1.961472e+00     2.079348e+00     1.882275e+00 \n      DrivingLic CarParkFree_Work           PTpass         internal \n    1.863227e+00     2.660375e+00     3.464699e+00     2.400178e+00 \n          Lisboa         Area_km2 \n    3.451852e+00     1.313708e+00 \n\n\nA common rule of thumb is that a VIF value greater than 5 indicates a high level of multicollinearity among the predictor variables, which is potentially concerning.\n\n\nCondition index\nCondition index is another diagnostic tool for detecting multicollinearity, with values above 10 indicating moderate multicollinearity and values above 30 indicating severe multicollinearity. You will learn how to to that in the next class: Factor analysis.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#assumption-4-independence-of-observations",
    "href": "linear-regression.html#assumption-4-independence-of-observations",
    "title": "6  Multiple Linear Regression",
    "section": "6.7 Assumption 4: Independence of observations",
    "text": "6.7 Assumption 4: Independence of observations\nMultiple linear regression assumes that each observation in the dataset is independent.\nThe error (E) is independent across observations and the error variance is constant across IV\nThe simplest way to determine if this assumption is met is to perform a Durbin-Watson test, which is a formal statistical test that tells us whether or not the residuals (and thus the observations) exhibit autocorrelation.\n\ndurbinWatsonTest(model)\n\n lag Autocorrelation D-W Statistic p-value\n   1      0.07881007      1.840798   0.126\n Alternative hypothesis: rho != 0\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the Durbin-Watson test, values of the D-W Statistic vary from 0 to 4.\nIf the values are from 1.8 to 2.2 this means that there is no autocorrelation in the model.\n\n\nH0 (null hypothesis): There is no correlation among the residuals. Since p-value &gt; 0.05, we do not reject the null hypothesis and we can not discard that there is autocorrelation in the model.\nIf the observations are not independent, you may consider to treat the observarions as a panel data.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#assumption-5-constant-variance-homoscedasticity",
    "href": "linear-regression.html#assumption-5-constant-variance-homoscedasticity",
    "title": "6  Multiple Linear Regression",
    "section": "6.8 Assumption 5: Constant Variance (Homoscedasticity)",
    "text": "6.8 Assumption 5: Constant Variance (Homoscedasticity)\nThe simplest way to determine if this assumption is met is to create a plot of standardized residuals versus predicted values.\n\nplot(model)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the Residuals, check the following assumptions:\n\nResiduals vs Fitted: This plot is used to detect non-linearity, heteroscedasticity, and outliers.\nNormal Q-Q: The quantile-quantile (Q-Q) plot is used to check if the disturbances follow a normal distribution\nScale-Location: This plot is used to verify if the residuals are spread equally (homoscedasticity) or not (heteroscedasticity) through the sample.\nResiduals vs Leverage: This plot is used to detect the impact of the outliers in the model. If the outliers are outside the Cook-distance, this may lead to serious problems in the model.\n\nTry analyzing the plots and check if the model meets the assumptions.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#assumption-6-residuals-are-normally-distributed",
    "href": "linear-regression.html#assumption-6-residuals-are-normally-distributed",
    "title": "6  Multiple Linear Regression",
    "section": "6.9 Assumption 6: Residuals are normally distributed",
    "text": "6.9 Assumption 6: Residuals are normally distributed\nAssess the Q-Q Residuals plot above. When the residuals clearly depart from a straight diagonal line, it indicates that they do not follow a normal distribution.\nUse a formal statistical test like Shapiro-Wilk or Kolmogorov-Smironov to validate those results.\n\n\n\n\n\n\nINE. 2018. “Mobilidade e Funcionalidade Do Território Nas Áreas Metropolitanas do Porto e de Lisboa: 2017.” Lisboa: Instituto National de Estatística. https://www.ine.pt/xportal/xmain?xpid=INE&xpgid=ine_publicacoes&PUBLICACOESpub_boui=349495406&PUBLICACOESmodo=2&xlang=pt.\n\n\n———. 2022. “Censos 2021- XVI Recenseamento Geral da População. VI Recenseamento Geral da Habitação.” Lisboa: Instituto National de Estatística. https://censos.ine.pt/xurl/pub/65586079.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "factor-analysis.html",
    "href": "factor-analysis.html",
    "title": "7  Factor Analysis",
    "section": "",
    "text": "7.1 Load packages\nThis exercise is based on the paper “Residential location satisfaction in the Lisbon metropolitan area”, by Martinez, Abreu e Silva, and Viegas (2010) .\nThe aim of this study was to examine the perception of households towards their residential location considering land use and accessibility factors, as well as household socio-economic and attitudinal characteristics.\nlibrary(tidyverse) # Pack of most used libraries for data science\nlibrary(summarytools) # Summary of the dataset\nlibrary(foreign) # Read SPSS files\nlibrary(nFactors) # Factor analysis\nlibrary(GPArotation) # GPA Rotation for Factor Analysis\nlibrary(psych) # Personality, psychometric, and psychological research",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Factor Analysis</span>"
    ]
  },
  {
    "objectID": "factor-analysis.html#dataset",
    "href": "factor-analysis.html#dataset",
    "title": "7  Factor Analysis",
    "section": "7.2 Dataset",
    "text": "7.2 Dataset\nIncluded variables:\n\nRespondentID - ID of the respondent\nDWELCLAS - Classification of the dwelling\nINCOME - Income of the household\nCHILD13 - Number of children under 13 years old\nH18 - Number of household members over 18 years old\nHEMPLOY - Number of household members employed\nHSIZE - Household size\nIAGE - Age of the respondent\nISEX - Sex of the respondent\nNCARS - Number of cars in the household\nAREA - Area of the dwelling\nBEDROOM - Number of bedrooms in the dwelling\nPARK - Number of parking spaces in the dwelling\nBEDSIZE - BEDROOM/HSIZE\nPARKSIZE - PARK/NCARS\nRAGE10 - 1 if Dwelling age &lt;= 10\nTCBD - Private car distance in time to CBD\nDISTTC - Euclidean distance to heavy public transport system stops\nTWCBD - Private car distance in time of workplace to CBD\nTDWWK - Private car distance in time of dwelling to work place\nHEADH - 1 if Head of the Household\nPOPDENS - Population density per hectare\nEQUINDEX - Number of undergraduate students/Population over 20 years old (500m)\n\n\n7.2.1 Import dataset\n\ndata = read.spss(\"../data/example_fact.sav\", to.data.frame = T)\n\n\n\n7.2.2 Get to know your dataset\nTake a look at the first values of the dataset\n\nhead(data, 5)\n\n  RespondentID DWELCLAS INCOME CHILD13 H18 HEMPLOY HSIZE AVADUAGE IAGE ISEX\n1    799161661        5   7500       1   2       2     3 32.00000   32    1\n2    798399409        6   4750       0   1       1     1 31.00000   31    1\n3    798374392        6   4750       2   2       2     4 41.50000   42    0\n4    798275277        5   7500       0   3       2     4 44.66667   52    1\n5    798264250        6   2750       1   1       1     2 33.00000   33    0\n  NCARS AREA BEDROOM PARK   BEDSIZE PARKSIZE RAGE10      TCBD   DISTHTC\n1     2  100       2    1 0.6666667      0.5      1 36.791237  629.1120\n2     1   90       2    1 2.0000000      1.0      0 15.472989  550.5769\n3     2  220       4    2 1.0000000      1.0      1 24.098125  547.8633\n4     3  120       3    0 0.7500000      0.0      0 28.724796 2350.5782\n5     1   90       2    0 1.0000000      0.0      0  7.283384  698.3000\n      TWCBD    TDWWK HEADH   POPDENS   EDUINDEX   GRAVCPC   GRAVCPT  GRAVPCPT\n1 10.003945 31.14282     1  85.70155 0.06406279 0.2492962 0.2492607 1.0001423\n2 15.502989  0.00000     1 146.43494 0.26723192 0.3293831 0.3102800 1.0615674\n3 12.709374 20.38427     1 106.60810 0.09996816 0.2396229 0.2899865 0.8263245\n4  3.168599 32.94246     1  36.78380 0.08671065 0.2734539 0.2487830 1.0991661\n5  5.364160 13.04013     1 181.62720 0.13091674 0.2854017 0.2913676 0.9795244\n  NSTRTC    DISTHW    DIVIDX    ACTDENS   DISTCBD\n1     38 2036.4661 0.3225354  0.6722406  9776.142\n2     34  747.7683 0.3484588  2.4860345  3523.994\n3     33 2279.0577 0.3237884  1.6249059 11036.407\n4      6 1196.4665 0.3272149  1.7664923  6257.262\n5     31 3507.2402 0.3545181 11.3249309  1265.239\n\nView(data) # open in table\n\nMake the RespondentID variable as row names or case number\n\ndata = data |&gt; column_to_rownames(var = \"RespondentID\")\n\nTake a look at the main characteristics of the dataset\n\nstr(data)\n\n'data.frame':   470 obs. of  31 variables:\n $ DWELCLAS: num  5 6 6 5 6 6 4 2 6 5 ...\n $ INCOME  : num  7500 4750 4750 7500 2750 1500 12500 1500 1500 1500 ...\n $ CHILD13 : num  1 0 2 0 1 0 0 0 0 0 ...\n $ H18     : num  2 1 2 3 1 3 3 4 1 1 ...\n $ HEMPLOY : num  2 1 2 2 1 2 0 2 1 1 ...\n $ HSIZE   : num  3 1 4 4 2 3 3 4 1 1 ...\n $ AVADUAGE: num  32 31 41.5 44.7 33 ...\n $ IAGE    : num  32 31 42 52 33 47 62 21 34 25 ...\n $ ISEX    : num  1 1 0 1 0 1 1 0 0 0 ...\n $ NCARS   : num  2 1 2 3 1 1 2 3 1 1 ...\n $ AREA    : num  100 90 220 120 90 100 178 180 80 50 ...\n $ BEDROOM : num  2 2 4 3 2 2 5 3 2 1 ...\n $ PARK    : num  1 1 2 0 0 0 2 0 0 1 ...\n $ BEDSIZE : num  0.667 2 1 0.75 1 ...\n $ PARKSIZE: num  0.5 1 1 0 0 0 1 0 0 1 ...\n $ RAGE10  : num  1 0 1 0 0 0 0 0 1 1 ...\n $ TCBD    : num  36.79 15.47 24.1 28.72 7.28 ...\n $ DISTHTC : num  629 551 548 2351 698 ...\n $ TWCBD   : num  10 15.5 12.71 3.17 5.36 ...\n $ TDWWK   : num  31.1 0 20.4 32.9 13 ...\n $ HEADH   : num  1 1 1 1 1 1 1 0 1 1 ...\n $ POPDENS : num  85.7 146.4 106.6 36.8 181.6 ...\n $ EDUINDEX: num  0.0641 0.2672 0.1 0.0867 0.1309 ...\n $ GRAVCPC : num  0.249 0.329 0.24 0.273 0.285 ...\n $ GRAVCPT : num  0.249 0.31 0.29 0.249 0.291 ...\n $ GRAVPCPT: num  1 1.062 0.826 1.099 0.98 ...\n $ NSTRTC  : num  38 34 33 6 31 45 12 6 4 22 ...\n $ DISTHW  : num  2036 748 2279 1196 3507 ...\n $ DIVIDX  : num  0.323 0.348 0.324 0.327 0.355 ...\n $ ACTDENS : num  0.672 2.486 1.625 1.766 11.325 ...\n $ DISTCBD : num  9776 3524 11036 6257 1265 ...\n - attr(*, \"variable.labels\")= Named chr(0) \n  ..- attr(*, \"names\")= chr(0) \n - attr(*, \"codepage\")= int 1252\n\n\nCheck summary statistics of variables\n\n# skimr::skim(data)\nprint(dfSummary(data),\n      method = \"render\")\n\n\nData Frame Summary\ndata\nDimensions: 470 x 31\n  Duplicates: 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo\nVariable\nStats / Values\nFreqs (% of Valid)\nGraph\nValid\nMissing\n\n\n\n\n1\nDWELCLAS [numeric]\n\n\n\nMean (sd) : 5.1 (1.3)\n\n\nmin ≤ med ≤ max:\n\n\n1 ≤ 5 ≤ 7\n\n\nIQR (CV) : 2 (0.2)\n\n\n\n\n\n\n1\n:\n5\n(\n1.1%\n)\n\n\n2\n:\n14\n(\n3.0%\n)\n\n\n3\n:\n31\n(\n6.6%\n)\n\n\n4\n:\n75\n(\n16.0%\n)\n\n\n5\n:\n130\n(\n27.7%\n)\n\n\n6\n:\n162\n(\n34.5%\n)\n\n\n7\n:\n53\n(\n11.3%\n)\n\n\n\n\n470 (100.0%)\n0 (0.0%)\n\n\n2\nINCOME [numeric]\n\n\n\nMean (sd) : 4259.6 (3001.8)\n\n\nmin ≤ med ≤ max:\n\n\n700 ≤ 2750 ≤ 12500\n\n\nIQR (CV) : 2000 (0.7)\n\n\n\n\n\n\n700\n:\n20\n(\n4.3%\n)\n\n\n1500\n:\n96\n(\n20.4%\n)\n\n\n2750\n:\n142\n(\n30.2%\n)\n\n\n4750\n:\n106\n(\n22.6%\n)\n\n\n7500\n:\n75\n(\n16.0%\n)\n\n\n12500\n:\n31\n(\n6.6%\n)\n\n\n\n\n470 (100.0%)\n0 (0.0%)\n\n\n3\nCHILD13 [numeric]\n\n\n\nMean (sd) : 0.4 (0.8)\n\n\nmin ≤ med ≤ max:\n\n\n0 ≤ 0 ≤ 4\n\n\nIQR (CV) : 0 (2)\n\n\n\n\n\n\n0\n:\n353\n(\n75.1%\n)\n\n\n1\n:\n62\n(\n13.2%\n)\n\n\n2\n:\n41\n(\n8.7%\n)\n\n\n3\n:\n13\n(\n2.8%\n)\n\n\n4\n:\n1\n(\n0.2%\n)\n\n\n\n\n470 (100.0%)\n0 (0.0%)\n\n\n4\nH18 [numeric]\n\n\n\nMean (sd) : 2.1 (0.9)\n\n\nmin ≤ med ≤ max:\n\n\n0 ≤ 2 ≤ 6\n\n\nIQR (CV) : 0.8 (0.4)\n\n\n\n\n\n\n0\n:\n1\n(\n0.2%\n)\n\n\n1\n:\n112\n(\n23.8%\n)\n\n\n2\n:\n239\n(\n50.9%\n)\n\n\n3\n:\n77\n(\n16.4%\n)\n\n\n4\n:\n35\n(\n7.4%\n)\n\n\n5\n:\n3\n(\n0.6%\n)\n\n\n6\n:\n3\n(\n0.6%\n)\n\n\n\n\n470 (100.0%)\n0 (0.0%)\n\n\n5\nHEMPLOY [numeric]\n\n\n\nMean (sd) : 1.5 (0.7)\n\n\nmin ≤ med ≤ max:\n\n\n0 ≤ 2 ≤ 5\n\n\nIQR (CV) : 1 (0.5)\n\n\n\n\n\n\n0\n:\n39\n(\n8.3%\n)\n\n\n1\n:\n171\n(\n36.4%\n)\n\n\n2\n:\n237\n(\n50.4%\n)\n\n\n3\n:\n21\n(\n4.5%\n)\n\n\n4\n:\n1\n(\n0.2%\n)\n\n\n5\n:\n1\n(\n0.2%\n)\n\n\n\n\n470 (100.0%)\n0 (0.0%)\n\n\n6\nHSIZE [numeric]\n\n\n\nMean (sd) : 2.6 (1.3)\n\n\nmin ≤ med ≤ max:\n\n\n1 ≤ 2 ≤ 7\n\n\nIQR (CV) : 2 (0.5)\n\n\n\n\n\n\n1\n:\n104\n(\n22.1%\n)\n\n\n2\n:\n147\n(\n31.3%\n)\n\n\n3\n:\n96\n(\n20.4%\n)\n\n\n4\n:\n96\n(\n20.4%\n)\n\n\n5\n:\n20\n(\n4.3%\n)\n\n\n6\n:\n5\n(\n1.1%\n)\n\n\n7\n:\n2\n(\n0.4%\n)\n\n\n\n\n470 (100.0%)\n0 (0.0%)\n\n\n7\nAVADUAGE [numeric]\n\n\n\nMean (sd) : 37.8 (9.9)\n\n\nmin ≤ med ≤ max:\n\n\n0 ≤ 36 ≤ 78\n\n\nIQR (CV) : 12.7 (0.3)\n\n\n\n126 distinct values\n\n470 (100.0%)\n0 (0.0%)\n\n\n8\nIAGE [numeric]\n\n\n\nMean (sd) : 36.9 (11.6)\n\n\nmin ≤ med ≤ max:\n\n\n0 ≤ 34 ≤ 78\n\n\nIQR (CV) : 15 (0.3)\n\n\n\n53 distinct values\n\n470 (100.0%)\n0 (0.0%)\n\n\n9\nISEX [numeric]\n\n\n\nMin : 0\n\n\nMean : 0.5\n\n\nMax : 1\n\n\n\n\n\n\n0\n:\n214\n(\n45.5%\n)\n\n\n1\n:\n256\n(\n54.5%\n)\n\n\n\n\n470 (100.0%)\n0 (0.0%)\n\n\n10\nNCARS [numeric]\n\n\n\nMean (sd) : 1.7 (0.9)\n\n\nmin ≤ med ≤ max:\n\n\n0 ≤ 2 ≤ 5\n\n\nIQR (CV) : 1 (0.5)\n\n\n\n\n\n\n0\n:\n23\n(\n4.9%\n)\n\n\n1\n:\n182\n(\n38.7%\n)\n\n\n2\n:\n193\n(\n41.1%\n)\n\n\n3\n:\n56\n(\n11.9%\n)\n\n\n4\n:\n13\n(\n2.8%\n)\n\n\n5\n:\n3\n(\n0.6%\n)\n\n\n\n\n470 (100.0%)\n0 (0.0%)\n\n\n11\nAREA [numeric]\n\n\n\nMean (sd) : 133 (121.5)\n\n\nmin ≤ med ≤ max:\n\n\n30 ≤ 110 ≤ 2250\n\n\nIQR (CV) : 60 (0.9)\n\n\n\n76 distinct values\n\n470 (100.0%)\n0 (0.0%)\n\n\n12\nBEDROOM [numeric]\n\n\n\nMean (sd) : 2.9 (1.1)\n\n\nmin ≤ med ≤ max:\n\n\n0 ≤ 3 ≤ 7\n\n\nIQR (CV) : 1 (0.4)\n\n\n\n\n\n\n0\n:\n1\n(\n0.2%\n)\n\n\n1\n:\n28\n(\n6.0%\n)\n\n\n2\n:\n153\n(\n32.6%\n)\n\n\n3\n:\n180\n(\n38.3%\n)\n\n\n4\n:\n73\n(\n15.5%\n)\n\n\n5\n:\n26\n(\n5.5%\n)\n\n\n6\n:\n7\n(\n1.5%\n)\n\n\n7\n:\n2\n(\n0.4%\n)\n\n\n\n\n470 (100.0%)\n0 (0.0%)\n\n\n13\nPARK [numeric]\n\n\n\nMean (sd) : 0.8 (1)\n\n\nmin ≤ med ≤ max:\n\n\n0 ≤ 1 ≤ 4\n\n\nIQR (CV) : 1 (1.2)\n\n\n\n\n\n\n0\n:\n224\n(\n47.7%\n)\n\n\n1\n:\n136\n(\n28.9%\n)\n\n\n2\n:\n84\n(\n17.9%\n)\n\n\n3\n:\n18\n(\n3.8%\n)\n\n\n4\n:\n8\n(\n1.7%\n)\n\n\n\n\n470 (100.0%)\n0 (0.0%)\n\n\n14\nBEDSIZE [numeric]\n\n\n\nMean (sd) : 1.4 (0.8)\n\n\nmin ≤ med ≤ max:\n\n\n0 ≤ 1 ≤ 5\n\n\nIQR (CV) : 0.7 (0.6)\n\n\n\n22 distinct values\n\n470 (100.0%)\n0 (0.0%)\n\n\n15\nPARKSIZE [numeric]\n\n\n\nMean (sd) : 0.5 (0.6)\n\n\nmin ≤ med ≤ max:\n\n\n0 ≤ 0.2 ≤ 3\n\n\nIQR (CV) : 1 (1.2)\n\n\n\n13 distinct values\n\n470 (100.0%)\n0 (0.0%)\n\n\n16\nRAGE10 [numeric]\n\n\n\nMin : 0\n\n\nMean : 0.2\n\n\nMax : 1\n\n\n\n\n\n\n0\n:\n356\n(\n75.7%\n)\n\n\n1\n:\n114\n(\n24.3%\n)\n\n\n\n\n470 (100.0%)\n0 (0.0%)\n\n\n17\nTCBD [numeric]\n\n\n\nMean (sd) : 24.7 (16.2)\n\n\nmin ≤ med ≤ max:\n\n\n0.8 ≤ 23.8 ≤ 73.3\n\n\nIQR (CV) : 25.7 (0.7)\n\n\n\n434 distinct values\n\n470 (100.0%)\n0 (0.0%)\n\n\n18\nDISTHTC [numeric]\n\n\n\nMean (sd) : 1347 (1815.8)\n\n\nmin ≤ med ≤ max:\n\n\n49 ≤ 719 ≤ 17732.7\n\n\nIQR (CV) : 1125 (1.3)\n\n\n\n434 distinct values\n\n470 (100.0%)\n0 (0.0%)\n\n\n19\nTWCBD [numeric]\n\n\n\nMean (sd) : 17 (16.2)\n\n\nmin ≤ med ≤ max:\n\n\n0.3 ≤ 9.9 ≤ 67.8\n\n\nIQR (CV) : 20 (1)\n\n\n\n439 distinct values\n\n470 (100.0%)\n0 (0.0%)\n\n\n20\nTDWWK [numeric]\n\n\n\nMean (sd) : 23.5 (17.1)\n\n\nmin ≤ med ≤ max:\n\n\n0 ≤ 22.2 ≤ 80.7\n\n\nIQR (CV) : 23.6 (0.7)\n\n\n\n414 distinct values\n\n470 (100.0%)\n0 (0.0%)\n\n\n21\nHEADH [numeric]\n\n\n\nMin : 0\n\n\nMean : 0.9\n\n\nMax : 1\n\n\n\n\n\n\n0\n:\n64\n(\n13.6%\n)\n\n\n1\n:\n406\n(\n86.4%\n)\n\n\n\n\n470 (100.0%)\n0 (0.0%)\n\n\n22\nPOPDENS [numeric]\n\n\n\nMean (sd) : 92 (58.2)\n\n\nmin ≤ med ≤ max:\n\n\n0 ≤ 83.2 ≤ 255.6\n\n\nIQR (CV) : 89.2 (0.6)\n\n\n\n431 distinct values\n\n470 (100.0%)\n0 (0.0%)\n\n\n23\nEDUINDEX [numeric]\n\n\n\nMean (sd) : 0.2 (0.1)\n\n\nmin ≤ med ≤ max:\n\n\n0 ≤ 0.2 ≤ 0.7\n\n\nIQR (CV) : 0.2 (0.6)\n\n\n\n434 distinct values\n\n470 (100.0%)\n0 (0.0%)\n\n\n24\nGRAVCPC [numeric]\n\n\n\nMean (sd) : 0.3 (0.1)\n\n\nmin ≤ med ≤ max:\n\n\n0.1 ≤ 0.3 ≤ 0.4\n\n\nIQR (CV) : 0.1 (0.2)\n\n\n\n433 distinct values\n\n470 (100.0%)\n0 (0.0%)\n\n\n25\nGRAVCPT [numeric]\n\n\n\nMean (sd) : 0.3 (0.1)\n\n\nmin ≤ med ≤ max:\n\n\n0 ≤ 0.3 ≤ 0.4\n\n\nIQR (CV) : 0.1 (0.2)\n\n\n\n434 distinct values\n\n470 (100.0%)\n0 (0.0%)\n\n\n26\nGRAVPCPT [numeric]\n\n\n\nMean (sd) : 1.2 (0.3)\n\n\nmin ≤ med ≤ max:\n\n\n0.5 ≤ 1.1 ≤ 2.9\n\n\nIQR (CV) : 0.2 (0.3)\n\n\n\n434 distinct values\n\n470 (100.0%)\n0 (0.0%)\n\n\n27\nNSTRTC [numeric]\n\n\n\nMean (sd) : 22.2 (12.6)\n\n\nmin ≤ med ≤ max:\n\n\n0 ≤ 22 ≤ 84\n\n\nIQR (CV) : 16 (0.6)\n\n\n\n59 distinct values\n\n470 (100.0%)\n0 (0.0%)\n\n\n28\nDISTHW [numeric]\n\n\n\nMean (sd) : 1883.4 (1748.3)\n\n\nmin ≤ med ≤ max:\n\n\n74.7 ≤ 1338.7 ≤ 16590.1\n\n\nIQR (CV) : 1820.5 (0.9)\n\n\n\n434 distinct values\n\n470 (100.0%)\n0 (0.0%)\n\n\n29\nDIVIDX [numeric]\n\n\n\nMean (sd) : 0.4 (0.1)\n\n\nmin ≤ med ≤ max:\n\n\n0.3 ≤ 0.4 ≤ 0.6\n\n\nIQR (CV) : 0.1 (0.2)\n\n\n\n144 distinct values\n\n470 (100.0%)\n0 (0.0%)\n\n\n30\nACTDENS [numeric]\n\n\n\nMean (sd) : 5.8 (8.5)\n\n\nmin ≤ med ≤ max:\n\n\n0 ≤ 2.5 ≤ 63.2\n\n\nIQR (CV) : 3.5 (1.5)\n\n\n\n144 distinct values\n\n470 (100.0%)\n0 (0.0%)\n\n\n31\nDISTCBD [numeric]\n\n\n\nMean (sd) : 7967.4 (7442.9)\n\n\nmin ≤ med ≤ max:\n\n\n148.9 ≤ 5542.3 ≤ 44004.6\n\n\nIQR (CV) : 9777.9 (0.9)\n\n\n\n434 distinct values\n\n470 (100.0%)\n0 (0.0%)\n\n\n\n\nGenerated by summarytools 1.1.4 (R version 4.5.2)2025-11-12\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe used a different library for the summary statistics.\nR allows you to do the same or similar tasks with different packages.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Factor Analysis</span>"
    ]
  },
  {
    "objectID": "factor-analysis.html#assumptions-for-factoral-analysis",
    "href": "factor-analysis.html#assumptions-for-factoral-analysis",
    "title": "7  Factor Analysis",
    "section": "7.3 Assumptions for factoral analysis",
    "text": "7.3 Assumptions for factoral analysis\nThere are some rules of thumb and assumptions that should be checked before performing a factor analysis.\nRules of thumb:\n\nAt least 10 variables\nn &lt; 50 (Unacceptable); n &gt; 200 (recommended)\nIt is recommended to use continuous variables. If your data contains categorical variables, you should transform them to dummy variables.\n\nAs we have 31 variables (none categorical) and more than 200 observations, we can proceed to check the assumptions.\nAssumptions:\n\nNormality;\nLinearity;\nHomogeneity;\nHomoscedasticity (some multicollinearity is desirable);\nCorrelations between variables &lt; 0.3 are not appropriate to use Factor Analysis\n\nLet’s run a random regression model in order to evaluate some of the assumptions.\n\n# random regression model\nrandom = rchisq(nrow(data), ncol(data))\nfake = lm(random ~ ., data = data)\nstandardized = rstudent(fake)\nfitted = scale(fake$fitted.values)\n\n\n7.3.1 Normality\n\nhist(standardized)\n\n\n\n\n\n\n\n\nThe histogram looks like a normal distribution.\n\n\n7.3.2 Linearity\n\nqqnorm(standardized)\nabline(0, 1)\n\n\n\n\n\n\n\n\nThe QQ plot shows that the points are close to the diagonal line, indicating linearity.\n\n\n7.3.3 Homogeneity\n\nplot(fitted, standardized)\nabline(h=0, v=0)\n\n\n\n\n\n\n\n\nThe residuals are randomly distributed around zero, indicating homogeneity.\n\n\n7.3.4 Correlations between variables\nCorrelation matrix\n\ncorr_matrix = cor(data, method = \"pearson\")\n\nThe Bartlett’s test examines if there is equal variance (homogeneity) between variables. Thus, it evaluates if there is any pattern between variables.\n\nCorrelation adequacy\nCheck for correlation adequacy - Bartlett’s Test\n\ncortest.bartlett(corr_matrix, n = nrow(data))\n\n$chisq\n[1] 9880.074\n\n$p.value\n[1] 0\n\n$df\n[1] 465\n\n\nThe null hypothesis is that there is no correlation between variables. Therefore, you want to reject the null hypothesis.\n\nNote: A p-value &lt; 0.05 indicates that there are correlations between variables, and that factor analysis may be useful with your data.\n\n\n\nSampling adequacy\nCheck for sampling adequacy - KMO test\n\nKMO(corr_matrix)\n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = corr_matrix)\nOverall MSA =  0.68\nMSA for each item = \nDWELCLAS   INCOME  CHILD13      H18  HEMPLOY    HSIZE AVADUAGE     IAGE \n    0.70     0.85     0.33     0.58     0.88     0.59     0.38     0.40 \n    ISEX    NCARS     AREA  BEDROOM     PARK  BEDSIZE PARKSIZE   RAGE10 \n    0.71     0.74     0.60     0.53     0.62     0.58     0.57     0.84 \n    TCBD  DISTHTC    TWCBD    TDWWK    HEADH  POPDENS EDUINDEX  GRAVCPC \n    0.88     0.88     0.82     0.89     0.47     0.82     0.85     0.76 \n GRAVCPT GRAVPCPT   NSTRTC   DISTHW   DIVIDX  ACTDENS  DISTCBD \n    0.71     0.31     0.83     0.76     0.63     0.70     0.86 \n\n\nWe want at least 0.7 of the overall Mean Sample Adequacy (MSA). If, 0.6 &lt; MSA &lt; 0.7, it is not a good value, but acceptable in some cases.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Factor Analysis</span>"
    ]
  },
  {
    "objectID": "factor-analysis.html#determine-the-number-of-factors-to-extract",
    "href": "factor-analysis.html#determine-the-number-of-factors-to-extract",
    "title": "7  Factor Analysis",
    "section": "7.4 Determine the number of factors to extract",
    "text": "7.4 Determine the number of factors to extract\nThere are several ways to determine the number of factors to extract. Here we will use three different methods:\n\nParallel Analysis\nKaiser Criterion\nPrincipal Component Analysis (PCA)\n\n\n7.4.1 Parallel Analysis\n\nnum_factors = fa.parallel(\n  x = data, \n  fm = \"ml\", # factor mathod = maximum likelihood\n  fa = \"fa\") # factor analysis\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  8  and the number of components =  NA \n\n\nThe selection of the number of factors in the Parallel analysis can be threefold:\n\nDetect where there is an “elbow” in the graph;\nDetect the intersection between the “FA Actual Data” and the “FA Simulated Data”;\nConsider the number of factors with eigenvalue &gt; 1.\n\n\n\n7.4.2 Kaiser Criterion\n\nsum(num_factors$fa.values &gt; 1) # Number of factors with eigenvalue &gt; 1\n\n[1] 4\n\nsum(num_factors$fa.values &gt; 0.7) # Number of factors with eigenvalue &gt; 0.7\n\n[1] 6\n\n\nYou can also consider factors with eigenvalue &gt; 0.7, since some of the literature indicate that this value does not overestimate the number of factors as much as considering an eigenvalue = 1.\n\n\n7.4.3 Principal Component Analysis (PCA)\n\n\n\n\n\n\nPCA is not the same thing as Factor Analysis\n\n\n\nPCA only considers the common information (variance) of the variables, while factor analysis takes into account also the unique variance of the variable. Both approaches are often mixed up. In this example we use PCA as only a first criteria for choosing the number of factors. PCA is usually used in image recognition and data reduction of big data.\n\n\n\nVariance explained by components\nPrint variance that explains the components\n\ndata_pca = princomp(data,\n                    cor = TRUE) # standardizes your dataset before running a PCA\nsummary(data_pca)  \n\nImportance of components:\n                         Comp.1    Comp.2     Comp.3     Comp.4     Comp.5\nStandard deviation     2.450253 1.9587909 1.61305418 1.43367870 1.27628545\nProportion of Variance 0.193669 0.1237697 0.08393367 0.06630434 0.05254531\nCumulative Proportion  0.193669 0.3174388 0.40137245 0.46767679 0.52022210\n                           Comp.6     Comp.7     Comp.8    Comp.9    Comp.10\nStandard deviation     1.26612033 1.22242045 1.11550534 1.0304937 0.99888665\nProportion of Variance 0.05171164 0.04820361 0.04014039 0.0342554 0.03218628\nCumulative Proportion  0.57193374 0.62013734 0.66027774 0.6945331 0.72671941\n                          Comp.11    Comp.12   Comp.13    Comp.14    Comp.15\nStandard deviation     0.97639701 0.92221635 0.9042314 0.85909928 0.80853555\nProportion of Variance 0.03075326 0.02743494 0.0263753 0.02380812 0.02108806\nCumulative Proportion  0.75747267 0.78490761 0.8112829 0.83509102 0.85617908\n                         Comp.16    Comp.17    Comp.18    Comp.19    Comp.20\nStandard deviation     0.7877571 0.74436225 0.72574751 0.69380677 0.67269732\nProportion of Variance 0.0200181 0.01787339 0.01699063 0.01552799 0.01459747\nCumulative Proportion  0.8761972 0.89407058 0.91106120 0.92658920 0.94118667\n                          Comp.21    Comp.22    Comp.23     Comp.24     Comp.25\nStandard deviation     0.63466979 0.61328635 0.55192724 0.397467153 0.384354087\nProportion of Variance 0.01299373 0.01213291 0.00982657 0.005096133 0.004765421\nCumulative Proportion  0.95418041 0.96631331 0.97613988 0.981236017 0.986001438\n                           Comp.26     Comp.27     Comp.28     Comp.29\nStandard deviation     0.364232811 0.322026864 0.276201256 0.262018088\nProportion of Variance 0.004279534 0.003345203 0.002460875 0.002214628\nCumulative Proportion  0.990280972 0.993626175 0.996087050 0.998301679\n                            Comp.30      Comp.31\nStandard deviation     0.1712372644 0.1527277294\nProportion of Variance 0.0009458774 0.0007524438\nCumulative Proportion  0.9992475562 1.0000000000\n\n\n\n\nScree Plot\n\nplot(data_pca, type = \"lines\", npcs = 31)\n\n\n\n\n\n\n\n\nCheck the cumulative variance of the first components and the scree plot, and see if the PCA is a good approach to detect the number of factors in this case.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Factor Analysis</span>"
    ]
  },
  {
    "objectID": "factor-analysis.html#exploratory-factor-analysis-efa",
    "href": "factor-analysis.html#exploratory-factor-analysis-efa",
    "title": "7  Factor Analysis",
    "section": "7.5 Exploratory Factor Analysis (EFA)",
    "text": "7.5 Exploratory Factor Analysis (EFA)\n\n7.5.1 Rotational indeterminacy\nWe can rotate the factors, so that the loadings will be as close as possible to a desired structure.\nThere are two types of rotation methods.\n\nOrthogonal\nOrthogonal rotation clarifies factor structure while preserving independence between factors - assumes the factors are independent from each other (i.e., their correlation is zero).\nThe underlying factors after rotation will be uncorrelated.\nThe method rotates the factor axes while keeping them perpendicular, so the factors remain uncorrelated.\n\n\n\nBased in Field (2024)\n\n\nThe goal is to increase each item’s loading on one main factor, and reduce its loadings on the other factors, which makes the factors easier to interpret.\nMethods include:\n\nVarimax\nQuartimax\nEquimax\n\n\n\nOblique\nOblique rotation improves factor interpretability while allowing the factors to be correlated with each other. Unlike orthogonal rotation, it does not force the factors to remain independent.\nBecause of this, oblique methods are often more suitable for complex and interrelated constructs, where some degree of relationship between factors is expected.\nOblique rotation does not require the factor axes to stay perpendicular. As a result, the rotated factors can correlate, allowing a more realistic representation of how variables relate to multiple underlying dimensions.\n\n\n\nBased in Field (2024)\n\n\nMethods include:\n\nOblimin\nQaurtimin\nPromax\n\nLet’s run three different factor analysis models with different rotation methods:\n\nModel 1: No rotation\nModel 2: Rotation Varimax\nModel 3: Rotation Oblimin\n\n\n# No rotation\ndata_factor = factanal(\n  data,\n  factors = 4, # change here the number of facotrs based on the EFA\n  rotation = \"none\",\n  scores = \"regression\",\n  fm = \"ml\"\n)\n\n# Rotation Varimax\ndata_factor_var = factanal(\n  data,\n  factors = 4,\n  rotation = \"varimax\", # orthogonal rotation (default)\n  scores = \"regression\",\n  fm = \"ml\"\n)\n\n# Rotation Oblimin\ndata_factor_obl = factanal(\n  data, \n  factors = 4,\n  rotation = \"oblimin\", # oblique rotation\n  scores = \"regression\",\n  fm = \"ml\"\n)\n\nPrint out the results of data_factor_obl, and have a look.\n\nprint(data_factor_obl,\n      digits = 2,\n      cutoff = 0.3, # &gt; 0.3 due to the sample size is higher than 350 observations.\n      sort = TRUE) \n\n\nCall:\nfactanal(x = data, factors = 4, scores = \"regression\", rotation = \"oblimin\",     fm = \"ml\")\n\nUniquenesses:\nDWELCLAS   INCOME  CHILD13      H18  HEMPLOY    HSIZE AVADUAGE     IAGE \n    0.98     0.82     0.09     0.01     0.73     0.01     0.98     0.99 \n    ISEX    NCARS     AREA  BEDROOM     PARK  BEDSIZE PARKSIZE   RAGE10 \n    0.98     0.64     0.93     0.79     0.89     0.62     0.92     0.91 \n    TCBD  DISTHTC    TWCBD    TDWWK    HEADH  POPDENS EDUINDEX  GRAVCPC \n    0.14     0.54     0.80     0.74     0.67     0.78     0.71     0.04 \n GRAVCPT GRAVPCPT   NSTRTC   DISTHW   DIVIDX  ACTDENS  DISTCBD \n    0.05     0.01     0.85     0.66     0.84     0.80     0.31 \n\nLoadings:\n         Factor1 Factor2 Factor3 Factor4\nTCBD      0.93                          \nDISTHTC   0.62                          \nEDUINDEX -0.53                          \nGRAVCPC  -0.98                          \nGRAVCPT  -0.74           -0.55          \nDISTHW    0.56                          \nDISTCBD   0.81                          \nH18               1.02                  \nHSIZE             0.74            0.54  \nNCARS             0.57                  \nBEDSIZE          -0.52                  \nHEADH            -0.56                  \nGRAVPCPT                  1.00          \nCHILD13                           0.97  \nDWELCLAS                                \nINCOME            0.38                  \nHEMPLOY           0.47                  \nAVADUAGE                                \nIAGE                                    \nISEX                                    \nAREA                                    \nBEDROOM           0.37                  \nPARK                                    \nPARKSIZE                                \nRAGE10                                  \nTWCBD     0.43                          \nTDWWK     0.49                          \nPOPDENS  -0.41                          \nNSTRTC   -0.37                          \nDIVIDX   -0.40                          \nACTDENS  -0.42                          \n\n            Factor1 Factor2 Factor3 Factor4\nSS loadings    5.24    3.13    1.66    1.57\n\nFactor Correlations:\n        Factor1 Factor2 Factor3 Factor4\nFactor1   1.000   0.062   0.117   0.021\nFactor2   0.062   1.000   0.011   0.199\nFactor3   0.117   0.011   1.000   0.054\nFactor4   0.021   0.199   0.054   1.000\n\nTest of the hypothesis that 4 factors are sufficient.\nThe chi square statistic is 3628.43 on 347 degrees of freedom.\nThe p-value is 0 \n\n\n\nThe variability contained in the factors is equal to Communality + Uniqueness.\n\n\n\n\n7.5.2 Factor scores and factor loadings\nIn addition to the loading structure, you may also want to know the factor scores of each observation.\nWe can extract the factor scores with\n\nView(data_factor_obl$scores)\n# write.csv(data_factor_obl$scores, \"data/data_factor_obl_scores.csv\", sep = \"\\t\")\nhead(data_factor_obl$scores)\n\n             Factor1     Factor2    Factor3    Factor4\n799161661  0.5269517 -0.01526867 -0.4212963  0.6723761\n798399409 -0.6999716 -1.28451045 -0.3425305 -0.5906694\n798374392  0.3140942  0.17925007 -1.0077065  1.8899099\n798275277  0.2576954  1.09054593 -0.1178061  0.3756358\n798264250 -0.3366206 -1.08082681 -0.5807561  0.6090050\n798235878 -0.5993148  0.84250056  1.2619817 -0.5142385\n\n\nThe individual indicator/subtest scores would be the weighted sum of the factor scores, where the weights are the determined by factor loadings.\n\nView(data_factor_obl$loadings)\n# write.csv(data_factor_obl$loadings, \"data/data_factor_obl_loadings.csv\", sep = \"\\t\")\nhead(data_factor_obl$loadings)\n\n               Factor1     Factor2     Factor3     Factor4\nDWELCLAS -0.0333847085 -0.04628588  0.13740126  0.03391240\nINCOME   -0.1309767372  0.37518984  0.04293329  0.10335426\nCHILD13   0.0041194104 -0.06692967  0.00769277  0.96689496\nH18       0.0008958126  1.01553559 -0.01668507 -0.17758151\nHEMPLOY   0.0638121517  0.47320725  0.07699066  0.09956475\nHSIZE    -0.0081682615  0.73984312 -0.01057724  0.53609771\n\n\n\n\n7.5.3 Visualize Rotation\nWe will define a plot function to make it easier to visualize several factor pairs.\n\n\nPlot function\n# define a plot function\nplot_factor_loading &lt;- function(data_factor,\n                                f1 = 1,\n                                f2 = 2,\n                                method = \"No rotation\",\n                                color = \"blue\") {\n  \n  # Convert to numeric matrix (works for psych loadings objects)\n  L &lt;- as.matrix(data_factor$loadings)\n  \n  # Extract selected factors\n  df &lt;- data.frame(item = rownames(L), x = L[, f1], y = L[, f2])\n  \n  ggplot(df, aes(x = x, y = y, label = item)) +\n    geom_point() +\n    geom_text(color = color,\n              vjust = -0.5,\n              size = 3) +\n    geom_hline(yintercept = 0) +\n    geom_vline(xintercept = 0) +\n    coord_equal(xlim = c(-1, 1), ylim = c(-1, 1)) +\n    labs(\n      x = paste0(\"Factor \", f1),\n      y = paste0(\"Factor \", f2),\n      title = method\n    ) +\n    theme_bw()\n}\n\n\n\nNo Rotation\nPlot factor 1 against factor 2, and compare the results\n\nplot_factor_loading(\n  data_factor = data_factor, # model no rotation\n  f1 = 1, # Factor 1\n  f2 = 2, # Factor 2\n  method = \"No rotation\", # plot title\n  color = \"blue\"\n)\n\n\n\n\n\n\n\n\n\n\nVarimax Rotation\n\nplot_factor_loading(\n  data_factor = data_factor_var, # model varimax\n  f1 = 1, # Factor 1\n  f2 = 2, # Factor 2\n  method = \"Varimax rotation\",\n  color = \"red\"\n)\n\n\n\n\n\n\n\n\n\n\nOblimin Rotation\n\nplot_factor_loading(\n  data_factor = data_factor_var, # model oblimn\n  f1 = 1, # Factor 1\n  f2 = 2, # Factor 2\n  method = \"Oblimin rotation\",\n  color = \"darkgreen\"\n)\n\n\n\n\n\n\n\n\nWhen we have more than two factors it is difficult to analyse the factors by the plots.\nVariables that have low explaining variance in the two factors analysed, could be highly explained by the other factors not present in the graph.\n\n\n\n\n\n\nYour turn\n\n\n\nTry comparing the plots with the factor loadings and plot the other factor pairs (replace f1 and f2) to get more familiar with exploratory factor analysis.\nInterpret the factors and try to give them a name.\n\n\nFor instance, we can plot all the factors against each other as follows:\n\n# create all combinations\np12 &lt;- plot_factor_loading(data_factor, 1, 2, method = \"No rotation\", color = \"blue\")\np13 &lt;- plot_factor_loading(data_factor, 1, 3, method = \"No rotation\", color = \"blue\")\np14 &lt;- plot_factor_loading(data_factor, 1, 4, method = \"No rotation\", color = \"blue\")\np23 &lt;- plot_factor_loading(data_factor, 2, 3, method = \"No rotation\", color = \"blue\")\np24 &lt;- plot_factor_loading(data_factor, 2, 4, method = \"No rotation\", color = \"blue\")\np34 &lt;- plot_factor_loading(data_factor, 3, 4, method = \"No rotation\", color = \"blue\")\n\nlibrary(patchwork)\n(p12 + p13 + p14) /\n(p23 + p24 + p34)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nField, A. 2024. Discovering Statistics Using IBM SPSS Statistics. 6th ed. SAGE Publications. https://books.google.pt/books?id=83L2EAAAQBAJ.\n\n\nMartinez, Luís Garrido, João de Abreu e Silva, and José Manuel Viegas. 2010. “Assessment of Residential Location Satisfaction in the Lisbon Metropolitan Area.” In. https://trid.trb.org/View/909819.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Factor Analysis</span>"
    ]
  },
  {
    "objectID": "clusters.html",
    "href": "clusters.html",
    "title": "8  Cluster Analysis",
    "section": "",
    "text": "8.1 Load packages\nBased on a dataset of European Airports, we will create clusters based on the observations.\nlibrary(tidyverse) # Pack of most used libraries for data science\nlibrary(readxl) # Import excel files\nlibrary(skimr) # Summary statistics\nlibrary(mclust) # Model based clustering\nlibrary(cluster) # Cluster analysis\nlibrary(factoextra) # Visualizing distances",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "clusters.html#dataset",
    "href": "clusters.html#dataset",
    "title": "8  Cluster Analysis",
    "section": "8.2 Dataset",
    "text": "8.2 Dataset\nIncluded variables:\n\nCode - Code of the airport\nAirport - Name of the airport\nOrdem - ID of the observations\nPassengers - Number of passengers\nMovements - Number of flights\nNumberofairlines - Number of airlines at each airport\nMainairlineflightspercentage - Percentage of flights of the main airline of each airport\nMaximumpercentageoftrafficpercountry - Maximum percentage of flights per country\nNumberofLCCflightsweekly - Number of weekly low cost flights\nNumberofLowCostAirlines - Number of low cost airlines of each airport\nLowCostAirlinespercentage - Percentage of the number of low cost airlines in each airport\nDestinations - Number of flights arriving at each airport\nAverage_route_Distance - Average route distance in km\nDistancetoclosestAirport - Distance to closest airport in km\nDistancetoclosestSimilarAirport - Distance to closest similar airport in km\nAirportRegionalRelevance - Relevance of the airport in a regional scale (0 - 1)\nDistancetocitykm - Distance between the airport and the city in km\nInhabitantscorrected - Population of the city\nnumberofvisitorscorrected - Number of visitors arrived in the airport\nGDPcorrected - Corrected value of the Gross Domestic Product\nCargoton - The total number of cargo [ton] transported in a certain period multiplied by the number of flights.\n\n\n8.2.1 Import dataset\n\ndata = read_excel(\"../data/Data_Aeroports_Clustersv1.xlsx\")\ndata = data.frame(data) # as data frame only\n\n\n\n8.2.2 Get to know your dataset\nTake a look at the first values of the dataset\n\nhead(data, 5)\n\n  Code Ordem          Airport Passengers Movements Numberofairlines\n1  NCE     1 Nice Côte d'Azur    9830987    119322               64\n2  CGN     2     Cologne Bonn    9742300    132200               29\n3  LPA     3     Gran Canaria    9155665    101557               47\n4  ALC     4         Alicante    9139479     74281               35\n5  LTN     5     London Luton    9129053     83013               11\n  Mainairlineflightspercentage Maximumpercentageoftrafficpercountry\n1                           18                                   20\n2                           33                                   13\n3                           17                                   26\n4                           29                                   23\n5                           37                                   22\n  NumberofLCCflightsweekly NumberofLowCostAirlines LowCostAirlinespercentage\n1                      256                      18                  28.12500\n2                      351                      12                  41.37931\n3                      259                      19                  40.42553\n4                      300                      18                  51.42857\n5                      227                       8                  72.72727\n  Destinations Average_Route_Distance DistancetoclosestAirport\n1          104                   1253                 23.66681\n2          189                   1721                 63.45766\n3          116                   3143                122.58936\n4          160                   1701                 63.09924\n5           87                   1582                 45.13247\n  DistancetoclosestSimilarAirport AirportRegionalrelevance Distancetocitykm\n1                       223.83824                0.8698581                6\n2                        63.45766                0.5127419               15\n3                       132.45082                0.7840877               19\n4                       134.50558                0.8098081                9\n5                        45.13247                0.1947903               55\n  Inhanbitantscorrected numberofvisitorscorrected GDPcorrected Cargoton\n1             3551805.0                 2152829.8        26300 11223.39\n2             4180133.5                 1151381.6        30100   562.00\n3              705807.8                 1678968.6        20700 25994.00\n4             1508358.6                 1944196.8        25000  3199.73\n5             1562709.8                  181063.5        32000 28698.00\n\n\nCheck summary statistics of variables\n\nskim(data)\n\n\nData summary\n\n\nName\ndata\n\n\nNumber of rows\n32\n\n\nNumber of columns\n21\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n19\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nCode\n0\n1\n3\n3\n0\n32\n0\n\n\nAirport\n0\n1\n4\n35\n0\n32\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nOrdem\n0\n1\n16.50\n9.38\n1.00\n8.75\n16.50\n24.25\n32.00\n▇▇▇▇▇\n\n\nPassengers\n0\n1\n20750710.88\n17601931.34\n456698.00\n8927021.50\n17275317.50\n28666511.50\n67054745.00\n▇▅▂▂▁\n\n\nMovements\n0\n1\n205111.16\n143564.45\n5698.00\n82765.75\n191742.50\n258654.50\n518018.00\n▇▅▇▂▃\n\n\nNumberofairlines\n0\n1\n57.81\n40.42\n1.00\n22.50\n55.50\n90.25\n136.00\n▇▆▅▆▃\n\n\nMainairlineflightspercentage\n0\n1\n33.78\n22.08\n12.00\n22.00\n28.50\n33.00\n95.00\n▇▆▁▁▂\n\n\nMaximumpercentageoftrafficpercountry\n0\n1\n17.47\n7.31\n9.00\n12.00\n15.00\n22.25\n35.00\n▇▂▃▂▂\n\n\nNumberofLCCflightsweekly\n0\n1\n397.59\n221.56\n37.00\n226.25\n366.50\n546.75\n776.00\n▅▇▅▅▆\n\n\nNumberofLowCostAirlines\n0\n1\n11.59\n5.60\n1.00\n7.75\n12.00\n16.00\n23.00\n▃▇▇▆▃\n\n\nLowCostAirlinespercentage\n0\n1\n36.44\n30.10\n6.25\n16.29\n19.59\n50.36\n100.00\n▇▂▁▁▂\n\n\nDestinations\n0\n1\n167.62\n80.13\n20.00\n109.25\n168.50\n222.50\n301.00\n▃▇▆▇▆\n\n\nAverage_Route_Distance\n0\n1\n2275.19\n930.28\n1225.00\n1599.50\n2152.00\n2765.00\n5635.00\n▇▆▂▁▁\n\n\nDistancetoclosestAirport\n0\n1\n90.19\n64.56\n13.84\n45.83\n66.50\n111.61\n244.50\n▇▇▃▁▂\n\n\nDistancetoclosestSimilarAirport\n0\n1\n248.64\n183.60\n38.16\n97.74\n206.12\n376.15\n635.05\n▇▅▃▁▃\n\n\nAirportRegionalrelevance\n0\n1\n0.73\n0.23\n0.19\n0.58\n0.80\n0.91\n0.99\n▁▃▁▆▇\n\n\nDistancetocitykm\n0\n1\n25.81\n25.44\n3.00\n9.75\n14.50\n35.00\n100.00\n▇▂▁▁▁\n\n\nInhanbitantscorrected\n0\n1\n4528561.95\n2590542.88\n329240.50\n2856960.30\n4532760.00\n6733158.88\n9870818.00\n▆▆▇▇▁\n\n\nnumberofvisitorscorrected\n0\n1\n2766002.58\n2549773.72\n80232.50\n1018390.89\n1896295.60\n3450491.78\n9732062.00\n▇▃▁▂▁\n\n\nGDPcorrected\n0\n1\n30160.75\n10510.93\n8500.00\n25000.00\n31150.00\n35550.00\n56600.00\n▃▅▇▃▁\n\n\nCargoton\n0\n1\n236531.76\n478310.12\n0.00\n10325.00\n72749.85\n153372.85\n1819000.00\n▇▁▁▁▁\n\n\n\n\n\nAs exploring the data, we can plot the Numberofairlines against the Destinations and observe.\n\n\nCode\nggplot(data, aes(x = Destinations, y = Numberofairlines)) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_text(aes(label = Airport), vjust = 1.5, size = 3, show.legend = FALSE) +\n  labs(\n    title = \"Airports\",\n    x = \"Number of destinations\",\n    y = \"Number of airlines\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nBy looking at the plot, you may already have a clue on the number of clusters with this two variables. However, this is not clear and it does not consider the other variables in the analysis.\n\n\n8.2.3 Prepare data\n\nRow-names\nMake the Code variable as row names or case number\n\ndata = data |&gt; column_to_rownames(var = \"Code\")\n\n\n\nRemove the non-continuous data\nLeave only continuous variables and the ordered ID.\n\ndata_continuous = data |&gt; select(-Ordem, -Airport) # remove chr and id variables\n\n\n\nStandardize variables\nTake a look at the scale of the variables. See how different they are!\n\nhead(data_continuous)\n\n    Passengers Movements Numberofairlines Mainairlineflightspercentage\nNCE    9830987    119322               64                           18\nCGN    9742300    132200               29                           33\nLPA    9155665    101557               47                           17\nALC    9139479     74281               35                           29\nLTN    9129053     83013               11                           37\nWAW    8320927    115934               36                           31\n    Maximumpercentageoftrafficpercountry NumberofLCCflightsweekly\nNCE                                   20                      256\nCGN                                   13                      351\nLPA                                   26                      259\nALC                                   23                      300\nLTN                                   22                      227\nWAW                                   14                      341\n    NumberofLowCostAirlines LowCostAirlinespercentage Destinations\nNCE                      18                  28.12500          104\nCGN                      12                  41.37931          189\nLPA                      19                  40.42553          116\nALC                      18                  51.42857          160\nLTN                       8                  72.72727           87\nWAW                       7                  19.44445          111\n    Average_Route_Distance DistancetoclosestAirport\nNCE                   1253                 23.66681\nCGN                   1721                 63.45766\nLPA                   3143                122.58936\nALC                   1701                 63.09924\nLTN                   1582                 45.13247\nWAW                   1460                244.49577\n    DistancetoclosestSimilarAirport AirportRegionalrelevance Distancetocitykm\nNCE                       223.83824                0.8698581                6\nCGN                        63.45766                0.5127419               15\nLPA                       132.45082                0.7840877               19\nALC                       134.50558                0.8098081                9\nLTN                        45.13247                0.1947903               55\nWAW                       559.31000                0.9810450               10\n    Inhanbitantscorrected numberofvisitorscorrected GDPcorrected Cargoton\nNCE             3551805.0                 2152829.8        26300 11223.39\nCGN             4180133.5                 1151381.6        30100   562.00\nLPA              705807.8                 1678968.6        20700 25994.00\nALC             1508358.6                 1944196.8        25000  3199.73\nLTN             1562709.8                  181063.5        32000 28698.00\nWAW             6626197.0                  770720.5        11200 82756.54\n\n\nZ-score standardization:\n\\[\nZ = \\frac{X - \\mu} {\\sigma}\n\\]\n\ndata_scaled = data_continuous |&gt; \n  mutate(across(everything(), ~ ( . - mean(.) ) / sd(.)))\n# Result = z-scores, same as scale()",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "clusters.html#hierarchical-clustering",
    "href": "clusters.html#hierarchical-clustering",
    "title": "8  Cluster Analysis",
    "section": "8.3 Hierarchical Clustering",
    "text": "8.3 Hierarchical Clustering\n\n8.3.1 Distance measures\nSimilarity of observations can be measured through different distance measures, including:\n\nEuclidean distance\nMinkowski distance\nManhattan distance\nMahanalobis distance\n\nLet’s measure the euclidean distances of our standartize data and visualize them on a heatmap\n\n# measure\ndistance = dist(data_scaled, method = \"euclidean\")\n\n# heatmap\nfviz_dist(\n  distance, \n  gradient = list(\n    low = \"#00AFBB\",\n    mid = \"white\",\n    high = \"#FC4E07\"\n  ),\n  order = FALSE\n)\n\n\n\n\n\n\n\n\nBy the color codes, you can have a clue of the airports that are more similar.\n\n\n8.3.2 Types of hierarchical clustering\nThere are many types of hierarchical clustering. We will explore some of them:\n\nSingle linkage (nearest neighbour) clustering algorithm\nComplete linkage (Farthest neighbour) clustering algorithm\nAverage linkage between groups\nWard`s method\nCentroid method\n\n\nSingle linkage\nThe single linkage method (which is closely related to the minimal spanning tree) adopts a ‘friends of friends’ clustering strategy.\nThis clustering algorithm is based on a bottom-up approach, by linking two clusters that have the closest distance between each other.\n\ncluster_single = hclust(distance, \"single\")\n\n# dendogram\nplot(\n  cluster_single,\n    xlab = \"Distance - Single linkage\",\n  hang = -1, # all to the bottom\n  cex = 0.6 # label text size\n)\nrect.hclust(cluster_single, k = 4, border = \"purple\") # cut on the dendogram at 4 clusters\n\n\n\n\n\n\n\n\nThis results in 4 clusters, with Heathrow Airport and Kaunas Airport at their own cluster.\n\n\nComplete linkage\nThe complete linkage method finds similar clusters.\nComplete linkage is based on the maximizing distance between observations in each cluster.\n\ncluster_complete = hclust(distance, \"complete\")\n\n# dendogram\nplot(\n  cluster_complete,\n    xlab = \"Distance - Complete linkage\",\n  hang = -1, # all to the bottom\n  cex = 0.6 # label text size\n)\nrect.hclust(cluster_complete, k = 4, border = \"blue\") # cut on the dendogram at 4 clusters\n\n\n\n\n\n\n\n\n\n\nAverage linkage\nThe average linkage considers the distance between clusters to be the average of the distances between observations in one cluster to all the members in the other cluster.\n\ncluster_average = hclust(distance, \"average\")\n\n# dendogram\nplot(\n  cluster_average,\n  xlab = \"Distance - Average linkage\",\n  hang = -1, # all to the bottom\n  cex = 0.6 # label text size\n)\nrect.hclust(cluster_complete, k = 4, border = \"red\") # cut on the dendogram at 4 clusters\n\n\n\n\n\n\n\n\n\n\nWard`s method\nWard’s minimum variance method aims at finding compact, spherical clusters.\nThe Ward`s method considers the measures of similarity as the sum of squares within the cluster summed over all variables.\n\ncluster_ward = hclust(distance, \"ward.D2\")\n\n# dendogram\nplot(\n  cluster_ward,\n  xlab = \"Distance - Ward's method\",\n  hang = -1, # all to the bottom\n  cex = 0.6 # label text size\n)\nrect.hclust(cluster_complete, k = 4, border = \"orange\") # cut on the dendogram at 4 clusters\n\n\n\n\n\n\n\n\n\n\nCentroid method\nThe centroid method considers the similarity between two clusters as the distance between its centroids.\n\ncluster_centroid = hclust(distance, \"centroid\")\n\n# dendogram\nplot(\n  cluster_centroid,\n  xlab = \"Distance - Centroid method\",\n  hang = -1, # all to the bottom\n  cex = 0.6 # label text size\n)\nrect.hclust(cluster_complete, k = 4, border = \"darkgreen\") # cut on the dendogram at 4 clusters\n\n\n\n\n\n\n\n\nAs mentioned in R Core Team (2025), note however, that methods centroid are not leading to a monotone distance measure, or equivalently the resulting dendrograms can have so called inversions or reversals which are hard to interpret, but note the trichotomies in Legendre and Legendre (2012) .\n\n\n\n8.3.3 Comparing results from different hierarchical methods\nNow let’s assess the membership of each observation with the cutree function for each method.\n\nnumber_clusters = 4 # change here\nmember_single = cutree(cluster_single, k = number_clusters)\nmember_complete = cutree(cluster_complete, k = number_clusters)\nmember_average = cutree(cluster_average, k = number_clusters)\nmember_ward = cutree(cluster_ward, k = number_clusters)\nmember_centroid = cutree(cluster_centroid, k = number_clusters)\n\nWe can make a table of cluster memberships for each observation to each cluster method, and compare them with a color code.\n\n\nCode\n# make a data frame\ncluster_membership = data.frame(member_single,\n                                member_complete,\n                                member_average,\n                                member_ward,\n                                member_centroid\n                                )\n# manipulate data for plot\ncluster_long = cluster_membership |&gt;\n  rownames_to_column(var = \"airport\") |&gt;  # keep airport names\n  pivot_longer(\n    cols = starts_with(\"member_\"),\n    names_to = \"method\",\n    values_to = \"cluster\") |&gt;\n  mutate(method = gsub(\"member_\", \"\", method), # clean names\n         method = factor(method, # preserve the label order\n                         levels = c(\"single\", \"complete\", \"average\", \"ward\", \"centroid\")))  \n# plot\nggplot(cluster_long,\n       aes(x = method,\n           y = airport,\n           fill = factor(cluster))) +\n  geom_tile(color = \"white\") +\n  scale_fill_brewer(palette = \"Set3\", name = \"Cluster\") +\n  theme_minimal() +\n  labs(title = \"Cluster memberships by method\",\n    x = \"Clustering method\",\n    y = \"Airport\") +\n  theme(axis.text.y = element_text(size = 6))\n\n\n\n\n\n\n\n\n\nCompare how common each method is to each other:\n\ntable(member_complete, member_average) # complete linkage vs. average linkage\n\n               member_average\nmember_complete  1  2  3  4\n              1 14  0  0  0\n              2 11  0  0  0\n              3  0  3  0  0\n              4  0  0  3  1\n\ntable(member_complete, member_ward) # complete linkage vs. ward's method\n\n               member_ward\nmember_complete  1  2  3  4\n              1  2 12  0  0\n              2 11  0  0  0\n              3  0  0  3  0\n              4  0  0  0  4\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nTry comparing other methods, and evaluate how common they are.\n\n\n\n\n8.3.4 Silhouette Plots\nThe silhouette plot evaluates how similar an observation is to its own cluster compared to other clusters. The clustering configuration is appropriate when most objects have high values.\nLow or negative values indicate that the clustering method is not appropriate or the number of clusters is not ideal.\n\nplot(silhouette(member_single, distance))\n\n\n\n\n\n\n\nplot(silhouette(member_complete, distance))\n\n\n\n\n\n\n\nplot(silhouette(member_average, distance))\n\n\n\n\n\n\n\nplot(silhouette(member_ward, distance))\n\n\n\n\n\n\n\nplot(silhouette(member_centroid, distance))",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "clusters.html#non-hirarchical-clustering",
    "href": "clusters.html#non-hirarchical-clustering",
    "title": "8  Cluster Analysis",
    "section": "8.4 Non-Hirarchical Clustering",
    "text": "8.4 Non-Hirarchical Clustering\nNon-hierarchical clustering is a method that partitions data points into a predetermined number of clusters, denoted by 𝑘, without creating a nested tree-like structure.\nUnlike hierarchical clustering, it requires the user to specify 𝑘 in advance and uses an iterative algorithm to optimize a criterion, such as minimizing the variance within each cluster.\nPopular examples include K-Means and K-Medoids, which assign points to the nearest cluster center (centroid or medoid) and repeat the process until the cluster assignments no longer change significantly.\n\n8.4.1 K-means clustering\n\nChoose the number of clusters\nWe can observe a measure of the goodness of the classification for each k-means, using the following ratio:\n\\[\\frac{Between_{SS}} {Total_{SS}}\\]\nSS stands for Sum of Squares, so it’s the usual decomposition of deviance in deviance “Between” and deviance “Within”. Ideally you want a clustering that has the properties of internal cohesion and external separation, i.e. the BSS/TSS ratio should approach 1.\nThis algorithm will detect how many clusters, from 1 to 10, explains more variance, with less clusters:\n\n# loop for the 10 cluster trials\nkmeans_diagnostic = data.frame()\n\nfor (i in 1:10) {\n  km = kmeans(data_scaled, centers = i)\n  km_diagn = data.frame(\n    k = i,\n    between_ss = km$betweenss,\n    tot_ss = km$totss,\n    ratio = km$betweenss / km$totss\n  )\n  kmeans_diagnostic = rbind(kmeans_diagnostic, km_diagn)\n}\n\n# marginal improvements for each new cluster\nkmeans_diagnostic = kmeans_diagnostic |&gt; \n  mutate(marginal = ratio - lag(ratio)) \n\nkmeans_diagnostic\n\n    k   between_ss tot_ss        ratio     marginal\n1   1 1.136868e-13    558 2.037399e-16           NA\n2   2 1.972975e+02    558 3.535798e-01  0.353579811\n3   3 2.672845e+02    558 4.790046e-01  0.125424742\n4   4 3.251070e+02    558 5.826290e-01  0.103624449\n5   5 3.553956e+02    558 6.369097e-01  0.054280729\n6   6 3.803925e+02    558 6.817070e-01  0.044797286\n7   7 4.081351e+02    558 7.314249e-01  0.049717918\n8   8 4.305964e+02    558 7.716782e-01  0.040253279\n9   9 4.291731e+02    558 7.691273e-01 -0.002550879\n10 10 4.493809e+02    558 8.053421e-01  0.036214812\n\n\nPlot the ratio into a scree plot\n\nplot(kmeans_diagnostic$k, kmeans_diagnostic$ratio,\n     type = \"b\",\n     ylab = \"Between SS / Total SS\",\n     xlab = \"Number of clusters\")\n\n\n\n\n\n\n\n\n\n\nPredetermined number of clusters\nIn this case we will use the K-means clustering, and define k = 3 (3 clusters).\nHere are the cluster results.\n\nkm_clust = kmeans(data_scaled, centers = 3) # k = 3\nkm_clust # print the results\n\nK-means clustering with 3 clusters of sizes 5, 16, 11\n\nCluster means:\n  Passengers  Movements Numberofairlines Mainairlineflightspercentage\n1 -0.9520429 -1.0882984       -1.3214660                    1.9931973\n2  0.7166133  0.7916255        0.8241767                   -0.4910120\n3 -0.6095999 -0.6567742       -0.5981361                   -0.1917995\n  Maximumpercentageoftrafficpercountry NumberofLCCflightsweekly\n1                            0.6473374               -1.2772631\n2                           -0.5857676                0.7739065\n3                            0.5577814               -0.5451081\n  NumberofLowCostAirlines LowCostAirlinespercentage Destinations\n1             -1.39205703                 1.9302799   -1.4105623\n2              0.38513206                -0.6868110    0.7534855\n3              0.07256111                 0.1215978   -0.4548142\n  Average_Route_Distance DistancetoclosestAirport\n1             -0.9635706                0.2765206\n2              0.6283075               -0.4268811\n3             -0.4759152                0.4952267\n  DistancetoclosestSimilarAirport AirportRegionalrelevance Distancetocitykm\n1                     -0.71705981                -1.236449        1.6976272\n2                      0.19264710                 0.227674       -0.2456769\n3                      0.04572232                 0.230860       -0.4143005\n  Inhanbitantscorrected numberofvisitorscorrected GDPcorrected   Cargoton\n1            -0.9802943                -0.7587213   -0.3558915 -0.4142745\n2             0.6365147                 0.5857249    0.5626763  0.4289738\n3            -0.4802512                -0.5070902   -0.6566693 -0.4356553\n\nClustering vector:\nNCE CGN LPA ALC LTN WAW FAO OPO STN CPH MAN VIE OSL DUS MXP BRU LIS LHR CDG FRA \n  3   3   3   3   1   3   3   3   3   2   2   2   3   2   2   2   3   2   2   2 \nMAD AMS FCO MUC LGW BCN NYO GRO ORY BSL KUN BVA \n  2   2   2   2   2   2   1   1   2   3   1   1 \n\nWithin cluster sum of squares by cluster:\n[1]  41.82151 166.93358  81.96037\n (between_SS / total_SS =  47.9 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nIf we want to export the cluster means for each variable and the cluster membership for each observation:\n\n# cluster means for each variable\nvar_cluster_means = data.frame(cluster = 1:nrow(km_clust$centers),\n                               size = km_clust$size,\n                               km_clust$centers)\n\n# cluster membership for each observation\nobs_cluster_member = data.frame(km_clust$cluster)\n\n\n\nPlotting the clusters\nFinally, plot again the Numberofairlines against the Destinations and observe the clusters results to check if they make sense.\n\n# add cluster membership to original data\ndata_clust = data |&gt;  mutate(cluster = factor(km_clust$cluster))\n\n# plot\nggplot(data_clust, aes(x = Destinations, y = Numberofairlines, color = cluster)) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_text(aes(label = Airport), vjust = 1.5, size = 3, show.legend = FALSE) +\n  scale_color_brewer(palette = \"Set1\") +\n  labs(\n    title = \"Airports clustered by k-means\",\n    x = \"Number of destinations\",\n    y = \"Number of airlines\",\n    color = \"Cluster\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat if… ?\n\n\n\nImagine that one of the airports was not operating any more.\nRemove one airport from the data, at your choice, and re-run the cluster analysis.\nHow different are the results? 🤔\n\n\n\n\n\n\n\n\nLegendre, Pierre, and Louis Legendre. 2012. “Chapter 8 - Cluster Analysis.” In Numerical Ecology, edited by Pierre Legendre and Louis Legendre, 3rd ed., 24:337–424. Developments in Environmental Modelling. Elsevier. https://doi.org/https://doi.org/10.1016/B978-0-444-53868-0.50008-3.\n\n\nR Core Team. 2025. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.",
    "crumbs": [
      "**Data and Models**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "spatial-data.html",
    "href": "spatial-data.html",
    "title": "9  Introduction to spatial data",
    "section": "",
    "text": "9.1 Import vector data\nSpatial data is data that is associated with a geometry. This geometry can be a point, a line, a polygon, or a grid.\nSpatial data can be represented in many ways, such as vector data and raster data. In this tutorial, we will learn how to work with spatial data in R.\nWe will use the sf package to work with vector data, and the dplyr package to manipulate data.\nThe sf package is a powerful package for working with spatial data in R. It includes hundreds of functions to deal with spatial data (Pebesma and Bivand 2023).\nDownload and open Municipalities_geo.gpkg under EITcourse/data repository.\nWithin the sf package, we use the st_read() to read spatial features.\nMunicipalities_geo = st_read(\"data/Municipalities_geo.gpkg\")",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to spatial data</span>"
    ]
  },
  {
    "objectID": "spatial-data.html#import-vector-data",
    "href": "spatial-data.html#import-vector-data",
    "title": "9  Introduction to spatial data",
    "section": "",
    "text": "You can also open directly from url from github. Example:\nurl = \"https://github.com/U-Shift/EITcourse/raw/main/data/Municipalities_geo.gpkg\" Municipalities_geo = st_read(url)\n\n\n\n\nProjected vs Geographic Coordinate Systems\nA projected coordinate system is a flat representation of the Earth’s surface. A geographic coordinate system is a spherical representation of the Earth’s surface.\n\n\n\nSource: ESRI\n\n\nThe st_crs() function can be used to check the coordinate reference system of a spatial object.\n\nst_crs(Municipalities_geo)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        MEMBER[\"World Geodetic System 1984 (G2296)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\nWGS84 is the most common geographic coordinate system, used in GPS, and EPSG:4326 is code for it.\nIf we want to project the data to a projected coordinate system, to use metric units instead of degrees, we can use the st_transform() function.\nIn this case, the EPGS:3857 is the code for the Pseudo-Mercator coordinate system.\n\nMunicipalities_projected = st_transform(Municipalities_geo, crs = 3857)\n\nNow see the differences when calling Municipalities_geo and Municipalities_projected.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to spatial data</span>"
    ]
  },
  {
    "objectID": "spatial-data.html#join-geometries-to-data-frames",
    "href": "spatial-data.html#join-geometries-to-data-frames",
    "title": "9  Introduction to spatial data",
    "section": "9.2 Join geometries to data frames",
    "text": "9.2 Join geometries to data frames\nImport TRIPSmun.Rds file and check data class\n\nTRIPSmun = readRDS(\"../data/TRIPSmun.Rds\")\nclass(TRIPSmun)\n\n[1] \"data.frame\"\n\nclass(Municipalities_geo)\n\n[1] \"sf\"         \"data.frame\"\n\n\nTo join the geometries from the Municipalities_geo to the data frame, we can use the left_join() function from the dplyr package.\n\nTRIPSgeo =\n  TRIPSmun |&gt; \n  left_join(Municipalities_geo)\n\nclass(TRIPSgeo)\n\n[1] \"data.frame\"\n\n\nAs you can see, this does not make the object a spatial feature. To do this, we need to use the st_as_sf() function.\n\nTRIPSgeo = TRIPSgeo |&gt; st_as_sf()\nclass(TRIPSgeo)\n\n[1] \"sf\"         \"data.frame\"\n\n\nNow we have a spatial feature with the data frame.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to spatial data</span>"
    ]
  },
  {
    "objectID": "spatial-data.html#create-spatial-data-from-coordinates",
    "href": "spatial-data.html#create-spatial-data-from-coordinates",
    "title": "9  Introduction to spatial data",
    "section": "9.3 Create spatial data from coordinates",
    "text": "9.3 Create spatial data from coordinates\nThe st_as_sf() function can also be used to create a spatial feature from a data frame with coordinates. In that case, we need to specify the columns with the coordinates.\nWe will use survey data (in .txt) with the participants’ home latitude/longitude coordinates to create a spatial feature.\n\nSURVEY = read.csv(\"../data/SURVEY.txt\", sep = \"\\t\") # tab delimiter\nclass(SURVEY)\n\n[1] \"data.frame\"\n\nSURVEYgeo = st_as_sf(SURVEY, coords = c(\"lon\", \"lat\"), crs = 4326) # create spatial feature\nclass(SURVEYgeo)\n\n[1] \"sf\"         \"data.frame\"\n\n\nWe can also set the crs of the spatial feature on the fly.\nCheck the differences between both data variables.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to spatial data</span>"
    ]
  },
  {
    "objectID": "spatial-data.html#visuzlize-spatial-data",
    "href": "spatial-data.html#visuzlize-spatial-data",
    "title": "9  Introduction to spatial data",
    "section": "9.4 Visuzlize spatial data",
    "text": "9.4 Visuzlize spatial data\nRepresent Transport Zones with Total and Car, using plot().\n\nplot(TRIPSgeo) # all variables\n\n\n\n\n\n\n\nplot(TRIPSgeo[\"Municipality\"])\n\n\n\n\n\n\n\nplot(TRIPSgeo[\"Total\"])\n\n\n\n\n\n\n\nplot(TRIPSgeo[\"Car\"])\n\n\n\n\n\n\n\n# plot pointy data\nplot(SURVEYgeo)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the next chapter we will learn how to create interactive maps.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to spatial data</span>"
    ]
  },
  {
    "objectID": "spatial-data.html#export-spatial-data",
    "href": "spatial-data.html#export-spatial-data",
    "title": "9  Introduction to spatial data",
    "section": "9.5 Export spatial data",
    "text": "9.5 Export spatial data\nYou can save your spatial data in different formats using the function st_write(), such as shapefiles (ESRI), GeoJSON, and GeoPackage.\nThis is also useful to convert spatial data between formats.\n\nst_write(TRIPSgeo, \"data/TRIPSgeo.gpkg\") # as geopackage\nst_write(TRIPSgeo, \"data/TRIPSgeo.shp\") # as shapefile\nst_write(TRIPSgeo, \"data/TRIPSgeo.geojson\") # as geojson\nst_write(TRIPSgeo, \"data/TRIPSgeo.csv\", layer_options = \"GEOMETRY=AS_WKT\") # as csv, with WKT geometry\n\n\n\n\n\n\n\nIf you already have a file with the same name, you can use the delete_dns = TRUE argument to overwrite it.\n\n\n\n\n\n\n\n\n\nPebesma, Edzer, and Roger Bivand. 2023. Spatial Data Science: With Applications in R. Boca Raton: Chapman; Hall/CRC. https://doi.org/10.1201/9780429459016.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to spatial data</span>"
    ]
  },
  {
    "objectID": "interactive-maps.html",
    "href": "interactive-maps.html",
    "title": "10  Interactive maps",
    "section": "",
    "text": "10.1 Mapview\nYou can plot a static map using plof(sf), but you can also create interactive maps.\nInteractive maps are useful to explore the data, as you can zoom in and out, and click on the points to see the data associated with them.\nThere are several R packages to create interactive maps. For instance, the tmap package, the leaflet package, and the mapview package.\nMapview allows to create quick interactive maps, only by declaring the function mapview().\nlibrary(mapview)\nmapview(TRIPSgeo)\nTo color the points by a variable, you can use the zcol argument.\nmapview(TRIPSgeo, zcol = \"Total\")\nAs you can see, a color palette is automatically assigned to the continuous variable.\nTry to use a categorical variable.\nCode\nmapview(TRIPSgeo,\n        zcol = \"Municipality\", # depending on the variable\n        alpha.regions = 0.4, # also add transparency\n        color = \"white\" # border color\n        )\nYou can go crazy with all the options that mapview offers. Please refer to the documentation to see all the options.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Interactive maps</span>"
    ]
  },
  {
    "objectID": "interactive-maps.html#mapview",
    "href": "interactive-maps.html#mapview",
    "title": "10  Interactive maps",
    "section": "",
    "text": "Note that you can change the basemap, and click on the geometries to see the data associated with them.\n\n\n\n\n\n\nExport\nYou can directly export the map as an html file or image, using the Viewer panel.\n\n\n\n\n\n\n\nThis is the most straightforward solution.\n\n\n\nYou can also export a map as an html file or image using code.\n\n# install.packages(\"webshot2\") # you will need this\n\nmap = mapview(TRIPSgeo, zcol = \"Total\") # first create an objet with the desired map\n\nmapshot2(map, \"data/map.html\") # as webpage\nmapshot2(map, file = \"data/map.png\") # as image",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Interactive maps</span>"
    ]
  },
  {
    "objectID": "interactive-maps.html#rmarkdown",
    "href": "interactive-maps.html#rmarkdown",
    "title": "10  Interactive maps",
    "section": "10.2 Rmarkdown",
    "text": "10.2 Rmarkdown\nTo include a map on a report, website, paper (any type), you can create an Rmarkdown file.\n\nAnd include a R code chunk (ctrl + alt + i) with a map. If the output is html, you will get an interactive map on your document!",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Interactive maps</span>"
    ]
  },
  {
    "objectID": "interactive-maps.html#flowmap-blue",
    "href": "interactive-maps.html#flowmap-blue",
    "title": "10  Interactive maps",
    "section": "10.3 Flowmap blue",
    "text": "10.3 Flowmap blue\nA way to visualize traffic or travel volumes between areas (OD pairs) is through the online tool flowmap.blue.\nBut you need to prepare the data so that they are exactly in the requested format. (See the used source code).\nYou need two files:\n\nlocations\nflows\n\nTry to copy and paste the content of these files in the corresponding fields the information about car trips in the Lisbon metropolitan area according to INE (2018).\n\n\n\n\n\n\n\nTry it yourself\n\n\n\nVisualize the public transit trips in the Lisbon metropolitan area. (flows_tp)\n\nWhich differences do you see between car and public transit trips?\nHow many trips are made daily in public transit with origin or destination in Sintra?\n\n\n\n\n\n\n\n\n\nINE. 2018. “Mobilidade e Funcionalidade Do Território Nas Áreas Metropolitanas do Porto e de Lisboa: 2017.” Lisboa: Instituto National de Estatística. https://www.ine.pt/xportal/xmain?xpid=INE&xpgid=ine_publicacoes&PUBLICACOESpub_boui=349495406&PUBLICACOESmodo=2&xlang=pt.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Interactive maps</span>"
    ]
  },
  {
    "objectID": "centroids.html",
    "href": "centroids.html",
    "title": "11  Centroids of transport zones",
    "section": "",
    "text": "11.1 Geometric centroids\nIn this section we will calculate the geometric and the weighted centroids of transport zones.\nTaking the Municipalities_geo data from the previous section, we will calculate the geometric centroids, using the st_centroid() function.\nCode\nlibrary(dplyr)\nlibrary(sf)\nlibrary(mapview)\n\nMunicipalities_geo = st_read(\"../data/Municipalities_geo.gpkg\", quiet = TRUE)\n\nCentroid_geo = st_centroid(Municipalities_geo)\nThis creates points at the geometric center of each polygon.\nCode\nmapview(Centroid_geo)\n\n\n\n\n\n\nCode\nmapview(Centroid_geo) + mapview(Municipalities_geo, alpha.regions = 0) # both maps, with full transparency in polygons\nBut… is this the best way to represent the center of a transport zone?\nThese results may be biased by the shape of the polygons, and not represent where activities are. Example: lakes, forests, etc.\nTo overcome this, we can use weighted centroids.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Centroids of transport zones</span>"
    ]
  },
  {
    "objectID": "centroids.html#weighted-centroids",
    "href": "centroids.html#weighted-centroids",
    "title": "11  Centroids of transport zones",
    "section": "11.2 Weighted centroids",
    "text": "11.2 Weighted centroids\nWe will weight centroids of transport zones by population and by number of buildings.\nFor this, we will need the census data (INE 2022).\n\n\nCode\nCensus = st_read(\"../data/census.gpkg\", quiet = TRUE)\n\nmapview(Census |&gt; filter(Municipality == \"Lisboa\"), zcol = \"Population\")\n\n\n\n\n\n\nIt was not that easy to estimate weighted centroids with R, as it is with GIS software. But there is this new package centr that can help us (Zomorrodi 2024).\nWe need to specify the group we want to calculate the mean centroids, and the weight variable we want to use.\n\n\nCode\nlibrary(centr)\nCentroid_pop = Census |&gt; \n  mean_center(group = \"Municipality\", weight = \"Population\")\n\n\nWe can do the same for buildings.\n\n\nCode\nCentroid_build = Census |&gt; \n  mean_center(group = \"Municipality\", weight = \"Buildings\")",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Centroids of transport zones</span>"
    ]
  },
  {
    "objectID": "centroids.html#compare-centroids-in-a-map",
    "href": "centroids.html#compare-centroids-in-a-map",
    "title": "11  Centroids of transport zones",
    "section": "11.3 Compare centroids in a map",
    "text": "11.3 Compare centroids in a map\n\n11.3.1 Interactive map\n\n\nCode\nmapview(Centroid_geo, col.region = \"blue\") +\n  mapview(Centroid_pop, col.region = \"red\") +\n  mapview(Centroid_build, col.region = \"black\") +\n  mapview(Municipalities_geo, alpha.regions = 0) # polygon limits\n\n\n\n\n\n\nSee how the building, population and geometric centroids of Sintra are apart, from closer to Tagus, to the rural area.\n\n\n11.3.2 Static map\nTo produce the same map, using only plot() and st_geometry(), we need to make sure that the geometries have the same crs.\n\n\nCode\nst_crs(Centroid_geo) # 4326 WGS84\nst_crs(Centroid_pop) # 3763 Portugal TM06\n\n\nSo, we need to transform the geometries to the same crs.\n\n\nCode\nCentroid_pop = st_transform(Centroid_pop, crs = 4326)\nCentroid_build = st_transform(Centroid_build, crs = 4326)\n\n\nNow, to use plot() we incrementally add layers to the plot.\n\n\nCode\n# Plot the Municipalities_geo polygons first (with no fill)\nplot(st_geometry(Municipalities_geo), col = NA, border = \"black\")\n\n# Add the Centroids_geo points in blue\nplot(st_geometry(Centroid_geo), col = \"blue\", pch = 16, add = TRUE) # add!\n\n# Add the Centroid_pop points in red\nplot(st_geometry(Centroid_pop), col = \"red\", pch = 16, add = TRUE)\n\n# Add the Centroid_build points in black\nplot(st_geometry(Centroid_build), col = \"black\", pch = 16, add = TRUE)\n\n\n\n\n\nStatic map of different centroids of Municipalities\n\n\n\n\nIn the next section we will use these centroids to calculate the desire lines between them.\n\n\n\n\n\n\nINE. 2022. “Censos 2021- XVI Recenseamento Geral da População. VI Recenseamento Geral da Habitação.” Lisboa: Instituto National de Estatística. https://censos.ine.pt/xurl/pub/65586079.\n\n\nZomorrodi, Ryan. 2024. Centr: Weighted and Unweighted Spatial Centers. https://ryanzomorrodi.github.io/centr/.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Centroids of transport zones</span>"
    ]
  },
  {
    "objectID": "desire-lines.html",
    "href": "desire-lines.html",
    "title": "12  OD pairs and desire lines",
    "section": "",
    "text": "12.1 Desire lines with od_to_sf\nDesire lines are a way to represent the flow of people or goods between two points. They are often used in transport planning to represent the flow of trips between zones.\nThere are many ways to create desire lines, connecting origins and destinations. For instance:\nThe stplanr package is a collection of functions for sustainable transport planning with R, and it is built on top of the sf package (Lovelace and Ellison 2018).\nIn this workshop, we will use the od package, a lightweight package with a few functions from stplanr, namely the ones to create desire lines from origin-destination (OD) pairs (Lovelace and Morgan 2024).\nTo create desire lines, we need a dataset with OD pairs and other dataset with the corresponding transport zones (spatial data).\nThe TRIPSmode.Rds dataset includes origins, destinations and number of trips between municipalities.\nCode\nTRIPSmode = readRDS(\"../data/TRIPSmode.Rds\")\nThe Municipalities_geo.gpkg dataset includes the geometry of the transport zones.\nCode\nlibrary(sf)\nMunicipalities_geo = st_read(\"../data/Municipalities_geo.gpkg\", quiet = TRUE) # supress mesage\nThen, we need to load the od package. We will use the od_to_sf() function to create desire lines from OD pairs.\nCode\n# install.packages(\"od\")\nlibrary(od)\n\nTRIPSdlines = od_to_sf(TRIPSmode, z = Municipalities_geo) # z for zones\nFor this magic to work smoothly, the first two columns of the TRIPSmode dataset must be the origin and destination zones, and these zones need to correspond to the first column of the Municipalities_geo dataset (with an associated geometry).\nNow we can visualize the desire lines using the mapview function.\nCode\nlibrary(mapview)\nmapview(TRIPSdlines, zcol = \"Total\")\nAs you can see, this is too much information to be able to understand the flows.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>OD pairs and desire lines</span>"
    ]
  },
  {
    "objectID": "desire-lines.html#desire-lines-with-od_to_sf",
    "href": "desire-lines.html#desire-lines-with-od_to_sf",
    "title": "12  OD pairs and desire lines",
    "section": "",
    "text": "See more options with the ?stplanr::od2line function.\n\n\n\n\n\n\n\n12.1.1 Filtering desire lines\nFilter intrazonal trips.\n\n\nCode\nlibrary(dplyr)\n\nTRIPSdlines_inter = TRIPSdlines |&gt; \n  filter(Origin != Destination) |&gt; # remove intrazonal trips\n  filter(Total &gt; 5000) # remove noise\n\nmapview(TRIPSdlines_inter, zcol = \"Total\", lwd = 5)\n\n\n\n\n\n\nFilter trips with origin or destination not in Lisbon.\n\n\nCode\nTRIPSdl_noLX = TRIPSdlines_inter |&gt; \n  filter(Origin != \"Lisboa\", Destination != \"Lisboa\")\n\nmapview(TRIPSdl_noLX, zcol = \"Total\", lwd = 8) # larger line width\n\n\n\n\n\n\nTry to replace the Total with other variables, such as Car, PTransit, and see the differences.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>OD pairs and desire lines</span>"
    ]
  },
  {
    "objectID": "desire-lines.html#oneway-desire-lines",
    "href": "desire-lines.html#oneway-desire-lines",
    "title": "12  OD pairs and desire lines",
    "section": "12.2 Oneway desire lines",
    "text": "12.2 Oneway desire lines\nNote that the od_to_sf() function creates bidirectional desire lines. This can be not the ideal for visualization / representation purposes, as you will have 2 lines overlaping.\nThe function od_oneway() aggregates oneway lines to produce bidirectional flows.\nBy default, it returns the sum of each numeric column for each bidirectional origin-destination pair.\n\n\nCode\nnrow(TRIPSdlines)\n\n\n[1] 315\n\n\nCode\nTRIPSdlines_oneway = od_oneway(TRIPSdlines) |&gt; \n  filter(o != d) # remove empty geometries\nnrow(TRIPSdlines_oneway)\n\n\n[1] 150\n\n\nNote that for the last municipalities you have less combinations now. Nevertheless, all the possible combinations are represented.\n\n\nCode\nhead(TRIPSdlines_oneway[,c(1,2)]) # just the first 2 columns\n\n\nSimple feature collection with 6 features and 2 fields\nAttribute-geometry relationships: identity (2)\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -9.229502 ymin: 38.62842 xmax: -8.915878 ymax: 38.75981\nGeodetic CRS:  WGS 84\n          o        d                       geometry\n1 Alcochete   Almada LINESTRING (-8.915878 38.73...\n2 Alcochete  Amadora LINESTRING (-8.915878 38.73...\n3    Almada  Amadora LINESTRING (-9.193061 38.63...\n4 Alcochete Barreiro LINESTRING (-8.915878 38.73...\n5    Almada Barreiro LINESTRING (-9.193061 38.63...\n6   Amadora Barreiro LINESTRING (-9.229502 38.75...\n\n\nCode\ntail(TRIPSdlines_oneway[,c(1,2)])\n\n\nSimple feature collection with 6 features and 2 fields\nAttribute-geometry relationships: identity (2)\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -9.357651 ymin: 38.49491 xmax: -8.80664 ymax: 38.92208\nGeodetic CRS:  WGS 84\n           o                   d                       geometry\n145   Oeiras Vila Franca de Xira LINESTRING (-9.276317 38.71...\n146  Palmela Vila Franca de Xira LINESTRING (-8.80664 38.617...\n147   Seixal Vila Franca de Xira LINESTRING (-9.108785 38.60...\n148 Sesimbra Vila Franca de Xira LINESTRING (-9.120129 38.49...\n149  Setúbal Vila Franca de Xira LINESTRING (-8.887489 38.51...\n150   Sintra Vila Franca de Xira LINESTRING (-9.357651 38.82...\n\n\nExample of visualization with Public Transit trips in both ways.\n\n\nCode\nTRIPSdlines_oneway_noLX = TRIPSdlines_oneway |&gt; \n  filter(PTransit &gt; 5000) # reduce noise\n\nmapview(TRIPSdlines_oneway_noLX, zcol = \"PTransit\", lwd = 8)",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>OD pairs and desire lines</span>"
    ]
  },
  {
    "objectID": "desire-lines.html#using-population-centroids",
    "href": "desire-lines.html#using-population-centroids",
    "title": "12  OD pairs and desire lines",
    "section": "12.3 Using population centroids",
    "text": "12.3 Using population centroids\nThe od_to_sf() function uses the geometric center of the zones to create the desire lines. But if we replace those zones by the weighted centroids, we can have a more realistic representation of the flows.\n\n\nCode\n# Centroid_pop = st_read(\"data/Centroid_pop.gpkg\")\n\nTRIPSdlines_pop = od_to_sf(TRIPSmode, z = Centroid_pop) |&gt;  # works the same way\n  od_oneway() |&gt; # oneway\n  filter(o != d) # remove empty geometries\n\n\nCheck differences of lines with trips from/to Lisbon:\n\n\nCode\nTRIPSdlines_geo_LX = TRIPSdlines_oneway |&gt; \n  filter(o == \"Lisboa\" | d == \"Lisboa\") # or condition\nTRIPSdlines_pop_LX = TRIPSdlines_pop |&gt; \n  filter(o == \"Lisboa\" | d == \"Lisboa\")\n\nmapview(TRIPSdlines_geo_LX, color = \"blue\") + mapview(TRIPSdlines_pop_LX, color = \"red\")\n\n\n\n\n\n\nThe next step will be estimating the euclidean distances between these centroids, and compare them with the routing distances.\n\n\n\n\n\n\nLovelace, Robin, and Richard Ellison. 2018. “Stplanr: A Package for Transport Planning.” The R Journal 10 (2): 10. https://doi.org/10.32614/RJ-2018-053.\n\n\nLovelace, Robin, Rosa Félix, and Dustin Carlino. 2022. “Jittering: A Computationally Efficient Method for Generating Realistic Route Networks from Origin-Destination Data.” Findings, April. https://doi.org/10.32866/001c.33873.\n\n\nLovelace, Robin, and Malcolm Morgan. 2024. Od: Manipulate and Map Origin-Destination Data. https://github.com/itsleeds/od.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>OD pairs and desire lines</span>"
    ]
  },
  {
    "objectID": "desire-lines.html#footnotes",
    "href": "desire-lines.html#footnotes",
    "title": "12  OD pairs and desire lines",
    "section": "",
    "text": "See (Lovelace, Félix, and Carlino 2022).↩︎",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>OD pairs and desire lines</span>"
    ]
  },
  {
    "objectID": "distances.html",
    "href": "distances.html",
    "title": "13  Euclidean and routing distances",
    "section": "",
    "text": "13.1 Euclidean distances\nWe will show how to estimate euclidean distances (as crown flights) using sf package, and the distances using a road network using openrouteservice package.\nTaking the survey respondents’ location, we will estimate the distance to the university (IST) using the sf package.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Euclidean and routing distances</span>"
    ]
  },
  {
    "objectID": "distances.html#euclidean-distances",
    "href": "distances.html#euclidean-distances",
    "title": "13  Euclidean and routing distances",
    "section": "",
    "text": "13.1.1 Import survey data frame convert to sf\nWe will use a survey dataset with 200 observations, with the following variables: ID, Affiliation, Age, Sex, Transport Mode to IST, and latitude and longitude coordinates.\n\nlibrary(dplyr)\n\nSURVEY = read.csv(\"../data/SURVEY.txt\", sep = \"\\t\") # tab delimiter\nnames(SURVEY)\n\n[1] \"ID\"   \"AFF\"  \"AGE\"  \"SEX\"  \"MODE\" \"lat\"  \"lon\" \n\n\nAs we have the coordinates, we can convert this data frame to a spatial feature, as explained in the Introduction to spatial data section.\n\nlibrary(sf)\n\nSURVEYgeo = st_as_sf(SURVEY, coords = c(\"lon\", \"lat\"), crs = 4326) # convert to as sf data\n\n\n\n13.1.2 Create new point at the university\nUsing coordinates from Instituto Superior Técnico, we can directly create a simple feature and assign its crs.\n\nUNIVERSITY = data.frame(place = \"IST\",\n                        lon = -9.1397404,\n                        lat = 38.7370168) |&gt;  # first a dataframe\n  st_as_sf(coords = c(\"lon\", \"lat\"), # then a spacial feature\n           crs = 4326)\n\nVisualize in a map:\n\nlibrary(mapview)\nmapview(SURVEYgeo, zcol = \"MODE\") + mapview(UNIVERSITY, col.region = \"red\", cex = 12)\n\n\n\n\n\n\n\n13.1.3 Straight lines\nFirst we will create lines connecting the survey locations to the university, using the st_nearest_points() function.\nThis function finds returns the nearest points between two geometries, and creates a line between them. This can be useful to find the nearest train station to each point, for instance.\nAs we only have 1 point at UNIVERSITY layer, we will have the same number of lines as number of surveys = 200.\n\nSURVEYeuclidean = st_nearest_points(SURVEYgeo, UNIVERSITY, pairwise = TRUE) |&gt;\n  st_as_sf() # this creates lines\n\nmapview(SURVEYeuclidean)\n\n\n\n\n\nNote that if we have more than one point in the second layer, the pairwise = TRUE will create a line for each combination of points. Set to FALSE if, for instance, you have the same number of points in both layers and want to create a line between the corresponding points.\n\n\n13.1.4 Distance\nNow we can estimate the distance using the st_length() function.\n\n# compute the line length and add directly in the first survey layer\nSURVEYgeo = SURVEYgeo |&gt; \n  mutate(distance = st_length(SURVEYeuclidean))\n\n# remove the units - can be useful\nSURVEYgeo$distance = units::drop_units(SURVEYgeo$distance) \n\nWe could also estimate the distance using the st_distance() function directly, although we would not get and sf with lines.\n\nSURVEYgeo = SURVEYgeo |&gt; \n  mutate(distance = st_distance(SURVEYgeo, UNIVERSITY)[,1] |&gt;  # in meters\n           units::drop_units()) |&gt;  # remove units\n  mutate(distance = round(distance)) # round to integer\n\nsummary(SURVEYgeo$distance)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    298    1106    2186    2658    3683    8600 \n\n\nSURVEYgeo is still a points’ sf.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Euclidean and routing distances</span>"
    ]
  },
  {
    "objectID": "distances.html#routing-engines",
    "href": "distances.html#routing-engines",
    "title": "13  Euclidean and routing distances",
    "section": "13.2 Routing Engines",
    "text": "13.2 Routing Engines\nThere are different types of routing engines, regarding the type of network they use, the type of transportation they consider, and the type of data they need. We can have:\n\nUni-modal vs. Multi-modal\n\nOne mode per trip vs. One trip with multiple legs that can be made with different modes\nMulti-modal routing may require GTFS data (realistic Public Transit)\n\nOutput level of the results\n\nRoutes (1 journey = 1 route)\nLegs\nSegments\n\nRouting profiles\n\nType of user\nfastest / shortest path\navoid barriers / tolls, etc\n\n\n\n\n\nRouting options in OpenRouteService\n\n\n\nLocal vs. Remote (service request - usually web API)\n\nSpeed vs. Quota limits / price\nHard vs. Easy set up\nHardware limitations in local routing\nGlobal coverage in remote routing, with frequent updates\n\n\nExamples: OSRM, Dodgr, r5r, Googleway, CycleStreets, HERE.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Euclidean and routing distances</span>"
    ]
  },
  {
    "objectID": "distances.html#routing-distances-with-openrouteservice",
    "href": "distances.html#routing-distances-with-openrouteservice",
    "title": "13  Euclidean and routing distances",
    "section": "13.3 Routing distances with openrouteservice",
    "text": "13.3 Routing distances with openrouteservice\nWe will use the openrouteservice r package to estimate the distances using a road network (Oleś 2025).\n\n\n\n\n\n\nTo properly use the openrouteservice package, you need to setup the ORS provider. See the setup instructions for more details.\n\n\n\nWe will use only respondents with a distance to the university less than 2 km.\n\nSURVEYsample = SURVEYgeo |&gt; filter(distance &lt;= 2000)\nnrow(SURVEYsample)\n\n[1] 95\n\n\nWe need an id (unique identifier) for each survey location, so we can compare them later.\nAlso, we need to extract the coordinates, from both datasets, to be used in the routing functions of openrouteservice::directions().\n\n# create id columns for both datasets\nSURVEYsample = SURVEYsample |&gt;\n  mutate(id = c(1:nrow(SURVEYsample))) |&gt;  # from 1 to the number of rows\n  mutate(coordinates = st_coordinates(geometry)) # extract coordinates\n\nUNIVERSITY = UNIVERSITY |&gt;\n  mutate(id = 1) |&gt; # only one row\n  mutate(coordinates = st_coordinates(geometry)) # extract coordinates\n\n\n13.3.1 Distances by car\nEstimate the routes with time and distance by car, from survey locations to University.\n\n\n\n\n\n\nThis one is not that easy to set-up because the function is prepared to retrieve only one result per request :( So we do a loop function.\n\n\n\n\nlibrary(openrouteservice)\n\nSURVEYcar = data.frame() # initial empty data frame\n\n# loop - the origin (i) is the survey location, and the UNIVERSITY is always the same destination\nfor (i in 1:nrow(SURVEYsample)) {\n  ROUTES1 = ors_directions(\n    data.frame(\n      lon = c(SURVEYsample$coordinates[i, 1], UNIVERSITY$coordinates[1, 1]),\n      lat = c(SURVEYsample$coordinates[i, 2], UNIVERSITY$coordinates[1, 2])\n    ),\n    profile = \"driving-car\", # or cycling-regular foot-walking\n    preference = \"fastest\", # or shortest\n    output = \"sf\"\n  )\n  ROUTES1$distance = ROUTES1$summary[[1]]$distance # extract these values from summary\n  ROUTES1$duration = ROUTES1$summary[[1]]$duration\n  \n  SURVEYcar = rbind(SURVEYcar, ROUTES1) # to keep adding in the same df\n}\n\nSURVEYcar = SURVEYcar |&gt;\n  select(distance, duration, geometry) |&gt; # discard unnecessary variables\n  mutate(ID = SURVEYsample$ID) # cbind with syrvey ID\n\n\n\n13.3.2 Distances by foot\nRepeat the same for foot-walking.\n\n\nCode\nSURVEYwalk = data.frame() # initial empty data frame\n\n# loop - the origin (i) is the survey location, and the UNIVERSITY is always the same destination\nfor (i in 1:nrow(SURVEYsample)) {\n  ROUTES1 = ors_directions(\n    data.frame(\n      lon = c(SURVEYsample$coordinates[i, 1], UNIVERSITY$coordinates[1, 1]),\n      lat = c(SURVEYsample$coordinates[i, 2], UNIVERSITY$coordinates[1, 2])\n    ),\n    profile = \"foot-walking\", # or driving-car cycling-regular cycling-electric\n    preference = \"fastest\", # or shortest\n    output = \"sf\"\n  )\n  ROUTES1$distance = ROUTES1$summary[[1]]$distance # extract these values from summary\n  ROUTES1$duration = ROUTES1$summary[[1]]$duration\n  \n  SURVEYwalk = rbind(SURVEYwalk, ROUTES1) # to keep adding in the same df\n}\n\nSURVEYwalk = SURVEYwalk |&gt;\n  select(distance, duration, geometry) |&gt; # discard unnecessary variables\n  mutate(ID = SURVEYsample$ID) # cbind with survey ID\n\n\n\n\n13.3.3 Distances by bike\nAnd for cycling-regular.\n\n\nCode\nSURVEYbike = data.frame() # initial empty data frame\n\n# loop - the origin (i) is the survey location, and the UNIVERSITY is always the same destination\nfor (i in 1:nrow(SURVEYsample)) {\n  ROUTES1 = ors_directions(\n    data.frame(\n      lon = c(SURVEYsample$coordinates[i, 1], UNIVERSITY$coordinates[1, 1]),\n      lat = c(SURVEYsample$coordinates[i, 2], UNIVERSITY$coordinates[1, 2])\n    ),\n    profile = \"cycling-regular\", \n    preference = \"fastest\", # or shortest\n    output = \"sf\"\n  )\n  ROUTES1$distance = ROUTES1$summary[[1]]$distance # extract these values from summary\n  ROUTES1$duration = ROUTES1$summary[[1]]$duration\n  \n  SURVEYbike = rbind(SURVEYbike, ROUTES1) # to keep adding in the same df\n}\n\nSURVEYbike = SURVEYbike |&gt;\n  select(distance, duration, geometry) |&gt; # discard unnecessary variables\n  mutate(ID = SURVEYsample$ID) # cbind with survey ID\n\n\n\nnames(SURVEYcar)\n\n[1] \"distance\" \"duration\" \"ID\"       \"geometry\"\n\n\n\n\n\n\n\n\nIf we want to know only time and distance, and not the route itself, we can use the ors_matrix().",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Euclidean and routing distances</span>"
    ]
  },
  {
    "objectID": "distances.html#compare",
    "href": "distances.html#compare",
    "title": "13  Euclidean and routing distances",
    "section": "13.4 Compare distances",
    "text": "13.4 Compare distances\nWe can now compare the euclidean and routing distances that we estimated for the survey locations under 2 km.\n\nsummary(SURVEYsample$distance) # Euclidean\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    298     790    1046    1112    1470    1963 \n\nsummary(SURVEYwalk$distance) # Walk\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  467.3  1009.0  1447.3  1471.8  1953.5  2769.4 \n\nsummary(SURVEYcar$distance) # Car\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  435.4  1227.0  1686.7  1771.4  2210.3  3503.2 \n\nsummary(SURVEYbike$distance) # Bike\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    436    1247    1752    1791    2225    3758 \n\n\n\nWhat can you understand from this results?\n\n\n\n\n\n\n\n\n\n\n\n13.4.1 Circuity\nCompare 1 single route.\n\n\nCode\nmapview(SURVEYeuclidean[165,], color = \"black\") + # 1556 meters\n  mapview(SURVEYwalk[78,], color = \"red\") + # 2126 meters\n  mapview(SURVEYcar[78,], color = \"blue\") + # 2058 meters\n  mapview(SURVEYbike[78,], color = \"gold\") # 2025 meters\n\n\n\n\n\n\nWith this we can see the circuity of the routes, a measure of route / transportation efficiency, which is the ratio between the routing distance and the euclidean distance.\nThe circuity for car (1.32) is usually lower than for walking (1.37) or biking, for longer distances, and higher opposite for shorter distances.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Euclidean and routing distances</span>"
    ]
  },
  {
    "objectID": "distances.html#visualize-routes",
    "href": "distances.html#visualize-routes",
    "title": "13  Euclidean and routing distances",
    "section": "13.5 Visualize routes",
    "text": "13.5 Visualize routes\nVisualize with transparency of 30%, to get a clue when they overlay.\n\nmapview(SURVEYwalk, alpha = 0.3)\n\n\n\n\nmapview(SURVEYcar, alpha = 0.3, color = \"red\")\n\n\n\n\nmapview(SURVEYbike, alpha = 0.3, color = \"darkgreen\")\n\n\n\n\n\nWe can also use the overline() function from stplanr package to break up the routes when they overline, and add them up.\n\n# we create a value that we can later sum\n# it can be the number of trips represented by this route\nSURVEYwalk$trips = 1 # in this case is only one respondent per route\n\nSURVEYwalk_overline = stplanr::overline(\n  SURVEYwalk,\n  attrib = \"trips\",\n  fun = sum\n)\n\nmapview(SURVEYwalk_overline, zcol = \"trips\", lwd = 3)\n\n\n\n\n\nWith this we can visually inform on how many people travel along a route, from the survey dataset1.\n\n\n\n\n\n\nQuestions\n\n\n\n\nHow many people are entering IST by the stairs near Bar de Civil?\nAnd by the North gate?\nAnd from Alameda stairs?\n\n\n\n\n\n\n\n\n\nOleś, Andrzej K. 2025. Openrouteservice: An ’Openrouteservice’ API Client. https://github.com/GIScience/openrouteservice-r.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Euclidean and routing distances</span>"
    ]
  },
  {
    "objectID": "distances.html#footnotes",
    "href": "distances.html#footnotes",
    "title": "13  Euclidean and routing distances",
    "section": "",
    "text": "Assuming all travel by the shortest path.↩︎",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Euclidean and routing distances</span>"
    ]
  },
  {
    "objectID": "buffers.html",
    "href": "buffers.html",
    "title": "14  Buffers vs. Isochones",
    "section": "",
    "text": "14.1 Buffer\nWe will use the stplnar package to create buffers in meters, and the openrouteservice package to create isochrones.\nRepresent a buffer of 500 m and 2000 m from IST1.\nIST = st_sfc(st_point(c(-9.1397404, 38.7370168)), crs = 4326) # create a point\nIST$coordinates = st_coordinates(IST)\n\n# BUFFERist500 = st_buffer(IST, dist = 500) # non  projected - results may be weird\nBUFFERist500 = geo_buffer(IST[1], dist = 500) # from stplnar, to make sure it is in meters.\nBUFFERist2000 = geo_buffer(IST[1], dist = 2000)\n\nmapview(BUFFERist500) + mapview(BUFFERist2000, alpha.regions = 0.5)",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Buffers vs. Isochones</span>"
    ]
  },
  {
    "objectID": "buffers.html#isochrone",
    "href": "buffers.html#isochrone",
    "title": "14  Buffers vs. Isochones",
    "section": "14.2 Isochrone",
    "text": "14.2 Isochrone\n\nIsochrone from 1 point - distance\nWe use again the openrouteservice r package.\n\nISOCist = ors_isochrones(\n  IST$coordinates,\n  profile = \"foot-walking\",\n  range_type = \"distance\", # or time\n  range = c(500, 1000, 2000),\n  output = \"sf\"\n)\n\nISOCist = arrange(ISOCist, -value) |&gt;  # to make the larger polygons on top of the table so the are displayed behind.\n          select(-center) # unnecessary variable\n\nmapview(ISOCist, zcol = \"value\", alpha.regions = 0.5)\n\n\n\n\n\nAs you can see, the distance buffer of 500m is larger than the isochrone of 500m. Actually we can measure their area of reach.\n\nISOCist$area = st_area(ISOCist)\nBUFFERist500$area = st_area(BUFFERist500)\nBUFFERist2000$area = st_area(BUFFERist2000)\n\nratio1 = BUFFERist500$area / ISOCist$area[ISOCist$value == 500] # 1.71\nratio2 = BUFFERist2000$area / ISOCist$area[ISOCist$value == 2000] # 1.22\n\nThe euclidean buffer of 500m is 1.7 times larger than its isochrone, and the buffer of 2000m is 1.23 times larger than its isochrone.\n\n\nIsochrone from more than 1 point - time\nFor this purpose we will use the high schools dataset.\n\n# import schools\nSCHOOLS = st_read(\"../geo/SCHOOLS_basicsec.gpkg\", quiet = TRUE)\n\nSCHOOLS$coordinates = st_coordinates(SCHOOLS) # create coordinate variable\n\nSCHOOLShigh = SCHOOLS |&gt;\n  filter(Nivel == \"Secundario\")  # filter the high schools\nSCHOOLShigh = SCHOOLShigh |&gt; mutate(id = 1:nrow(SCHOOLShigh)) # provide and id\n\n# list of XY coordinates for ORS\ncoord_schools = data.frame(lon = SCHOOLShigh$coordinates[, 1],\n                           lat = SCHOOLShigh$coordinates[, 2])\n                           # id = SCHOOLShigh$id)\n\nAnd proceed with the time isochrones, for a range of 20 min, with 5 min intervals.\n\nISOCschools = ors_isochrones(\n  coord_schools,\n  profile = \"foot-walking\",\n  range_type = \"time\", # or distance\n  range = 20*60, # 20 minutes in seconds\n  interval = 5*60, # to have intervals of 5 minutes\n  attributes = \"area\", #you can directly get area, population, and so on.\n  output = \"sf\"\n)\n\nISOCschools = arrange(ISOCschools, -value) |&gt; # larger polygons on top of the table so the are displayed behind.\n  select(-center)\n\n\nAnd now merge this information with the schools’ names.\n\nISOCschools = ISOCschools |&gt;\n  mutate(id = group_index + 1, # because group_index starts at 0\n         value = value/60) |&gt; \n    left_join(SCHOOLShigh |&gt;\n                st_drop_geometry() |&gt;\n                select(id, INF_NOME, Alunos))\n\n\nmapview(ISOCschools, zcol = \"value\", alpha.regions = 0.5)\n\n\n\n\n\nAnd estimate areas for the 20min isochrones\n\nsummary(ISOCschools$area[ISOCschools$value==20])/1000000 # in km²\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  3.295   4.303   5.363   5.098   5.804   6.511",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Buffers vs. Isochones</span>"
    ]
  },
  {
    "objectID": "buffers.html#footnotes",
    "href": "buffers.html#footnotes",
    "title": "14  Buffers vs. Isochones",
    "section": "",
    "text": "Here I selected only the first variable because now we also have the coordinates information (unecessary for this procedure)↩︎",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Buffers vs. Isochones</span>"
    ]
  },
  {
    "objectID": "open-data.html",
    "href": "open-data.html",
    "title": "15  Open transportation data",
    "section": "",
    "text": "15.1 Road Networks\nIn this chapter we will guide you through sources of open data for transportation analysis: road networks and public transportation information.",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Open transportation data</span>"
    ]
  },
  {
    "objectID": "open-data.html#road-networks",
    "href": "open-data.html#road-networks",
    "title": "15  Open transportation data",
    "section": "",
    "text": "15.1.1 OpenStreetMap\nThe OpenStreetMap is a collaborative online mapping project that creates a free editable map of the world.\nThis is the most used source of road network data for transportation analysis in academia, since it is available almost everywhere in the world, is open and free to use.\n\n\n\n\n\n\nAlthough it can be not 100% accurate, OSM is a good source of data for most of the cases.\n\n\n\nYou can access it’s visualization tool at www.openstreetmap.org. To edit the map, you can use the Editor, once you register.\nIf you want to download the data, you can use the following tools.\n\nOverpass API\nGeofabrik\n\nThese websites include all the OSM data, with much more information than you need.\n\n\n15.1.2 HOT Export Tool\nThis interactive tool helps you to select the region you want to extract, the type of information to include, and the output data format.\nAccess via export.hotosm.org1.\n\nAfter the export, you can read in R using the sf package:\n\n\nCode\nGent = sf::st_read(\"../data/Gent_center.gpkg\", quiet = TRUE)\n\nmapview::mapview(Gent, zcol = \"highway\")\n\n\n\n\n\n\n\n\n15.1.3 OSM in R\nThere are also some R packages that can help you to download and work with OpenStreetMap data, such as:\n\nosmdata\nosmextract\n\nThis is an example of how to download OpenStreetMap road network data using the osmextract package:\n\n\nCode\nlibrary(osmextract)\nOSM_Malta = oe_get_network(place = \"Malta\") # it will geocode the place\n\nMalta_main_roads = OSM_Malta |&gt; \n  filter(highway %in% c(\"primary\", \"secondary\", \"tertiary\", \"trunk\"))\n\nplot(Malta_main_roads[\"highway\"])",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Open transportation data</span>"
    ]
  },
  {
    "objectID": "open-data.html#transportation-services-data",
    "href": "open-data.html#transportation-services-data",
    "title": "15  Open transportation data",
    "section": "15.2 Transportation Services’ Data",
    "text": "15.2 Transportation Services’ Data\n\n15.2.1 GTFS\nGeneral Transit Feed Specification (GTFS) is standard format for documenting public transportation information, including: routes, schedules, stop locations, calendar patterns, trips, and possible transfers. Transit agencies are responsible for maintaining the data up-to-date.\nThis information is used in several applications, such as Google Maps, to provide public transportation directions. It can be offered for a city, a region, or even a whole country, depending on the PT agency.\nThe recent version 2 of the GTFS standard includes more information, such as real-time data.\nThe data is usually in a .zip file that includes several .txt files (one for each type of information) with tabular relations.\n\n\n\nSource: trilliumtransit.com\n\n\n\nOnline sources\nYou can find most GTFS data in the following websites:\n\nTransitLand\nTransitFeeds\n\nSome PT agencies also provide their open-data in their websites.\n\n\nR packages\nThere are some nice R packages to read and manipulate GTFS data, such as:\n\ntidytransit\ngtfstools\n\n\n\n\n\n\n\nBe aware that they may share the same function names, so it is important to use of of them at the time.\n\n\n\n\n\n\n15.2.2 National Access Points\nThe European Union has a directive that requires the member states to provide access to transportation data. Data includes not only Public Transportation data, but also road networks, car parking, and other transportation-related information.\nList of the European Union members states with National Access Points for Transportation data\nExample of Bus services data in Belgium:\n\n\n\nSource: Transport Data Belgium",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Open transportation data</span>"
    ]
  },
  {
    "objectID": "open-data.html#footnotes",
    "href": "open-data.html#footnotes",
    "title": "15  Open transportation data",
    "section": "",
    "text": "You need an OSM account to use it.↩︎",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Open transportation data</span>"
    ]
  },
  {
    "objectID": "grid.html",
    "href": "grid.html",
    "title": "16  Grids",
    "section": "",
    "text": "16.1 Make grid\nlibrary(sf)\nlibrary(dplyr)\nlibrary(mapview)\n\nMUNICIPIOSgeo = st_read(\"../data/Municipalities_geo.gpkg\", quiet = TRUE)\nLISBON = MUNICIPIOSgeo |&gt; filter(Municipality == \"Lisboa\") # filter only Lisbon\nTo make a grid we is the st_make_grid() function.\nGRID = LISBON |&gt;\n  st_transform(crs = 3857) |&gt; # to a projected crs\n  st_make_grid(cellsize = 500, # meters, we are using a projected crs\n               what = \"polygons\",\n               square = TRUE) |&gt; # if FALSE, hexagons\n  st_sf() |&gt; # from list to sf\n  st_transform(crs = 4326) # to WGS84\n\nGRID = GRID |&gt;  \n  mutate(id = c(1:nrow(GRID)))  # just to give an ID to each cell \n\nmapview(GRID, alpha = 0)",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Grids</span>"
    ]
  },
  {
    "objectID": "grid.html#count-points-in-polygons",
    "href": "grid.html#count-points-in-polygons",
    "title": "16  Grids",
    "section": "16.2 Count points in polygons",
    "text": "16.2 Count points in polygons\nWe can use the grid to count survey respondents in each cell.\n\nSURVEY = read.csv(\"../data/SURVEY.txt\", sep = \"\\t\")\nSURVEYgeo = SURVEY |&gt; st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326) # convert to sf from coordinates\n\nSURVEY_with_GRIDid = SURVEYgeo |&gt; \n  st_join(GRID, \n          join = st_intersects) |&gt; \n    st_drop_geometry() |&gt; \n    group_by(id) |&gt; \n    summarise(count = n()) |&gt; # variable with the number of points\n    ungroup()\n\n# back to grid\nGRIDdensity = GRID |&gt; left_join(SURVEY_with_GRIDid)\n\nmapview(GRIDdensity, zcol = \"count\")\n\n\n\n\n\nOr to count bus stops in each cell.\n\nBUSstops = st_read(\"../geo/Carris_stops.gpkg\", quiet = TRUE)\n\nBUS_with_GRIDid = BUSstops |&gt; \n  st_join(GRID, \n          join = st_intersects) |&gt; \n    st_drop_geometry() |&gt; \n    group_by(id) |&gt; \n    summarise(freq = sum(frequency)) |&gt; \n    ungroup()\n\n# back to grid\nGRIDdensity = GRID |&gt; left_join(BUS_with_GRIDid)\n\nmapview(GRIDdensity |&gt; filter(!is.na(freq)), # exclude NAs\n        zcol = \"freq\",\n        alpha.regions = 0.5, # area transparency\n        alpha = 0) # hide cell limits",
    "crumbs": [
      "**GIS in R**",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Grids</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Engel, Claudia A. 2023. Introduction to r. cengel.github.io/R-intro/.\n\n\nField, A. 2024. Discovering Statistics Using IBM SPSS\nStatistics. 6th ed. SAGE Publications. https://books.google.pt/books?id=83L2EAAAQBAJ.\n\n\nINE. 2018. “Mobilidade e Funcionalidade Do Território Nas Áreas Metropolitanas do Porto e de Lisboa:\n2017.” Lisboa: Instituto National de Estatística. https://www.ine.pt/xportal/xmain?xpid=INE&xpgid=ine_publicacoes&PUBLICACOESpub_boui=349495406&PUBLICACOESmodo=2&xlang=pt.\n\n\n———. 2022. “Censos 2021- XVI Recenseamento\nGeral da População. VI Recenseamento Geral da Habitação.”\nLisboa: Instituto National de Estatística. https://censos.ine.pt/xurl/pub/65586079.\n\n\nLegendre, Pierre, and Louis Legendre. 2012. “Chapter 8 - Cluster\nAnalysis.” In Numerical Ecology, edited by Pierre\nLegendre and Louis Legendre, 3rd ed., 24:337–424. Developments in\nEnvironmental Modelling. Elsevier. https://doi.org/https://doi.org/10.1016/B978-0-444-53868-0.50008-3.\n\n\nLovelace, Robin, and Richard Ellison. 2018. “Stplanr: A Package\nfor Transport Planning.” The R Journal 10\n(2): 10. https://doi.org/10.32614/RJ-2018-053.\n\n\nLovelace, Robin, Rosa Félix, and Dustin Carlino. 2022. “Jittering:\nA Computationally Efficient Method for Generating Realistic Route\nNetworks from Origin-Destination Data.” Findings, April.\nhttps://doi.org/10.32866/001c.33873.\n\n\nLovelace, Robin, and Malcolm Morgan. 2024. Od: Manipulate and Map\nOrigin-Destination Data. https://github.com/itsleeds/od.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2024.\nGeocomputation with r. 2nd ed. Chapman; Hall/CRC. https://r.geocompx.org/.\n\n\nMartinez, Luís Garrido, João de Abreu e Silva, and José Manuel Viegas.\n2010. “Assessment of Residential Location\nSatisfaction in the Lisbon Metropolitan Area.” In. https://trid.trb.org/View/909819.\n\n\nOleś, Andrzej K. 2025. Openrouteservice: An ’Openrouteservice’ API\nClient. https://github.com/GIScience/openrouteservice-r.\n\n\nPebesma, Edzer, and Roger Bivand. 2023. Spatial Data Science: With\nApplications in R. Boca Raton: Chapman; Hall/CRC. https://doi.org/10.1201/9780429459016.\n\n\nR Core Team. 2025. R: A Language and Environment for Statistical\nComputing. Vienna, Austria: R Foundation for Statistical Computing.\nhttps://www.R-project.org/.\n\n\nWickham, Hadley, Mine Çetinaka-Rundel, and Garrett Grolemund. 2017.\nR for Data Science. 2nd ed. O’Reilly Sebastopol. https://r4ds.hadley.nz/.\n\n\nZomorrodi, Ryan. 2024. Centr: Weighted and Unweighted Spatial\nCenters. https://ryanzomorrodi.github.io/centr/.",
    "crumbs": [
      "References"
    ]
  }
]